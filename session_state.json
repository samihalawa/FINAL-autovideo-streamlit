{"logs": ["Optimized settings", "Optimized toggle_theme", "Optimized imports", "Optimized cached_download", "Optimized generate_storyboard", "Optimized validate_storyboard", "Optimized parse_storyboard", "Optimized fetch_video_clips", "Optimized create_fallback_clip", "Optimized generate_voiceover", "Optimized create_silent_audio", "Optimized create_scene_clip", "Optimized add_fade_transition", "Optimized create_animated_text", "Optimized apply_color_grading", "Optimized create_lower_third", "Optimized smart_cut", "Optimized apply_speed_changes", "Optimized create_video", "Optimized enhance_clip", "Optimized process_clip", "Optimized apply_fade_effects", "Optimized add_text_overlay", "Optimized add_narration", "Optimized add_background_music", "Optimized add_watermark", "Optimized split_video", "Optimized merge_video_parts", "Optimized save_storyboard_backup", "Optimized load_storyboard_backup", "Optimized add_subtitles_to_video", "Optimized preview_storyboard_slideshow", "Optimized add_logo_to_video", "Optimized compress_video", "Optimized apply_bw_filter", "Optimized overlay_image_on_video", "Optimized adjust_video_speed", "Optimized crop_video", "Optimized adjust_resolution_based_on_system", "Optimized generate_video_thumbnail", "Optimized add_intro_outro", "Optimized adjust_audio_volume", "Optimized generate_gradient_text_overlay", "Optimized run_video_rendering_thread", "Optimized check_system_capabilities", "Optimized log_system_resources", "Optimized download_additional_assets", "Optimized calculate_estimated_render_time", "Optimized manage_temp_directory", "Optimized handle_session_expiration", "Optimized split_storyboard_scenes", "Optimized add_transition_effects_between_scenes", "Optimized optimize_storyboard_text_prompts", "Optimized select_background_music", "Optimized analyze_script", "Optimized apply_transition", "Optimized cleanup_temp_files", "Optimized process_scene_with_progress", "Optimized generate_valid_storyboard", "Optimized prompt_card", "Optimized predict_processing_issues", "Optimized main", "Optimized fetch_video_clips", "Optimized create_video_workflow", "Optimized generate_script", "Optimized display_storyboard_preview", "Optimized color_gradient", "Optimized imports", "Optimized toggle_theme", "Optimized generate_storyboard", "Optimized cached_download", "Optimized settings", "Optimized validate_storyboard", "Optimized parse_storyboard", "Optimized fetch_video_clips", "Optimized create_fallback_clip", "Optimized generate_voiceover", "Optimized create_silent_audio", "Optimized create_scene_clip", "Optimized add_fade_transition", "Optimized create_animated_text", "Optimized apply_color_grading", "Optimized create_lower_third", "Optimized smart_cut", "Optimized apply_speed_changes", "Optimized create_video", "Optimized enhance_clip", "Optimized process_clip", "Optimized apply_fade_effects", "Optimized add_text_overlay", "Optimized add_narration", "Optimized add_background_music", "Optimized add_watermark", "Optimized split_video", "Optimized merge_video_parts", "Optimized save_storyboard_backup", "Optimized load_storyboard_backup", "Optimized add_subtitles_to_video", "Optimized preview_storyboard_slideshow", "Optimized add_logo_to_video", "Optimized compress_video", "Optimized apply_bw_filter", "Optimized overlay_image_on_video", "Optimized adjust_video_speed", "Optimized crop_video", "Optimized adjust_resolution_based_on_system", "Optimized generate_video_thumbnail", "Optimized add_intro_outro", "Optimized adjust_audio_volume", "Optimized generate_gradient_text_overlay", "Optimized run_video_rendering_thread", "Optimized check_system_capabilities", "Optimized log_system_resources", "Optimized download_additional_assets", "Optimized calculate_estimated_render_time", "Optimized manage_temp_directory", "Optimized handle_session_expiration", "Optimized split_storyboard_scenes", "Optimized add_transition_effects_between_scenes", "Optimized optimize_storyboard_text_prompts", "Optimized select_background_music", "Optimized analyze_script", "Optimized apply_transition", "Optimized cleanup_temp_files", "Optimized process_scene_with_progress", "Optimized generate_valid_storyboard", "Optimized prompt_card", "Optimized predict_processing_issues", "Optimized generate_script", "Optimized fetch_video_clips", "Optimized create_video_workflow", "Optimized main", "Optimized display_storyboard_preview", "Optimized color_gradient", "Optimized toggle_theme", "Optimized cached_download", "Optimized generate_storyboard", "Optimized validate_storyboard", "Optimized parse_storyboard", "Optimized fetch_video_clips", "Optimized create_fallback_clip", "Optimized generate_voiceover", "Optimized create_silent_audio", "Optimized create_scene_clip", "Optimized add_fade_transition", "Optimized create_animated_text", "Optimized apply_color_grading", "Optimized create_lower_third", "Optimized smart_cut", "Optimized apply_speed_changes", "Optimized create_video", "Optimized enhance_clip", "Optimized process_clip", "Optimized apply_fade_effects", "Optimized add_text_overlay", "Optimized add_narration", "Optimized add_background_music", "Optimized add_watermark", "Optimized split_video", "Optimized merge_video_parts", "Optimized save_storyboard_backup", "Optimized load_storyboard_backup", "Optimized add_subtitles_to_video", "Optimized preview_storyboard_slideshow", "Optimized add_logo_to_video", "Optimized compress_video", "Optimized apply_bw_filter", "Optimized overlay_image_on_video", "Optimized adjust_video_speed", "Optimized crop_video", "Optimized adjust_resolution_based_on_system", "Optimized generate_video_thumbnail", "Optimized add_intro_outro", "Optimized adjust_audio_volume", "Optimized generate_gradient_text_overlay", "Optimized run_video_rendering_thread", "Optimized check_system_capabilities", "Optimized log_system_resources", "Optimized download_additional_assets", "Optimized calculate_estimated_render_time", "Optimized manage_temp_directory", "Optimized handle_session_expiration", "Optimized split_storyboard_scenes", "Optimized add_transition_effects_between_scenes", "Optimized optimize_storyboard_text_prompts", "Optimized select_background_music", "Optimized analyze_script", "Optimized apply_transition", "Optimized cleanup_temp_files", "Optimized process_scene_with_progress", "Optimized generate_valid_storyboard", "Optimized prompt_card", "Optimized predict_processing_issues", "Optimized generate_script", "Optimized fetch_video_clips", "Optimized create_video_workflow", "Optimized main", "Optimized display_storyboard_preview", "Optimized color_gradient"], "current_function_index": 0, "script_content": "import streamlit as st\nfrom openai import OpenAI\nimport os\nimport moviepy.editor as mpe\nimport requests\nfrom tempfile import NamedTemporaryFile\nfrom moviepy.video.fx.all import fadein, fadeout, resize\nimport psutil\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport shutil\nimport logging\nfrom gtts import gTTS\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment\nimport random\nimport moviepy.video.fx.all as vfx\nimport cv2\nfrom pydub.silence import split_on_silence\nimport numpy as np\nimport tempfile\nfrom functools import lru_cache\nfrom huggingface_hub import InferenceClient, hf_hub_download\nfrom huggingface_hub import login, HfApi\nfrom datasets import load_dataset\nimport textwrap\nfrom scenedetect import detect, ContentDetector\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\nimport re\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Set page config at the very beginning of the script\nst.set_page_config(page_title=\"AutovideoAI\", page_icon=\"\ud83c\udfa5\", layout=\"wide\")\n\n# Load environment variables\nload_dotenv()\nhf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n\n# Check if already logged in\napi = HfApi()\ntry:\n    api.whoami(token=hf_token)\n    logger.info(\"Already logged in to Hugging Face\")\nexcept Exception:\n    logger.info(\"Logging in to Hugging Face\")\n    login(token=hf_token, add_to_git_credential=False)\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Theme customization\nif 'theme' not in st.session_state:\n    st.session_state.theme = 'light'\n\ndef toggle_theme():\n    st.session_state.theme = 'dark' if st.session_state.theme == 'light' else 'light'\n\n# Apply theme\nif st.session_state.theme == 'dark':\n    st.markdown(\"\"\"\n    <style>\n    .stApp {\n        background-color: #1E1E1E;\n        color: white;\n    }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n\n# Sample prompts\nSAMPLE_PROMPTS = [\n    \"Create a motivational video about overcoming challenges\",\n    \"Make an educational video explaining photosynthesis\",\n    \"Design a funny video about the struggles of working from home\",\n]\n\n# Replace the MUSIC_GENRES list with a dictionary of genre-specific tracks\nMUSIC_TRACKS = {\n    \"Electronic\": \"https://www.bensound.com/bensound-dubstep\",\n    \"Experimental\": \"https://www.bensound.com/bensound-enigmatic\",\n    \"Folk\": \"https://www.bensound.com/bensound-acousticbreeze\",\n    \"Hip-Hop\": \"https://www.bensound.com/bensound-groovyhiphop\",\n    \"Instrumental\": \"https://www.bensound.com/bensound-pianomoment\",\n    \"Pop\": \"https://www.bensound.com/bensound-ukulele\",\n    \"Rock\": \"https://www.bensound.com/bensound-extremeaction\"\n}\n\n# Set up caching for downloaded assets\n@lru_cache(maxsize=100)\ndef cached_download(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.content\n    return None\n\n# 1. Function to generate storyboard based on user prompt using structured JSON\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef generate_storyboard(prompt, style=\"motivational\"):\n    try:\n        input_text = f\"\"\"Generate a detailed {style} video storyboard based on this prompt: \"{prompt}\"\n        Provide the storyboard in JSON format with the following structure:\n        {{\n            \"title\": \"Overall video title\",\n            \"scenes\": [\n                {{\n                    \"scene_number\": 1,\n                    \"title\": \"Scene title\",\n                    \"description\": \"Detailed scene description\",\n                    \"narration\": \"Narration text for the scene\",\n                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n                    \"duration\": \"Duration in seconds\",\n                    \"visual_elements\": [\"List of visual elements to include\"],\n                    \"transitions\": {{\n                        \"in\": \"Transition type for entering the scene\",\n                        \"out\": \"Transition type for exiting the scene\"\n                    }}\n                }}\n            ],\n            \"target_audience\": \"Description of the target audience\",\n            \"overall_tone\": \"Description of the overall tone of the video\"\n        }}\n        Ensure there are at least 3 scenes in the storyboard.\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative video storyboard generator. Respond with valid JSON following the specified structure.\"},\n                {\"role\": \"user\", \"content\": input_text}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        storyboard = json.loads(response.choices[0].message.content)\n        return storyboard\n    except Exception as e:\n        logger.error(f\"Error generating storyboard: {str(e)}\")\n        st.error(\"An error occurred while generating the storyboard. Please try again.\")\n    return None\n\ndef validate_storyboard(storyboard):\n    if \"title\" not in storyboard or \"scenes\" not in storyboard or not isinstance(storyboard[\"scenes\"], list):\n        return False\n    if len(storyboard[\"scenes\"]) < 3:\n        return False\n    \n    required_fields = [\"scene_number\", \"title\", \"description\", \"narration\", \"keywords\", \"duration\", \"overlay_text\", \"visual_elements\", \"audio_cues\", \"transitions\"]\n    return all(all(field in scene for field in required_fields) for scene in storyboard[\"scenes\"])\n\n# 2. Function to parse structured JSON storyboard data\ndef parse_storyboard(storyboard):\n    try:\n        return json.loads(storyboard).get(\"scenes\", [])\n    except json.JSONDecodeError:\n        return []\n\n# 3. Function to fetch video clips dynamically based on scene keywords\ndef fetch_video_clips(scenes, max_retries=3):\n    logger.info(f\"Fetching video clips for {len(scenes)} scenes\")\n    video_clips = []\n    \n    api = HfApi()\n    \n    for i, scene in enumerate(scenes):\n        logger.info(f\"Fetching clip for scene {i+1}: {scene['title']}\")\n        \n        for attempt in range(max_retries):\n            try:\n                # Expand the search query to include more relevant terms\n                keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\n                search_query = \" OR \".join(keywords)\n                \n                search_results = api.list_datasets(search=search_query, limit=20)  # Increase the limit\n                matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\n                \n                if matching_datasets:\n                    chosen_dataset = random.choice(matching_datasets)\n                    dataset_info = api.dataset_info(chosen_dataset.id)\n                    \n                    if dataset_info.card_data and 'samples' in dataset_info.card_data:\n                        sample = random.choice(dataset_info.card_data['samples'])\n                        if 'video' in sample:\n                            video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\n                            clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))  # Limit to 10 seconds max\n                            logger.info(f\"Clip fetched for scene {i+1}: duration={clip.duration}s\")\n                        else:\n                            raise ValueError(\"No video found in the sample\")\n                    else:\n                        raise ValueError(\"No samples found in the dataset\")\n                else:\n                    raise ValueError(\"No matching datasets found\")\n                \n                if clip:\n                    break  # Successfully fetched a clip, exit the retry loop\n            except Exception as e:\n                logger.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n                if attempt == max_retries - 1:\n                    logger.warning(f\"All attempts failed. Creating fallback clip for scene {i+1}.\")\n                    clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\n        \n        video_clips.append({'clip': clip, 'scene': scene})\n    \n    return video_clips\n\ndef create_fallback_clip(scene, duration=5):\n    text = scene.get('title', 'Scene')\n    size = (1280, 720)\n    \n    # Create a black background\n    img = Image.new('RGB', size, color='black')\n    draw = ImageDraw.Draw(img)\n    \n    # Use a default font\n    font = ImageFont.load_default()\n    \n    # Wrap text\n    wrapped_text = textwrap.wrap(text, width=20)\n    \n    # Calculate text position\n    y_text = (size[1] - len(wrapped_text) * 80) // 2\n    \n    # Draw text\n    for line in wrapped_text:\n        line_width, line_height = draw.textbbox((0, 0), line, font=font)[2:]\n        position = ((size[0] - line_width) / 2, y_text)\n        draw.text(position, line, font=font, fill='white')\n        y_text += line_height + 10\n    \n    # Convert PIL Image to numpy array\n    img_array = np.array(img)\n    \n    # Create video clip from the image\n    clip = mpe.ImageClip(img_array).set_duration(duration)\n    \n    return clip\n\n# 4. Function to generate voiceover with Hugging Face Inference API\ndef generate_voiceover(narration_text):\n    logger.info(f\"Generating voiceover for text: {narration_text[:50]}...\")\n    try:\n        tts = gTTS(text=narration_text, lang='en', slow=False)\n        temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n        tts.save(temp_audio_file.name)\n        return mpe.AudioFileClip(temp_audio_file.name)\n    except Exception as e:\n        logger.error(f\"Error generating voiceover: {str(e)}\")\n        return create_silent_audio(len(narration_text.split()) / 2)  # Assuming 2 words per second\n\ndef create_silent_audio(duration):\n    silent_segment = AudioSegment.silent(duration=int(duration * 1000))  # pydub uses milliseconds\n    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n    silent_segment.export(temp_audio_file.name, format=\"mp3\")\n    return mpe.AudioFileClip(temp_audio_file.name)\n\ndef create_scene_clip(scene, duration=5):\n    try:\n        # Create a gradient background\n        gradient = np.linspace(0, 255, 1280)\n        background = np.tile(gradient, (720, 1)).astype(np.uint8)\n        background = np.stack((background,) * 3, axis=-1)\n        \n        # Create video clip from the background\n        clip = mpe.ImageClip(background).set_duration(duration)\n        \n        # Add text\n        txt_clip = mpe.TextClip(scene['title'], fontsize=70, color='white', font='Arial-Bold')\n        txt_clip = txt_clip.set_position('center').set_duration(duration)\n        \n        # Add keywords as subtitles\n        if 'keywords' in scene:\n            keywords_txt = \", \".join(scene['keywords'])\n            keywords_clip = mpe.TextClip(keywords_txt, fontsize=30, color='yellow', font='Arial')\n            keywords_clip = keywords_clip.set_position(('center', 0.8), relative=True).set_duration(duration)\n            clip = mpe.CompositeVideoClip([clip, txt_clip, keywords_clip])\n        else:\n            clip = mpe.CompositeVideoClip([clip, txt_clip])\n        \n        # Add fade in and out\n        clip = clip.fadein(0.5).fadeout(0.5)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating scene clip: {str(e)}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)\n\n# Add this function for smooth transitions\ndef add_fade_transition(clip1, clip2, duration=1):\n    return mpe.CompositeVideoClip([clip1.crossfadeout(duration), clip2.crossfadein(duration)])\n\n# Add this function for dynamic text animations\ndef create_animated_text(text, duration=5, font_size=70, color='white'):\n    try:\n        # Create a black background image\n        img = Image.new('RGB', (1280, 720), color='black')\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=font_size)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position to center the text\n        position = ((1280 - text_width) / 2, (720 - text_height) / 2)\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill=color)\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        clip = mpe.ImageClip(img_array).set_duration(duration)\n        \n        # Add fade in and fade out effects\n        clip = clip.fadein(1).fadeout(1)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating animated text: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)\n\n# Add this function for color grading\ndef apply_color_grading(clip, brightness=1.0, contrast=1.0, saturation=1.0):\n    return clip.fx(vfx.colorx, brightness).fx(vfx.lum_contrast, contrast=contrast).fx(vfx.colorx, saturation)\n\n# Add this function for creating lower thirds\ndef create_lower_third(text, duration):\n    try:\n        # Create a transparent background\n        img = Image.new('RGBA', (1280, 720), (0, 0, 0, 0))\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=30)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position for lower third\n        position = ((1280 - text_width) / 2, 720 - text_height - 50)\n        \n        # Draw semi-transparent background\n        bg_bbox = (position[0]-10, position[1]-10, position[0]+text_width+10, position[1]+text_height+10)\n        draw.rectangle(bg_bbox, fill=(0,0,0,153))\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill='white')\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        return mpe.ImageClip(img_array).set_duration(duration)\n    except Exception as e:\n        logger.error(f\"Error creating lower third: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0,0)).set_duration(duration)\n\ndef smart_cut(video_clip, audio_clip):\n    try:\n        # Check if audio_clip is a file path or an AudioClip object\n        if isinstance(audio_clip, str):\n            audio_segment = AudioSegment.from_wav(audio_clip)\n        elif isinstance(audio_clip, mpe.AudioClip):\n            # Convert AudioClip to numpy array\n            audio_array = audio_clip.to_soundarray()\n            audio_segment = AudioSegment(\n                audio_array.tobytes(),\n                frame_rate=audio_clip.fps,\n                sample_width=audio_array.dtype.itemsize,\n                channels=1 if audio_array.ndim == 1 else audio_array.shape[1]\n            )\n        else:\n            logger.warning(\"Unsupported audio type. Returning original video clip.\")\n            return video_clip\n\n        # Split audio on silences\n        chunks = split_on_silence(audio_segment, min_silence_len=500, silence_thresh=-40)\n        \n        # Calculate timestamps for cuts\n        cut_times = [0]\n        for chunk in chunks:\n            cut_times.append(cut_times[-1] + len(chunk) / 1000)\n        \n        # Cut video based on audio\n        cut_clips = [video_clip.subclip(start, end) for start, end in zip(cut_times[:-1], cut_times[1:])]\n        \n        return mpe.concatenate_videoclips(cut_clips)\n    except Exception as e:\n        logger.error(f\"Error in smart_cut: {str(e)}\")\n        return video_clip\n\ndef apply_speed_changes(clip, speed_factor=1.5, threshold=0.1):\n    try:\n        if not hasattr(clip, 'fps') or clip.fps is None:\n            clip = clip.set_fps(24)  # Set a default fps if not present\n        \n        frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in clip.iter_frames()]\n        motion = [np.mean(cv2.absdiff(frames[i], frames[i+1])) for i in range(len(frames)-1)]\n        \n        speed_clip = clip.fl(lambda gf, t: gf(t * speed_factor) if motion[int(t*clip.fps)] < threshold else gf(t))\n        \n        return speed_clip\n    except Exception as e:\n        logger.error(f\"Error in apply_speed_changes: {str(e)}\")\n        return clip\n\n# 7. Function to create and finalize the video\ndef create_video(video_clips, background_music_file, video_title):\n    logger.info(f\"Starting video creation process for '{video_title}'\")\n    try:\n        clips = []\n        for i, clip_data in enumerate(video_clips):\n            try:\n                clip = clip_data['clip']\n                narration = clip_data['narration']\n                clip = clip.set_audio(narration)\n                clips.append(clip)\n                logger.info(f\"Processed clip {i+1} successfully\")\n            except Exception as e:\n                logger.error(f\"Error processing clip {i+1}: {str(e)}\")\n        \n        if not clips:\n            raise ValueError(\"No valid clips were created\")\n        \n        logger.info(f\"Concatenating {len(clips)} scene clips\")\n        final_clip = mpe.concatenate_videoclips(clips)\n        \n        if background_music_file:\n            logger.info(\"Adding background music\")\n            background_music = mpe.AudioFileClip(background_music_file).volumex(0.1)\n            background_music = background_music.audio_loop(duration=final_clip.duration)\n            final_audio = mpe.CompositeAudioClip([final_clip.audio, background_music])\n            final_clip = final_clip.set_audio(final_audio)\n        \n        # Add intro and outro\n        intro_clip = mpe.TextClip(video_title, fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        outro_clip = mpe.TextClip(\"Thanks for watching!\", fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        final_clip = mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\n        \n        output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name\n        logger.info(f\"Writing final video to {output_file}\")\n        final_clip.write_videofile(output_file, codec='libx264', audio_codec='aac', fps=24)\n        logger.info(\"Video creation process completed\")\n        return output_file\n    except Exception as e:\n        logger.error(f\"Error in create_video: {str(e)}\")\n        st.error(f\"An error occurred while creating the video: {str(e)}\")\n        return None\n\ndef enhance_clip(clip, script_analysis):\n    # Apply color grading based on sentiment\n    if script_analysis['sentiment'] == 'POSITIVE':\n        clip = apply_color_grading(clip, brightness=1.1, saturation=1.2)\n    elif script_analysis['sentiment'] == 'NEGATIVE':\n        clip = apply_color_grading(clip, brightness=0.9, contrast=1.1)\n    \n    # Add dynamic text animations\n    if clip.duration > 2:\n        text_clip = create_animated_text(clip.scene['title'], duration=2)\n        clip = mpe.CompositeVideoClip([clip, text_clip.set_start(1)])\n    \n    # Add smooth transitions\n    clip = clip.crossfadein(0.5).crossfadeout(0.5)\n    \n    return clip\n\ndef process_clip(enhanced_clip, clip_data, script_analysis):\n    scene = clip_data['scene']\n    duration = float(scene['duration'])\n    processed_clip = enhanced_clip.set_duration(duration)\n    \n    # Apply color grading based on sentiment and style\n    if script_analysis['sentiment'] == 'POSITIVE':\n        processed_clip = apply_color_grading(processed_clip, brightness=1.1)\n    else:\n        processed_clip = apply_color_grading(processed_clip, brightness=0.9)\n    \n    if script_analysis['style'] in ['humorous', 'casual']:\n        processed_clip = processed_clip.fx(vfx.colorx, 1.2)  # More vibrant for humorous/casual content\n    elif script_analysis['style'] in ['dramatic', 'formal']:\n        processed_clip = processed_clip.fx(vfx.lum_contrast, contrast=1.2)  # More contrast for dramatic/formal content\n    \n    if scene.get('overlay_text'):\n        text_clip = create_animated_text(scene['overlay_text'], duration)\n        processed_clip = mpe.CompositeVideoClip([processed_clip, text_clip])\n    \n    lower_third = create_lower_third(scene['title'], duration)\n    processed_clip = mpe.CompositeVideoClip([processed_clip, lower_third])\n    \n    return processed_clip\n\n# 8. Function to apply fade-in/fade-out effects to video clips\ndef apply_fade_effects(clip, duration=1):\n    try:\n        return fadein(clip, duration).fx(fadeout, duration)\n    except Exception as e:\n        raise ValueError(f\"Error applying fade effects: {e}\")\n\n# 9. Function to add text overlay to video clips\ndef add_text_overlay(clip, text):\n    if text:\n        try:\n            text_clip = mpe.TextClip(text, fontsize=70, color='white', font='Arial-Bold')\n            text_clip = text_clip.set_position('center').set_duration(clip.duration)\n            return mpe.CompositeVideoClip([clip, text_clip])\n        except Exception as e:\n            raise ValueError(f\"Error adding text overlay: {e}\")\n    return clip\n\n# 10. Function to add narration to video clip\ndef add_narration(clip, narration_file):\n    try:\n        return clip.set_audio(mpe.AudioFileClip(narration_file))\n    except Exception as e:\n        raise ValueError(f\"Error adding narration: {e}\")\n\n# 11. Function to add background music to video\ndef add_background_music(clip, music_file):\n    try:\n        background_audio = mpe.AudioFileClip(music_file)\n        return clip.set_audio(mpe.CompositeAudioClip([clip.audio, background_audio.volumex(0.1)]))\n    except Exception as e:\n        raise ValueError(f\"Error adding background music: {e}\")\n\n# 12. Function to add watermarks to video clips\ndef add_watermark(clip, watermark_text=\"Sample Watermark\"):\n    try:\n        watermark = mpe.TextClip(watermark_text, fontsize=30, color='white', font='Arial')\n        watermark = watermark.set_position(('right', 'bottom')).set_duration(clip.duration)\n        return mpe.CompositeVideoClip([clip, watermark])\n    except Exception as e:\n        st.error(f\"Error adding watermark: {e}\")\n        return clip  # Return original clip if watermark fails\n\n# 13. Function to split video into parts for processing\ndef split_video(video_clip, part_duration=10):\n    try:\n        return [video_clip.subclip(start, min(start + part_duration, video_clip.duration)) for start in range(0, int(video_clip.duration), part_duration)]\n    except Exception as e:\n        st.error(f\"Error splitting video: {e}\")\n        return [video_clip]  # Return original clip if splitting fails\n\n# 14. Function to merge video parts back together\ndef merge_video_parts(video_parts):\n    try:\n        return mpe.concatenate_videoclips(video_parts, method=\"compose\")\n    except Exception as e:\n        st.error(f\"Error merging video parts: {e}\")\n        return video_parts[0] if video_parts else None  # Return first part if merging fails\n\n# 15. Function to save a temporary JSON backup of generated storyboard\ndef save_storyboard_backup(storyboard, filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'w') as f:\n            json.dump(storyboard, f)\n        st.success(f\"Storyboard backup saved to {filename}\")\n    except Exception as e:\n        st.error(f\"Error saving storyboard backup: {e}\")\n\n# 16. Function to load a saved storyboard from backup\ndef load_storyboard_backup(filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'r') as f:\n            storyboard = json.load(f)\n        st.success(f\"Storyboard loaded from {filename}\")\n        return storyboard\n    except FileNotFoundError:\n        st.warning(f\"Backup file {filename} not found.\")\n        return None\n    except json.JSONDecodeError:\n        st.error(f\"Error decoding JSON from {filename}\")\n        return None\n    except Exception as e:\n        st.error(f\"Error loading storyboard backup: {e}\")\n        return None\n\n# 17. Function to add subtitles to video\ndef add_subtitles_to_video(clip, subtitles):\n    try:\n        subtitle_clips = [\n            mpe.TextClip(subtitle['text'], fontsize=50, color='white', size=clip.size, font='Arial-Bold')\n            .set_position(('bottom')).set_start(subtitle['start']).set_duration(subtitle['duration'])\n            for subtitle in subtitles\n        ]\n        return mpe.CompositeVideoClip([clip] + subtitle_clips)\n    except Exception as e:\n        st.error(f\"Error adding subtitles: {e}\")\n        return clip  # Return original clip if adding subtitles fails\n\n# 18. Function to preview storyboard as a slideshow\ndef preview_storyboard_slideshow(scenes, duration_per_scene=5):\n    try:\n        slides = [create_animated_text(scene['title'], duration=duration_per_scene) for scene in scenes]\n        slideshow = mpe.concatenate_videoclips(slides, method='compose')\n        slideshow.write_videofile(\"storyboard_preview.mp4\", codec='libx264')\n        st.success(\"Storyboard preview created successfully.\")\n        st.video(\"storyboard_preview.mp4\")\n    except Exception as e:\n        st.error(f\"Error creating storyboard slideshow: {e}\")\n\n# 19. Function to add logo to video\ndef add_logo_to_video(clip, logo_path, position=('right', 'top')):\n    try:\n        logo = mpe.ImageClip(logo_path).set_duration(clip.duration).resize(height=100).set_position(position)\n        return mpe.CompositeVideoClip([clip, logo])\n    except FileNotFoundError:\n        st.error(f\"Logo file not found: {logo_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error adding logo to video: {e}\")\n        return clip  # Return original clip if adding logo fails\n\n# 20. Function to compress video output for faster uploading\ndef compress_video(input_path, output_path=\"compressed_video.mp4\", bitrate=\"500k\"):\n    try:\n        os.system(f\"ffmpeg -i {input_path} -b:v {bitrate} -bufsize {bitrate} {output_path}\")\n        st.success(f\"Video compressed successfully. Saved to {output_path}\")\n    except Exception as e:\n        st.error(f\"Error compressing video: {e}\")\n\n# 21. Function to apply black-and-white filter to video\ndef apply_bw_filter(clip):\n    try:\n        return clip.fx(mpe.vfx.blackwhite)\n    except Exception as e:\n        st.error(f\"Error applying black-and-white filter: {e}\")\n        return clip  # Return original clip if filter fails\n\n# 23. Function to overlay images on video\ndef overlay_image_on_video(clip, image_path, position=(0, 0)):\n    try:\n        image = mpe.ImageClip(image_path).set_duration(clip.duration).set_position(position)\n        return mpe.CompositeVideoClip([clip, image])\n    except FileNotFoundError:\n        st.error(f\"Image file not found: {image_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error overlaying image on video: {e}\")\n        return clip  # Return original clip if overlay fails\n\n# 24. Function to adjust video speed\ndef adjust_video_speed(clip, speed=1.0):\n    try:\n        return clip.fx(mpe.vfx.speedx, speed)\n    except Exception as e:\n        st.error(f\"Error adjusting video speed: {e}\")\n        return clip  # Return original clip if speed adjustment fails\n\n# 25. Function to crop video clips\ndef crop_video(clip, x1, y1, x2, y2):\n    try:\n        return clip.crop(x1=x1, y1=y1, x2=x2, y2=y2)\n    except Exception as e:\n        st.error(f\"Error cropping video: {e}\")\n        return clip  # Return original clip if cropping fails\n\n# 26. Function to adjust resolution dynamically based on system capacity\ndef adjust_resolution_based_on_system(clip):\n    try:\n        memory = psutil.virtual_memory()\n        resolution = (640, 360) if memory.available < 1000 * 1024 * 1024 else (1280, 720)\n        return resize(clip, newsize=resolution)\n    except Exception as e:\n        st.error(f\"Error adjusting resolution: {e}\")\n        return clip  # Return original clip if resolution adjustment fails\n\n# 27. Function to generate video thumbnail\ndef generate_video_thumbnail(clip, output_path=\"thumbnail.png\"):\n    try:\n        frame = clip.get_frame(1)\n        image = Image.fromarray(frame)\n        image.save(output_path)\n        st.success(f\"Thumbnail generated successfully. Saved to {output_path}\")\n        return output_path\n    except Exception as e:\n        st.error(f\"Error generating video thumbnail: {e}\")\n        return None\n\n# 29. Function to add intro and outro sequences to video\ndef add_intro_outro(final_clip, video_title):\n    intro_clip = create_animated_text(video_title, duration=3, font_size=60).fx(vfx.fadeout, duration=1)\n    outro_clip = create_animated_text(\"Thanks for Watching!\", duration=3, font_size=60).fx(vfx.fadein, duration=1)\n    return mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\n\n# 30. Function to adjust audio volume levels\ndef adjust_audio_volume(audio_clip, volume_level=1.0):\n    try:\n        return audio_clip.volumex(volume_level)\n    except Exception as e:\n        st.error(f\"Error adjusting audio volume: {e}\")\n        return audio_clip  # Return original audio clip if volume adjustment fails\n\n# 31. Function to generate a text overlay with gradient background\ndef generate_gradient_text_overlay(text, clip_duration, size=(1920, 1080)):\n    try:\n        gradient = color_gradient(size, p1=(0, 0), p2=(size[0], size[1]), color1=(255, 0, 0), color2=(0, 0, 255))\n        image = Image.fromarray(gradient)\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default()\n        text_size = draw.textsize(text, font=font)\n        draw.text(((size[0] - text_size[0]) / 2, (size[1] - text_size[1]) / 2), text, font=font, fill=(255, 255, 255))\n        image.save(\"gradient_overlay.png\")\n        return mpe.ImageClip(\"gradient_overlay.png\").set_duration(clip_duration)\n    except Exception as e:\n        st.error(f\"Error generating gradient text overlay: {e}\")\n        return mpe.TextClip(text, fontsize=70, color='white', size=size).set_duration(clip_duration)\n\n# 32. Function to run video rendering in a separate thread\ndef run_video_rendering_thread(target_function, *args):\n    try:\n        rendering_thread = threading.Thread(target=target_function, args=args)\n        rendering_thread.start()\n        return rendering_thread\n    except Exception as e:\n        st.error(f\"Error running rendering thread: {e}\")\n        return None\n\n# 33. Function to check system capabilities before rendering\ndef check_system_capabilities():\n    try:\n        memory = psutil.virtual_memory()\n        if memory.available < 500 * 1024 * 1024:  # Less than 500MB\n            st.warning(\"Low memory detected. Consider closing other applications.\")\n        cpu_usage = psutil.cpu_percent()\n        if cpu_usage > 80:\n            st.warning(\"High CPU usage detected. Rendering may be slow.\")\n    except Exception as e:\n        st.error(f\"Error checking system capabilities: {e}\")\n\n# 34. Function to log system resources during video generation\ndef log_system_resources():\n    try:\n        memory = psutil.virtual_memory()\n        cpu = psutil.cpu_percent()\n        st.write(f\"Memory Usage: {memory.percent}% | CPU Usage: {cpu}%\")\n    except Exception as e:\n        st.error(f\"Error logging system resources: {e}\")\n\n# 35. Function to download additional video assets (e.g., background music)\ndef download_additional_assets(url):\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            temp_asset_file = NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_asset_file.write(response.content)\n            temp_asset_file.flush()\n            st.success(f\"Asset downloaded successfully: {temp_asset_file.name}\")\n            return temp_asset_file.name\n        else:\n            st.error(\"Failed to download asset. Invalid URL or server error.\")\n            return None\n    except Exception as e:\n        st.error(f\"Error downloading asset: {e}\")\n        return None\n\n# 36. Function to calculate estimated video rendering time\ndef calculate_estimated_render_time(duration, resolution=(1280, 720)):\n    try:\n        estimated_time = duration * (resolution[0] * resolution[1]) / 1e6\n        st.info(f\"Estimated rendering time: {estimated_time:.2f} seconds\")\n        return estimated_time\n    except Exception as e:\n        st.error(f\"Error calculating render time: {e}\")\n        return None\n\n# 37. Function to manage temporary directories\ndef manage_temp_directory(directory_path):\n    try:\n        if os.path.exists(directory_path):\n            shutil.rmtree(directory_path)\n        os.makedirs(directory_path)\n        st.success(f\"Temporary directory created: {directory_path}\")\n    except Exception as e:\n        st.error(f\"Error managing temporary directory: {e}\")\n\n# 38. Function to handle session expiration or token errors\ndef handle_session_expiration():\n    try:\n        st.error(\"Session expired. Please refresh and try again.\")\n        if st.button(\"Refresh Page\"):\n            st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n    except Exception as e:\n        st.error(f\"Error handling session expiration: {e}\")\n\n# 39. Function to split storyboard scenes for easy preview\ndef split_storyboard_scenes(scenes, batch_size=5):\n    try:\n        return [scenes[i:i + batch_size] for i in range(0, len(scenes), batch_size)]\n    except Exception as e:\n        st.error(f\"Error splitting storyboard scenes: {e}\")\n        return [scenes]  # Return all scenes in one batch if splitting fails\n\n# 40. Function to add transition effects between storyboard scenes\ndef add_transition_effects_between_scenes(scenes):\n    try:\n        return [animate_scene_transition(scene1, scene2) for scene1, scene2 in zip(scenes, scenes[1:])]\n    except Exception as e:\n        st.error(f\"Error adding transition effects: {e}\")\n        return scenes  # Return original scenes if adding transitions fails\n\n# 41. Function to optimize storyboard scene text prompts\ndef optimize_storyboard_text_prompts(scenes):\n    try:\n        for scene in scenes:\n            scene['title'] = scene['title'].capitalize()\n        return scenes\n    except Exception as e:\n        st.error(f\"Error optimizing storyboard text prompts: {e}\")\n        return scenes  # Return original scenes if optimization fails\n\n# Update the select_background_music function\ndef select_background_music(genre):\n    try:\n        if genre in MUSIC_TRACKS:\n            track_url = MUSIC_TRACKS[genre]\n        else:\n            track_url = random.choice(list(MUSIC_TRACKS.values()))\n        \n        response = requests.get(track_url)\n        if response.status_code == 200:\n            temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_audio_file.write(response.content)\n            temp_audio_file.close()\n            return temp_audio_file.name\n    except Exception as e:\n        logger.error(f\"Error selecting background music: {str(e)}\")\n    \n    return None  # Return None if no music could be selected\n\ndef analyze_script(script):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Analyze the script and provide a JSON response with keys: 'sentiment' (POSITIVE, NEGATIVE, or NEUTRAL), 'style' (e.g., formal, casual, humorous), and 'transitions' (list of transition types).\"},\n                {\"role\": \"user\", \"content\": f\"Analyze this script: {script}\"}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        analysis = json.loads(response.choices[0].message.content)\n        \n        if not all(key in analysis for key in ['sentiment', 'style', 'transitions']):\n            raise ValueError(\"Invalid analysis structure\")\n        \n        return analysis\n    except Exception as e:\n        logger.error(f\"Error analyzing script: {str(e)}\")\n        return {\n            'sentiment': 'NEUTRAL',\n            'style': 'formal',\n            'transitions': ['fade', 'cut']\n        }\n\ndef apply_transition(clip1, clip2, transition_type):\n    transition_functions = {\n        'fade': lambda: clip1.crossfadeout(1).crossfadein(1),\n        'slide': lambda: clip1.slide_out(1, 'left').slide_in(1, 'right'),\n        'whip': lambda: clip1.fx(vfx.speedx, 2).fx(vfx.crop, x1=0, y1=0, x2=0.5, y2=1).crossfadeout(0.5),\n        'zoom': lambda: clip1.fx(vfx.resize, 1.5).fx(vfx.crop, x_center=0.5, y_center=0.5, width=1/1.5, height=1/1.5).crossfadeout(1)\n    }\n    return transition_functions.get(transition_type, lambda: clip1)()\n\ndef cleanup_temp_files():\n    try:\n        temp_dir = tempfile.gettempdir()\n        for filename in os.listdir(temp_dir):\n            if filename.startswith('videocreator_'):\n                file_path = os.path.join(temp_dir, filename)\n                if os.path.isfile(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n        logger.info(\"Temporary files cleaned up successfully.\")\n    except Exception as e:\n        logger.error(f\"Error cleaning up temporary files: {str(e)}\")\n\ndef process_scene_with_progress(scene, index, total_scenes):\n    scene_progress = st.empty()\n    scene_progress.text(f\"Processing scene {index + 1} of {total_scenes}: {scene['title']}\")\n    \n    clip_progress, voice_progress = st.columns(2)\n    \n    with clip_progress:\n        st.text(\"Creating video clip...\")\n        clip = create_fallback_clip(scene)\n        st.success(\"Video clip processed\")\n    \n    with voice_progress:\n        st.text(\"Generating voiceover...\")\n        narration_file = generate_voiceover(scene['narration'])\n        st.success(\"Voiceover generated\")\n    \n    scene_progress.success(f\"Scene {index + 1} processed successfully!\")\n    return {'clip': clip, 'scene': scene, 'narration': narration_file}\n\ndef generate_valid_storyboard(prompt, style, max_attempts=3):\n    for attempt in range(max_attempts):\n        storyboard = generate_storyboard(prompt, style)\n        if storyboard is not None:\n            return storyboard\n        logger.warning(f\"Storyboard generation attempt {attempt + 1} failed. Retrying...\")\n    logger.error(\"Failed to generate a valid storyboard after multiple attempts.\")\n    st.error(\"Failed to generate a valid storyboard after multiple attempts. Please try again with a different prompt or style.\")\n    return None\n\ndef prompt_card(prompt):\n    st.markdown(f\"**Sample Prompt:** {prompt}\")\n    if st.button(\"Use this prompt\", key=f\"btn_{prompt}\"):\n        st.session_state.prompt = prompt\n\ndef predict_processing_issues(video_clips, system_resources):\n    potential_issues = []\n    if len(video_clips) * 5 > system_resources['available_memory'] / 1e6:  # Assuming 5 seconds per clip\n        potential_issues.append(\"Insufficient memory for processing all clips\")\n    if system_resources['cpu_usage'] > 80:\n        potential_issues.append(\"High CPU usage may slow down processing\")\n    return potential_issues\n\ndef generate_script(prompt, duration):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert video scriptwriter. Create a JSON script for a video based on the given prompt and duration. Use the following schema:\"},\n                {\"role\": \"user\", \"content\": f\"\"\"\n                Create a script for a {duration} second video about: {prompt}. \n                Return the script as a JSON object with the following structure:\n                {{\n                    \"title\": \"Overall video title\",\n                    \"scenes\": [\n                        {{\n                            \"scene_number\": 1,\n                            \"title\": \"Scene title\",\n                            \"description\": \"Detailed scene description\",\n                            \"narration\": \"Narration text for the scene\",\n                            \"duration\": \"Duration in seconds\"\n                        }}\n                    ]\n                }}\n                Include 3-5 scenes in total.\n                \"\"\"}\n            ],\n            max_tokens=1000,\n            response_format={\"type\": \"json_object\"}\n        )\n        script = json.loads(response.choices[0].message.content)\n        logger.info(f\"Generated script: {script}\")\n        return script\n    except Exception as e:\n        logger.error(f\"Error generating script: {str(e)}\")\n        return None\n\ndef fetch_video_clips(scenes):\n    logger.info(f\"Fetching video clips for {len(scenes)} scenes\")\n    video_clips = []\n    \n    api = HfApi()\n    \n    for i, scene in enumerate(scenes):\n        logger.info(f\"Fetching clip for scene {i+1}: {scene['title']}\")\n        \n        # Expand the search query to include more relevant terms\n        keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\n        search_query = \" OR \".join(keywords)\n        \n        try:\n            search_results = api.list_datasets(search=search_query, limit=20)  # Increase the limit\n            matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\n            \n            if matching_datasets:\n                chosen_dataset = random.choice(matching_datasets)\n                dataset_info = api.dataset_info(chosen_dataset.id)\n                \n                if dataset_info.card_data and 'samples' in dataset_info.card_data:\n                    sample = random.choice(dataset_info.card_data['samples'])\n                    if 'video' in sample:\n                        video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\n                        clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))  # Limit to 10 seconds max\n                        logger.info(f\"Clip fetched for scene {i+1}: duration={clip.duration}s\")\n                    else:\n                        raise ValueError(\"No video found in the sample\")\n                else:\n                    raise ValueError(\"No samples found in the dataset\")\n            else:\n                raise ValueError(\"No matching datasets found\")\n        \n        except Exception as e:\n            logger.warning(f\"Error fetching video for scene {i+1}: {str(e)}. Creating fallback clip.\")\n            clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\n        \n        video_clips.append({'clip': clip, 'scene': scene})\n    \n    return video_clips\n\ndef create_video_workflow(prompt, duration, music_style):\n    try:\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n\n        logger.info(\"Starting video creation workflow\")\n        status_text.text(\"Generating script...\")\n        script = generate_script(prompt, duration)\n        if not script:\n            raise ValueError(\"Failed to generate script\")\n        logger.info(\"Script generated successfully\")\n        progress_bar.progress(20)\n\n        scenes = script['scenes']\n        if not scenes:\n            logger.error(f\"No scenes found in the script. Script content: {script}\")\n            raise ValueError(\"No scenes found in the script\")\n        logger.info(f\"Successfully extracted {len(scenes)} scenes\")\n        progress_bar.progress(30)\n\n        status_text.text(\"Fetching video clips...\")\n        video_clips = fetch_video_clips(scenes)\n        if not video_clips:\n            raise ValueError(\"Failed to fetch video clips\")\n        progress_bar.progress(50)\n\n        status_text.text(\"Generating voiceovers...\")\n        for clip_data in video_clips:\n            clip_data['narration'] = generate_voiceover(clip_data['scene']['narration'])\n        progress_bar.progress(70)\n\n        status_text.text(\"Selecting background music...\")\n        background_music_file = select_background_music(music_style)\n        progress_bar.progress(80)\n\n        status_text.text(\"Creating your video...\")\n        video_file = create_video(video_clips, background_music_file, script['title'])\n        progress_bar.progress(90)\n\n        if video_file and os.path.exists(video_file):\n            status_text.text(\"Finalizing...\")\n            progress_bar.progress(100)\n            logger.info(\"Video creation successful\")\n            st.success(\"\ud83c\udf89 Video created successfully!\")\n            st.video(video_file)\n            with open(video_file, 'rb') as vf:\n                st.download_button(label=\"\ud83d\udce5 Download Video\", data=vf, file_name=\"AutovideoAI_creation.mp4\")\n        else:\n            logger.error(\"Failed to create the video\")\n            st.error(\"\u274c Failed to create the video. Please try again.\")\n    except Exception as e:\n        logger.error(f\"An error occurred during video creation: {str(e)}\")\n        st.error(f\"An error occurred: {str(e)}\")\n    finally:\n        status_text.empty()\n        progress_bar.empty()\n\ndef main():\n    st.markdown(\"<h1 style='text-align: center; color: #4A90E2;'>AutovideoAI</h1>\", unsafe_allow_html=True)\n    st.markdown(\"<p style='text-align: center; font-size: 1.2em;'>Create Amazing Videos with AI</p>\", unsafe_allow_html=True)\n\n    with st.expander(\"\u2139\ufe0f How to use AutovideoAI\", expanded=False):\n        st.markdown(\"\"\"\n        1. Enter your video idea or choose a sample prompt.\n        2. Customize your video style, duration, and background music.\n        3. Generate a storyboard and preview it.\n        4. Create your AI-powered video!\n        \"\"\")\n\n    col1, col2 = st.columns([2, 1])\n    \n    with col1:\n        st.subheader(\"1\ufe0f\u20e3 Enter Your Video Idea\")\n        prompt = st.text_area(\"What's your video about?\", height=100, value=st.session_state.get('prompt', ''))\n    \n    with col2:\n        st.subheader(\"Sample Prompts\")\n        for sample_prompt in SAMPLE_PROMPTS:\n            if st.button(f\"\ud83d\udccc {sample_prompt}\", key=f\"btn_{sample_prompt}\"):\n                st.session_state.prompt = sample_prompt\n                st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n\n    st.subheader(\"2\ufe0f\u20e3 Customize Your Video\")\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        style = st.selectbox(\"Video Style \ud83c\udfad\", [\"Motivational\", \"Dramatic\", \"Educational\", \"Funny\"])\n    with col2:\n        duration = st.slider(\"Estimated Duration \u23f1\ufe0f\", 30, 300, 60, help=\"Duration in seconds\")\n    with col3:\n        music_style = st.selectbox(\"Background Music \ud83c\udfb5\", list(MUSIC_TRACKS.keys()))\n\n    if st.button(\"\ud83d\udd8b\ufe0f Generate Storyboard\", use_container_width=True):\n        with st.spinner(\"Crafting your storyboard...\"):\n            storyboard = generate_script(prompt, duration)\n        if storyboard:\n            st.session_state.storyboard = storyboard\n            st.success(\"\u2705 Storyboard generated successfully!\")\n            display_storyboard_preview(storyboard)\n        else:\n            st.error(\"\u274c Failed to generate a storyboard. Please try again.\")\n\n    if 'storyboard' in st.session_state:\n        if st.button(\"\ud83c\udfac Create Video\", use_container_width=True):\n            create_video_workflow(prompt, duration, music_style)\n\ndef display_storyboard_preview(storyboard):\n    with st.expander(\"\ud83d\udd0d Preview Storyboard\", expanded=True):\n        st.markdown(f\"### {storyboard['title']}\")\n        st.markdown(\"### Scene Breakdown\")\n        for scene in storyboard['scenes']:\n            with st.container():\n                col1, col2 = st.columns([1, 2])\n                with col1:\n                    st.markdown(f\"**Scene {scene['scene_number']}**\")\n                    st.write(f\"Duration: {scene['duration']} seconds\")\n                with col2:\n                    st.markdown(f\"**{scene['title']}**\")\n                    st.write(f\"{scene['description']}\")\n                st.markdown(\"---\")\n\n# Add this function definition near the top of the file, after the imports\ndef color_gradient(size, p1, p2, color1, color2):\n    x = np.linspace(0, 1, size[0])[:, None]\n    y = np.linspace(0, 1, size[1])[None, :]\n    gradient = x * (p2[0] - p1[0]) + y * (p2[1] - p1[1])\n    gradient = np.clip(gradient, 0, 1)\n    return np.array(color1) * (1 - gradient[:, :, None]) + np.array(color2) * gradient[:, :, None]\n\nif __name__ == \"__main__\":\n    main()", "ace_editor": "import streamlit as st\nfrom openai import OpenAI\nimport os\nimport moviepy.editor as mpe\nimport requests\nfrom tempfile import NamedTemporaryFile\nfrom moviepy.video.fx.all import fadein, fadeout, resize\nimport psutil\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport shutil\nimport logging\nfrom gtts import gTTS\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment\nimport random\nimport moviepy.video.fx.all as vfx\nimport cv2\nfrom pydub.silence import split_on_silence\nimport numpy as np\nimport tempfile\nfrom functools import lru_cache\nfrom huggingface_hub import InferenceClient, hf_hub_download\nfrom huggingface_hub import login, HfApi\nfrom datasets import load_dataset\nimport textwrap\nfrom scenedetect import detect, ContentDetector\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\nimport re\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Set page config at the very beginning of the script\nst.set_page_config(page_title=\"AutovideoAI\", page_icon=\"\ud83c\udfa5\", layout=\"wide\")\n\n# Load environment variables\nload_dotenv()\nhf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n\n# Check if already logged in\napi = HfApi()\ntry:\n    api.whoami(token=hf_token)\n    logger.info(\"Already logged in to Hugging Face\")\nexcept Exception:\n    logger.info(\"Logging in to Hugging Face\")\n    login(token=hf_token, add_to_git_credential=False)\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Theme customization\nif 'theme' not in st.session_state:\n    st.session_state.theme = 'light'\n\ndef toggle_theme():\n    st.session_state.theme = 'dark' if st.session_state.theme == 'light' else 'light'\n\n# Apply theme\nif st.session_state.theme == 'dark':\n    st.markdown(\"\"\"\n    <style>\n    .stApp {\n        background-color: #1E1E1E;\n        color: white;\n    }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n\n# Sample prompts\nSAMPLE_PROMPTS = [\n    \"Create a motivational video about overcoming challenges\",\n    \"Make an educational video explaining photosynthesis\",\n    \"Design a funny video about the struggles of working from home\",\n]\n\n# Replace the MUSIC_GENRES list with a dictionary of genre-specific tracks\nMUSIC_TRACKS = {\n    \"Electronic\": \"https://www.bensound.com/bensound-dubstep\",\n    \"Experimental\": \"https://www.bensound.com/bensound-enigmatic\",\n    \"Folk\": \"https://www.bensound.com/bensound-acousticbreeze\",\n    \"Hip-Hop\": \"https://www.bensound.com/bensound-groovyhiphop\",\n    \"Instrumental\": \"https://www.bensound.com/bensound-pianomoment\",\n    \"Pop\": \"https://www.bensound.com/bensound-ukulele\",\n    \"Rock\": \"https://www.bensound.com/bensound-extremeaction\"\n}\n\n# Set up caching for downloaded assets\n@lru_cache(maxsize=100)\ndef cached_download(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.content\n    return None\n\n# 1. Function to generate storyboard based on user prompt using structured JSON\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef generate_storyboard(prompt, style=\"motivational\"):\n    try:\n        input_text = f\"\"\"Generate a detailed {style} video storyboard based on this prompt: \"{prompt}\"\n        Provide the storyboard in JSON format with the following structure:\n        {{\n            \"title\": \"Overall video title\",\n            \"scenes\": [\n                {{\n                    \"scene_number\": 1,\n                    \"title\": \"Scene title\",\n                    \"description\": \"Detailed scene description\",\n                    \"narration\": \"Narration text for the scene\",\n                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n                    \"duration\": \"Duration in seconds\",\n                    \"visual_elements\": [\"List of visual elements to include\"],\n                    \"transitions\": {{\n                        \"in\": \"Transition type for entering the scene\",\n                        \"out\": \"Transition type for exiting the scene\"\n                    }}\n                }}\n            ],\n            \"target_audience\": \"Description of the target audience\",\n            \"overall_tone\": \"Description of the overall tone of the video\"\n        }}\n        Ensure there are at least 3 scenes in the storyboard.\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative video storyboard generator. Respond with valid JSON following the specified structure.\"},\n                {\"role\": \"user\", \"content\": input_text}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        storyboard = json.loads(response.choices[0].message.content)\n        return storyboard\n    except Exception as e:\n        logger.error(f\"Error generating storyboard: {str(e)}\")\n        st.error(\"An error occurred while generating the storyboard. Please try again.\")\n    return None\n\ndef validate_storyboard(storyboard):\n    if \"title\" not in storyboard or \"scenes\" not in storyboard or not isinstance(storyboard[\"scenes\"], list):\n        return False\n    if len(storyboard[\"scenes\"]) < 3:\n        return False\n    \n    required_fields = [\"scene_number\", \"title\", \"description\", \"narration\", \"keywords\", \"duration\", \"overlay_text\", \"visual_elements\", \"audio_cues\", \"transitions\"]\n    return all(all(field in scene for field in required_fields) for scene in storyboard[\"scenes\"])\n\n# 2. Function to parse structured JSON storyboard data\ndef parse_storyboard(storyboard):\n    try:\n        return json.loads(storyboard).get(\"scenes\", [])\n    except json.JSONDecodeError:\n        return []\n\n# 3. Function to fetch video clips dynamically based on scene keywords\ndef fetch_video_clips(scenes, max_retries=3):\n    logger.info(f\"Fetching video clips for {len(scenes)} scenes\")\n    video_clips = []\n    \n    api = HfApi()\n    \n    for i, scene in enumerate(scenes):\n        logger.info(f\"Fetching clip for scene {i+1}: {scene['title']}\")\n        \n        for attempt in range(max_retries):\n            try:\n                # Expand the search query to include more relevant terms\n                keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\n                search_query = \" OR \".join(keywords)\n                \n                search_results = api.list_datasets(search=search_query, limit=20)  # Increase the limit\n                matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\n                \n                if matching_datasets:\n                    chosen_dataset = random.choice(matching_datasets)\n                    dataset_info = api.dataset_info(chosen_dataset.id)\n                    \n                    if dataset_info.card_data and 'samples' in dataset_info.card_data:\n                        sample = random.choice(dataset_info.card_data['samples'])\n                        if 'video' in sample:\n                            video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\n                            clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))  # Limit to 10 seconds max\n                            logger.info(f\"Clip fetched for scene {i+1}: duration={clip.duration}s\")\n                        else:\n                            raise ValueError(\"No video found in the sample\")\n                    else:\n                        raise ValueError(\"No samples found in the dataset\")\n                else:\n                    raise ValueError(\"No matching datasets found\")\n                \n                if clip:\n                    break  # Successfully fetched a clip, exit the retry loop\n            except Exception as e:\n                logger.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n                if attempt == max_retries - 1:\n                    logger.warning(f\"All attempts failed. Creating fallback clip for scene {i+1}.\")\n                    clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\n        \n        video_clips.append({'clip': clip, 'scene': scene})\n    \n    return video_clips\n\ndef create_fallback_clip(scene, duration=5):\n    text = scene.get('title', 'Scene')\n    size = (1280, 720)\n    \n    # Create a black background\n    img = Image.new('RGB', size, color='black')\n    draw = ImageDraw.Draw(img)\n    \n    # Use a default font\n    font = ImageFont.load_default()\n    \n    # Wrap text\n    wrapped_text = textwrap.wrap(text, width=20)\n    \n    # Calculate text position\n    y_text = (size[1] - len(wrapped_text) * 80) // 2\n    \n    # Draw text\n    for line in wrapped_text:\n        line_width, line_height = draw.textbbox((0, 0), line, font=font)[2:]\n        position = ((size[0] - line_width) / 2, y_text)\n        draw.text(position, line, font=font, fill='white')\n        y_text += line_height + 10\n    \n    # Convert PIL Image to numpy array\n    img_array = np.array(img)\n    \n    # Create video clip from the image\n    clip = mpe.ImageClip(img_array).set_duration(duration)\n    \n    return clip\n\n# 4. Function to generate voiceover with Hugging Face Inference API\ndef generate_voiceover(narration_text):\n    logger.info(f\"Generating voiceover for text: {narration_text[:50]}...\")\n    try:\n        tts = gTTS(text=narration_text, lang='en', slow=False)\n        temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n        tts.save(temp_audio_file.name)\n        return mpe.AudioFileClip(temp_audio_file.name)\n    except Exception as e:\n        logger.error(f\"Error generating voiceover: {str(e)}\")\n        return create_silent_audio(len(narration_text.split()) / 2)  # Assuming 2 words per second\n\ndef create_silent_audio(duration):\n    silent_segment = AudioSegment.silent(duration=int(duration * 1000))  # pydub uses milliseconds\n    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n    silent_segment.export(temp_audio_file.name, format=\"mp3\")\n    return mpe.AudioFileClip(temp_audio_file.name)\n\ndef create_scene_clip(scene, duration=5):\n    try:\n        # Create a gradient background\n        gradient = np.linspace(0, 255, 1280)\n        background = np.tile(gradient, (720, 1)).astype(np.uint8)\n        background = np.stack((background,) * 3, axis=-1)\n        \n        # Create video clip from the background\n        clip = mpe.ImageClip(background).set_duration(duration)\n        \n        # Add text\n        txt_clip = mpe.TextClip(scene['title'], fontsize=70, color='white', font='Arial-Bold')\n        txt_clip = txt_clip.set_position('center').set_duration(duration)\n        \n        # Add keywords as subtitles\n        if 'keywords' in scene:\n            keywords_txt = \", \".join(scene['keywords'])\n            keywords_clip = mpe.TextClip(keywords_txt, fontsize=30, color='yellow', font='Arial')\n            keywords_clip = keywords_clip.set_position(('center', 0.8), relative=True).set_duration(duration)\n            clip = mpe.CompositeVideoClip([clip, txt_clip, keywords_clip])\n        else:\n            clip = mpe.CompositeVideoClip([clip, txt_clip])\n        \n        # Add fade in and out\n        clip = clip.fadein(0.5).fadeout(0.5)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating scene clip: {str(e)}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)\n\n# Add this function for smooth transitions\ndef add_fade_transition(clip1, clip2, duration=1):\n    return mpe.CompositeVideoClip([clip1.crossfadeout(duration), clip2.crossfadein(duration)])\n\n# Add this function for dynamic text animations\ndef create_animated_text(text, duration=5, font_size=70, color='white'):\n    try:\n        # Create a black background image\n        img = Image.new('RGB', (1280, 720), color='black')\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=font_size)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position to center the text\n        position = ((1280 - text_width) / 2, (720 - text_height) / 2)\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill=color)\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        clip = mpe.ImageClip(img_array).set_duration(duration)\n        \n        # Add fade in and fade out effects\n        clip = clip.fadein(1).fadeout(1)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating animated text: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)\n\n# Add this function for color grading\ndef apply_color_grading(clip, brightness=1.0, contrast=1.0, saturation=1.0):\n    return clip.fx(vfx.colorx, brightness).fx(vfx.lum_contrast, contrast=contrast).fx(vfx.colorx, saturation)\n\n# Add this function for creating lower thirds\ndef create_lower_third(text, duration):\n    try:\n        # Create a transparent background\n        img = Image.new('RGBA', (1280, 720), (0, 0, 0, 0))\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=30)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position for lower third\n        position = ((1280 - text_width) / 2, 720 - text_height - 50)\n        \n        # Draw semi-transparent background\n        bg_bbox = (position[0]-10, position[1]-10, position[0]+text_width+10, position[1]+text_height+10)\n        draw.rectangle(bg_bbox, fill=(0,0,0,153))\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill='white')\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        return mpe.ImageClip(img_array).set_duration(duration)\n    except Exception as e:\n        logger.error(f\"Error creating lower third: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0,0)).set_duration(duration)\n\ndef smart_cut(video_clip, audio_clip):\n    try:\n        # Check if audio_clip is a file path or an AudioClip object\n        if isinstance(audio_clip, str):\n            audio_segment = AudioSegment.from_wav(audio_clip)\n        elif isinstance(audio_clip, mpe.AudioClip):\n            # Convert AudioClip to numpy array\n            audio_array = audio_clip.to_soundarray()\n            audio_segment = AudioSegment(\n                audio_array.tobytes(),\n                frame_rate=audio_clip.fps,\n                sample_width=audio_array.dtype.itemsize,\n                channels=1 if audio_array.ndim == 1 else audio_array.shape[1]\n            )\n        else:\n            logger.warning(\"Unsupported audio type. Returning original video clip.\")\n            return video_clip\n\n        # Split audio on silences\n        chunks = split_on_silence(audio_segment, min_silence_len=500, silence_thresh=-40)\n        \n        # Calculate timestamps for cuts\n        cut_times = [0]\n        for chunk in chunks:\n            cut_times.append(cut_times[-1] + len(chunk) / 1000)\n        \n        # Cut video based on audio\n        cut_clips = [video_clip.subclip(start, end) for start, end in zip(cut_times[:-1], cut_times[1:])]\n        \n        return mpe.concatenate_videoclips(cut_clips)\n    except Exception as e:\n        logger.error(f\"Error in smart_cut: {str(e)}\")\n        return video_clip\n\ndef apply_speed_changes(clip, speed_factor=1.5, threshold=0.1):\n    try:\n        if not hasattr(clip, 'fps') or clip.fps is None:\n            clip = clip.set_fps(24)  # Set a default fps if not present\n        \n        frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in clip.iter_frames()]\n        motion = [np.mean(cv2.absdiff(frames[i], frames[i+1])) for i in range(len(frames)-1)]\n        \n        speed_clip = clip.fl(lambda gf, t: gf(t * speed_factor) if motion[int(t*clip.fps)] < threshold else gf(t))\n        \n        return speed_clip\n    except Exception as e:\n        logger.error(f\"Error in apply_speed_changes: {str(e)}\")\n        return clip\n\n# 7. Function to create and finalize the video\ndef create_video(video_clips, background_music_file, video_title):\n    logger.info(f\"Starting video creation process for '{video_title}'\")\n    try:\n        clips = []\n        for i, clip_data in enumerate(video_clips):\n            try:\n                clip = clip_data['clip']\n                narration = clip_data['narration']\n                clip = clip.set_audio(narration)\n                clips.append(clip)\n                logger.info(f\"Processed clip {i+1} successfully\")\n            except Exception as e:\n                logger.error(f\"Error processing clip {i+1}: {str(e)}\")\n        \n        if not clips:\n            raise ValueError(\"No valid clips were created\")\n        \n        logger.info(f\"Concatenating {len(clips)} scene clips\")\n        final_clip = mpe.concatenate_videoclips(clips)\n        \n        if background_music_file:\n            logger.info(\"Adding background music\")\n            background_music = mpe.AudioFileClip(background_music_file).volumex(0.1)\n            background_music = background_music.audio_loop(duration=final_clip.duration)\n            final_audio = mpe.CompositeAudioClip([final_clip.audio, background_music])\n            final_clip = final_clip.set_audio(final_audio)\n        \n        # Add intro and outro\n        intro_clip = mpe.TextClip(video_title, fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        outro_clip = mpe.TextClip(\"Thanks for watching!\", fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        final_clip = mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\n        \n        output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name\n        logger.info(f\"Writing final video to {output_file}\")\n        final_clip.write_videofile(output_file, codec='libx264', audio_codec='aac', fps=24)\n        logger.info(\"Video creation process completed\")\n        return output_file\n    except Exception as e:\n        logger.error(f\"Error in create_video: {str(e)}\")\n        st.error(f\"An error occurred while creating the video: {str(e)}\")\n        return None\n\ndef enhance_clip(clip, script_analysis):\n    # Apply color grading based on sentiment\n    if script_analysis['sentiment'] == 'POSITIVE':\n        clip = apply_color_grading(clip, brightness=1.1, saturation=1.2)\n    elif script_analysis['sentiment'] == 'NEGATIVE':\n        clip = apply_color_grading(clip, brightness=0.9, contrast=1.1)\n    \n    # Add dynamic text animations\n    if clip.duration > 2:\n        text_clip = create_animated_text(clip.scene['title'], duration=2)\n        clip = mpe.CompositeVideoClip([clip, text_clip.set_start(1)])\n    \n    # Add smooth transitions\n    clip = clip.crossfadein(0.5).crossfadeout(0.5)\n    \n    return clip\n\ndef process_clip(enhanced_clip, clip_data, script_analysis):\n    scene = clip_data['scene']\n    duration = float(scene['duration'])\n    processed_clip = enhanced_clip.set_duration(duration)\n    \n    # Apply color grading based on sentiment and style\n    if script_analysis['sentiment'] == 'POSITIVE':\n        processed_clip = apply_color_grading(processed_clip, brightness=1.1)\n    else:\n        processed_clip = apply_color_grading(processed_clip, brightness=0.9)\n    \n    if script_analysis['style'] in ['humorous', 'casual']:\n        processed_clip = processed_clip.fx(vfx.colorx, 1.2)  # More vibrant for humorous/casual content\n    elif script_analysis['style'] in ['dramatic', 'formal']:\n        processed_clip = processed_clip.fx(vfx.lum_contrast, contrast=1.2)  # More contrast for dramatic/formal content\n    \n    if scene.get('overlay_text'):\n        text_clip = create_animated_text(scene['overlay_text'], duration)\n        processed_clip = mpe.CompositeVideoClip([processed_clip, text_clip])\n    \n    lower_third = create_lower_third(scene['title'], duration)\n    processed_clip = mpe.CompositeVideoClip([processed_clip, lower_third])\n    \n    return processed_clip\n\n# 8. Function to apply fade-in/fade-out effects to video clips\ndef apply_fade_effects(clip, duration=1):\n    try:\n        return fadein(clip, duration).fx(fadeout, duration)\n    except Exception as e:\n        raise ValueError(f\"Error applying fade effects: {e}\")\n\n# 9. Function to add text overlay to video clips\ndef add_text_overlay(clip, text):\n    if text:\n        try:\n            text_clip = mpe.TextClip(text, fontsize=70, color='white', font='Arial-Bold')\n            text_clip = text_clip.set_position('center').set_duration(clip.duration)\n            return mpe.CompositeVideoClip([clip, text_clip])\n        except Exception as e:\n            raise ValueError(f\"Error adding text overlay: {e}\")\n    return clip\n\n# 10. Function to add narration to video clip\ndef add_narration(clip, narration_file):\n    try:\n        return clip.set_audio(mpe.AudioFileClip(narration_file))\n    except Exception as e:\n        raise ValueError(f\"Error adding narration: {e}\")\n\n# 11. Function to add background music to video\ndef add_background_music(clip, music_file):\n    try:\n        background_audio = mpe.AudioFileClip(music_file)\n        return clip.set_audio(mpe.CompositeAudioClip([clip.audio, background_audio.volumex(0.1)]))\n    except Exception as e:\n        raise ValueError(f\"Error adding background music: {e}\")\n\n# 12. Function to add watermarks to video clips\ndef add_watermark(clip, watermark_text=\"Sample Watermark\"):\n    try:\n        watermark = mpe.TextClip(watermark_text, fontsize=30, color='white', font='Arial')\n        watermark = watermark.set_position(('right', 'bottom')).set_duration(clip.duration)\n        return mpe.CompositeVideoClip([clip, watermark])\n    except Exception as e:\n        st.error(f\"Error adding watermark: {e}\")\n        return clip  # Return original clip if watermark fails\n\n# 13. Function to split video into parts for processing\ndef split_video(video_clip, part_duration=10):\n    try:\n        return [video_clip.subclip(start, min(start + part_duration, video_clip.duration)) for start in range(0, int(video_clip.duration), part_duration)]\n    except Exception as e:\n        st.error(f\"Error splitting video: {e}\")\n        return [video_clip]  # Return original clip if splitting fails\n\n# 14. Function to merge video parts back together\ndef merge_video_parts(video_parts):\n    try:\n        return mpe.concatenate_videoclips(video_parts, method=\"compose\")\n    except Exception as e:\n        st.error(f\"Error merging video parts: {e}\")\n        return video_parts[0] if video_parts else None  # Return first part if merging fails\n\n# 15. Function to save a temporary JSON backup of generated storyboard\ndef save_storyboard_backup(storyboard, filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'w') as f:\n            json.dump(storyboard, f)\n        st.success(f\"Storyboard backup saved to {filename}\")\n    except Exception as e:\n        st.error(f\"Error saving storyboard backup: {e}\")\n\n# 16. Function to load a saved storyboard from backup\ndef load_storyboard_backup(filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'r') as f:\n            storyboard = json.load(f)\n        st.success(f\"Storyboard loaded from {filename}\")\n        return storyboard\n    except FileNotFoundError:\n        st.warning(f\"Backup file {filename} not found.\")\n        return None\n    except json.JSONDecodeError:\n        st.error(f\"Error decoding JSON from {filename}\")\n        return None\n    except Exception as e:\n        st.error(f\"Error loading storyboard backup: {e}\")\n        return None\n\n# 17. Function to add subtitles to video\ndef add_subtitles_to_video(clip, subtitles):\n    try:\n        subtitle_clips = [\n            mpe.TextClip(subtitle['text'], fontsize=50, color='white', size=clip.size, font='Arial-Bold')\n            .set_position(('bottom')).set_start(subtitle['start']).set_duration(subtitle['duration'])\n            for subtitle in subtitles\n        ]\n        return mpe.CompositeVideoClip([clip] + subtitle_clips)\n    except Exception as e:\n        st.error(f\"Error adding subtitles: {e}\")\n        return clip  # Return original clip if adding subtitles fails\n\n# 18. Function to preview storyboard as a slideshow\ndef preview_storyboard_slideshow(scenes, duration_per_scene=5):\n    try:\n        slides = [create_animated_text(scene['title'], duration=duration_per_scene) for scene in scenes]\n        slideshow = mpe.concatenate_videoclips(slides, method='compose')\n        slideshow.write_videofile(\"storyboard_preview.mp4\", codec='libx264')\n        st.success(\"Storyboard preview created successfully.\")\n        st.video(\"storyboard_preview.mp4\")\n    except Exception as e:\n        st.error(f\"Error creating storyboard slideshow: {e}\")\n\n# 19. Function to add logo to video\ndef add_logo_to_video(clip, logo_path, position=('right', 'top')):\n    try:\n        logo = mpe.ImageClip(logo_path).set_duration(clip.duration).resize(height=100).set_position(position)\n        return mpe.CompositeVideoClip([clip, logo])\n    except FileNotFoundError:\n        st.error(f\"Logo file not found: {logo_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error adding logo to video: {e}\")\n        return clip  # Return original clip if adding logo fails\n\n# 20. Function to compress video output for faster uploading\ndef compress_video(input_path, output_path=\"compressed_video.mp4\", bitrate=\"500k\"):\n    try:\n        os.system(f\"ffmpeg -i {input_path} -b:v {bitrate} -bufsize {bitrate} {output_path}\")\n        st.success(f\"Video compressed successfully. Saved to {output_path}\")\n    except Exception as e:\n        st.error(f\"Error compressing video: {e}\")\n\n# 21. Function to apply black-and-white filter to video\ndef apply_bw_filter(clip):\n    try:\n        return clip.fx(mpe.vfx.blackwhite)\n    except Exception as e:\n        st.error(f\"Error applying black-and-white filter: {e}\")\n        return clip  # Return original clip if filter fails\n\n# 23. Function to overlay images on video\ndef overlay_image_on_video(clip, image_path, position=(0, 0)):\n    try:\n        image = mpe.ImageClip(image_path).set_duration(clip.duration).set_position(position)\n        return mpe.CompositeVideoClip([clip, image])\n    except FileNotFoundError:\n        st.error(f\"Image file not found: {image_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error overlaying image on video: {e}\")\n        return clip  # Return original clip if overlay fails\n\n# 24. Function to adjust video speed\ndef adjust_video_speed(clip, speed=1.0):\n    try:\n        return clip.fx(mpe.vfx.speedx, speed)\n    except Exception as e:\n        st.error(f\"Error adjusting video speed: {e}\")\n        return clip  # Return original clip if speed adjustment fails\n\n# 25. Function to crop video clips\ndef crop_video(clip, x1, y1, x2, y2):\n    try:\n        return clip.crop(x1=x1, y1=y1, x2=x2, y2=y2)\n    except Exception as e:\n        st.error(f\"Error cropping video: {e}\")\n        return clip  # Return original clip if cropping fails\n\n# 26. Function to adjust resolution dynamically based on system capacity\ndef adjust_resolution_based_on_system(clip):\n    try:\n        memory = psutil.virtual_memory()\n        resolution = (640, 360) if memory.available < 1000 * 1024 * 1024 else (1280, 720)\n        return resize(clip, newsize=resolution)\n    except Exception as e:\n        st.error(f\"Error adjusting resolution: {e}\")\n        return clip  # Return original clip if resolution adjustment fails\n\n# 27. Function to generate video thumbnail\ndef generate_video_thumbnail(clip, output_path=\"thumbnail.png\"):\n    try:\n        frame = clip.get_frame(1)\n        image = Image.fromarray(frame)\n        image.save(output_path)\n        st.success(f\"Thumbnail generated successfully. Saved to {output_path}\")\n        return output_path\n    except Exception as e:\n        st.error(f\"Error generating video thumbnail: {e}\")\n        return None\n\n# 29. Function to add intro and outro sequences to video\ndef add_intro_outro(final_clip, video_title):\n    intro_clip = create_animated_text(video_title, duration=3, font_size=60).fx(vfx.fadeout, duration=1)\n    outro_clip = create_animated_text(\"Thanks for Watching!\", duration=3, font_size=60).fx(vfx.fadein, duration=1)\n    return mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\n\n# 30. Function to adjust audio volume levels\ndef adjust_audio_volume(audio_clip, volume_level=1.0):\n    try:\n        return audio_clip.volumex(volume_level)\n    except Exception as e:\n        st.error(f\"Error adjusting audio volume: {e}\")\n        return audio_clip  # Return original audio clip if volume adjustment fails\n\n# 31. Function to generate a text overlay with gradient background\ndef generate_gradient_text_overlay(text, clip_duration, size=(1920, 1080)):\n    try:\n        gradient = color_gradient(size, p1=(0, 0), p2=(size[0], size[1]), color1=(255, 0, 0), color2=(0, 0, 255))\n        image = Image.fromarray(gradient)\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default()\n        text_size = draw.textsize(text, font=font)\n        draw.text(((size[0] - text_size[0]) / 2, (size[1] - text_size[1]) / 2), text, font=font, fill=(255, 255, 255))\n        image.save(\"gradient_overlay.png\")\n        return mpe.ImageClip(\"gradient_overlay.png\").set_duration(clip_duration)\n    except Exception as e:\n        st.error(f\"Error generating gradient text overlay: {e}\")\n        return mpe.TextClip(text, fontsize=70, color='white', size=size).set_duration(clip_duration)\n\n# 32. Function to run video rendering in a separate thread\ndef run_video_rendering_thread(target_function, *args):\n    try:\n        rendering_thread = threading.Thread(target=target_function, args=args)\n        rendering_thread.start()\n        return rendering_thread\n    except Exception as e:\n        st.error(f\"Error running rendering thread: {e}\")\n        return None\n\n# 33. Function to check system capabilities before rendering\ndef check_system_capabilities():\n    try:\n        memory = psutil.virtual_memory()\n        if memory.available < 500 * 1024 * 1024:  # Less than 500MB\n            st.warning(\"Low memory detected. Consider closing other applications.\")\n        cpu_usage = psutil.cpu_percent()\n        if cpu_usage > 80:\n            st.warning(\"High CPU usage detected. Rendering may be slow.\")\n    except Exception as e:\n        st.error(f\"Error checking system capabilities: {e}\")\n\n# 34. Function to log system resources during video generation\ndef log_system_resources():\n    try:\n        memory = psutil.virtual_memory()\n        cpu = psutil.cpu_percent()\n        st.write(f\"Memory Usage: {memory.percent}% | CPU Usage: {cpu}%\")\n    except Exception as e:\n        st.error(f\"Error logging system resources: {e}\")\n\n# 35. Function to download additional video assets (e.g., background music)\ndef download_additional_assets(url):\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            temp_asset_file = NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_asset_file.write(response.content)\n            temp_asset_file.flush()\n            st.success(f\"Asset downloaded successfully: {temp_asset_file.name}\")\n            return temp_asset_file.name\n        else:\n            st.error(\"Failed to download asset. Invalid URL or server error.\")\n            return None\n    except Exception as e:\n        st.error(f\"Error downloading asset: {e}\")\n        return None\n\n# 36. Function to calculate estimated video rendering time\ndef calculate_estimated_render_time(duration, resolution=(1280, 720)):\n    try:\n        estimated_time = duration * (resolution[0] * resolution[1]) / 1e6\n        st.info(f\"Estimated rendering time: {estimated_time:.2f} seconds\")\n        return estimated_time\n    except Exception as e:\n        st.error(f\"Error calculating render time: {e}\")\n        return None\n\n# 37. Function to manage temporary directories\ndef manage_temp_directory(directory_path):\n    try:\n        if os.path.exists(directory_path):\n            shutil.rmtree(directory_path)\n        os.makedirs(directory_path)\n        st.success(f\"Temporary directory created: {directory_path}\")\n    except Exception as e:\n        st.error(f\"Error managing temporary directory: {e}\")\n\n# 38. Function to handle session expiration or token errors\ndef handle_session_expiration():\n    try:\n        st.error(\"Session expired. Please refresh and try again.\")\n        if st.button(\"Refresh Page\"):\n            st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n    except Exception as e:\n        st.error(f\"Error handling session expiration: {e}\")\n\n# 39. Function to split storyboard scenes for easy preview\ndef split_storyboard_scenes(scenes, batch_size=5):\n    try:\n        return [scenes[i:i + batch_size] for i in range(0, len(scenes), batch_size)]\n    except Exception as e:\n        st.error(f\"Error splitting storyboard scenes: {e}\")\n        return [scenes]  # Return all scenes in one batch if splitting fails\n\n# 40. Function to add transition effects between storyboard scenes\ndef add_transition_effects_between_scenes(scenes):\n    try:\n        return [animate_scene_transition(scene1, scene2) for scene1, scene2 in zip(scenes, scenes[1:])]\n    except Exception as e:\n        st.error(f\"Error adding transition effects: {e}\")\n        return scenes  # Return original scenes if adding transitions fails\n\n# 41. Function to optimize storyboard scene text prompts\ndef optimize_storyboard_text_prompts(scenes):\n    try:\n        for scene in scenes:\n            scene['title'] = scene['title'].capitalize()\n        return scenes\n    except Exception as e:\n        st.error(f\"Error optimizing storyboard text prompts: {e}\")\n        return scenes  # Return original scenes if optimization fails\n\n# Update the select_background_music function\ndef select_background_music(genre):\n    try:\n        if genre in MUSIC_TRACKS:\n            track_url = MUSIC_TRACKS[genre]\n        else:\n            track_url = random.choice(list(MUSIC_TRACKS.values()))\n        \n        response = requests.get(track_url)\n        if response.status_code == 200:\n            temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_audio_file.write(response.content)\n            temp_audio_file.close()\n            return temp_audio_file.name\n    except Exception as e:\n        logger.error(f\"Error selecting background music: {str(e)}\")\n    \n    return None  # Return None if no music could be selected\n\ndef analyze_script(script):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Analyze the script and provide a JSON response with keys: 'sentiment' (POSITIVE, NEGATIVE, or NEUTRAL), 'style' (e.g., formal, casual, humorous), and 'transitions' (list of transition types).\"},\n                {\"role\": \"user\", \"content\": f\"Analyze this script: {script}\"}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        analysis = json.loads(response.choices[0].message.content)\n        \n        if not all(key in analysis for key in ['sentiment', 'style', 'transitions']):\n            raise ValueError(\"Invalid analysis structure\")\n        \n        return analysis\n    except Exception as e:\n        logger.error(f\"Error analyzing script: {str(e)}\")\n        return {\n            'sentiment': 'NEUTRAL',\n            'style': 'formal',\n            'transitions': ['fade', 'cut']\n        }\n\ndef apply_transition(clip1, clip2, transition_type):\n    transition_functions = {\n        'fade': lambda: clip1.crossfadeout(1).crossfadein(1),\n        'slide': lambda: clip1.slide_out(1, 'left').slide_in(1, 'right'),\n        'whip': lambda: clip1.fx(vfx.speedx, 2).fx(vfx.crop, x1=0, y1=0, x2=0.5, y2=1).crossfadeout(0.5),\n        'zoom': lambda: clip1.fx(vfx.resize, 1.5).fx(vfx.crop, x_center=0.5, y_center=0.5, width=1/1.5, height=1/1.5).crossfadeout(1)\n    }\n    return transition_functions.get(transition_type, lambda: clip1)()\n\ndef cleanup_temp_files():\n    try:\n        temp_dir = tempfile.gettempdir()\n        for filename in os.listdir(temp_dir):\n            if filename.startswith('videocreator_'):\n                file_path = os.path.join(temp_dir, filename)\n                if os.path.isfile(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n        logger.info(\"Temporary files cleaned up successfully.\")\n    except Exception as e:\n        logger.error(f\"Error cleaning up temporary files: {str(e)}\")\n\ndef process_scene_with_progress(scene, index, total_scenes):\n    scene_progress = st.empty()\n    scene_progress.text(f\"Processing scene {index + 1} of {total_scenes}: {scene['title']}\")\n    \n    clip_progress, voice_progress = st.columns(2)\n    \n    with clip_progress:\n        st.text(\"Creating video clip...\")\n        clip = create_fallback_clip(scene)\n        st.success(\"Video clip processed\")\n    \n    with voice_progress:\n        st.text(\"Generating voiceover...\")\n        narration_file = generate_voiceover(scene['narration'])\n        st.success(\"Voiceover generated\")\n    \n    scene_progress.success(f\"Scene {index + 1} processed successfully!\")\n    return {'clip': clip, 'scene': scene, 'narration': narration_file}\n\ndef generate_valid_storyboard(prompt, style, max_attempts=3):\n    for attempt in range(max_attempts):\n        storyboard = generate_storyboard(prompt, style)\n        if storyboard is not None:\n            return storyboard\n        logger.warning(f\"Storyboard generation attempt {attempt + 1} failed. Retrying...\")\n    logger.error(\"Failed to generate a valid storyboard after multiple attempts.\")\n    st.error(\"Failed to generate a valid storyboard after multiple attempts. Please try again with a different prompt or style.\")\n    return None\n\ndef prompt_card(prompt):\n    st.markdown(f\"**Sample Prompt:** {prompt}\")\n    if st.button(\"Use this prompt\", key=f\"btn_{prompt}\"):\n        st.session_state.prompt = prompt\n\ndef predict_processing_issues(video_clips, system_resources):\n    potential_issues = []\n    if len(video_clips) * 5 > system_resources['available_memory'] / 1e6:  # Assuming 5 seconds per clip\n        potential_issues.append(\"Insufficient memory for processing all clips\")\n    if system_resources['cpu_usage'] > 80:\n        potential_issues.append(\"High CPU usage may slow down processing\")\n    return potential_issues\n\ndef generate_script(prompt, duration):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert video scriptwriter. Create a JSON script for a video based on the given prompt and duration. Use the following schema:\"},\n                {\"role\": \"user\", \"content\": f\"\"\"\n                Create a script for a {duration} second video about: {prompt}. \n                Return the script as a JSON object with the following structure:\n                {{\n                    \"title\": \"Overall video title\",\n                    \"scenes\": [\n                        {{\n                            \"scene_number\": 1,\n                            \"title\": \"Scene title\",\n                            \"description\": \"Detailed scene description\",\n                            \"narration\": \"Narration text for the scene\",\n                            \"duration\": \"Duration in seconds\"\n                        }}\n                    ]\n                }}\n                Include 3-5 scenes in total.\n                \"\"\"}\n            ],\n            max_tokens=1000,\n            response_format={\"type\": \"json_object\"}\n        )\n        script = json.loads(response.choices[0].message.content)\n        logger.info(f\"Generated script: {script}\")\n        return script\n    except Exception as e:\n        logger.error(f\"Error generating script: {str(e)}\")\n        return None\n\ndef fetch_video_clips(scenes):\n    logger.info(f\"Fetching video clips for {len(scenes)} scenes\")\n    video_clips = []\n    \n    api = HfApi()\n    \n    for i, scene in enumerate(scenes):\n        logger.info(f\"Fetching clip for scene {i+1}: {scene['title']}\")\n        \n        # Expand the search query to include more relevant terms\n        keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\n        search_query = \" OR \".join(keywords)\n        \n        try:\n            search_results = api.list_datasets(search=search_query, limit=20)  # Increase the limit\n            matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\n            \n            if matching_datasets:\n                chosen_dataset = random.choice(matching_datasets)\n                dataset_info = api.dataset_info(chosen_dataset.id)\n                \n                if dataset_info.card_data and 'samples' in dataset_info.card_data:\n                    sample = random.choice(dataset_info.card_data['samples'])\n                    if 'video' in sample:\n                        video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\n                        clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))  # Limit to 10 seconds max\n                        logger.info(f\"Clip fetched for scene {i+1}: duration={clip.duration}s\")\n                    else:\n                        raise ValueError(\"No video found in the sample\")\n                else:\n                    raise ValueError(\"No samples found in the dataset\")\n            else:\n                raise ValueError(\"No matching datasets found\")\n        \n        except Exception as e:\n            logger.warning(f\"Error fetching video for scene {i+1}: {str(e)}. Creating fallback clip.\")\n            clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\n        \n        video_clips.append({'clip': clip, 'scene': scene})\n    \n    return video_clips\n\ndef create_video_workflow(prompt, duration, music_style):\n    try:\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n\n        logger.info(\"Starting video creation workflow\")\n        status_text.text(\"Generating script...\")\n        script = generate_script(prompt, duration)\n        if not script:\n            raise ValueError(\"Failed to generate script\")\n        logger.info(\"Script generated successfully\")\n        progress_bar.progress(20)\n\n        scenes = script['scenes']\n        if not scenes:\n            logger.error(f\"No scenes found in the script. Script content: {script}\")\n            raise ValueError(\"No scenes found in the script\")\n        logger.info(f\"Successfully extracted {len(scenes)} scenes\")\n        progress_bar.progress(30)\n\n        status_text.text(\"Fetching video clips...\")\n        video_clips = fetch_video_clips(scenes)\n        if not video_clips:\n            raise ValueError(\"Failed to fetch video clips\")\n        progress_bar.progress(50)\n\n        status_text.text(\"Generating voiceovers...\")\n        for clip_data in video_clips:\n            clip_data['narration'] = generate_voiceover(clip_data['scene']['narration'])\n        progress_bar.progress(70)\n\n        status_text.text(\"Selecting background music...\")\n        background_music_file = select_background_music(music_style)\n        progress_bar.progress(80)\n\n        status_text.text(\"Creating your video...\")\n        video_file = create_video(video_clips, background_music_file, script['title'])\n        progress_bar.progress(90)\n\n        if video_file and os.path.exists(video_file):\n            status_text.text(\"Finalizing...\")\n            progress_bar.progress(100)\n            logger.info(\"Video creation successful\")\n            st.success(\"\ud83c\udf89 Video created successfully!\")\n            st.video(video_file)\n            with open(video_file, 'rb') as vf:\n                st.download_button(label=\"\ud83d\udce5 Download Video\", data=vf, file_name=\"AutovideoAI_creation.mp4\")\n        else:\n            logger.error(\"Failed to create the video\")\n            st.error(\"\u274c Failed to create the video. Please try again.\")\n    except Exception as e:\n        logger.error(f\"An error occurred during video creation: {str(e)}\")\n        st.error(f\"An error occurred: {str(e)}\")\n    finally:\n        status_text.empty()\n        progress_bar.empty()\n\ndef main():\n    st.markdown(\"<h1 style='text-align: center; color: #4A90E2;'>AutovideoAI</h1>\", unsafe_allow_html=True)\n    st.markdown(\"<p style='text-align: center; font-size: 1.2em;'>Create Amazing Videos with AI</p>\", unsafe_allow_html=True)\n\n    with st.expander(\"\u2139\ufe0f How to use AutovideoAI\", expanded=False):\n        st.markdown(\"\"\"\n        1. Enter your video idea or choose a sample prompt.\n        2. Customize your video style, duration, and background music.\n        3. Generate a storyboard and preview it.\n        4. Create your AI-powered video!\n        \"\"\")\n\n    col1, col2 = st.columns([2, 1])\n    \n    with col1:\n        st.subheader(\"1\ufe0f\u20e3 Enter Your Video Idea\")\n        prompt = st.text_area(\"What's your video about?\", height=100, value=st.session_state.get('prompt', ''))\n    \n    with col2:\n        st.subheader(\"Sample Prompts\")\n        for sample_prompt in SAMPLE_PROMPTS:\n            if st.button(f\"\ud83d\udccc {sample_prompt}\", key=f\"btn_{sample_prompt}\"):\n                st.session_state.prompt = sample_prompt\n                st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n\n    st.subheader(\"2\ufe0f\u20e3 Customize Your Video\")\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        style = st.selectbox(\"Video Style \ud83c\udfad\", [\"Motivational\", \"Dramatic\", \"Educational\", \"Funny\"])\n    with col2:\n        duration = st.slider(\"Estimated Duration \u23f1\ufe0f\", 30, 300, 60, help=\"Duration in seconds\")\n    with col3:\n        music_style = st.selectbox(\"Background Music \ud83c\udfb5\", list(MUSIC_TRACKS.keys()))\n\n    if st.button(\"\ud83d\udd8b\ufe0f Generate Storyboard\", use_container_width=True):\n        with st.spinner(\"Crafting your storyboard...\"):\n            storyboard = generate_script(prompt, duration)\n        if storyboard:\n            st.session_state.storyboard = storyboard\n            st.success(\"\u2705 Storyboard generated successfully!\")\n            display_storyboard_preview(storyboard)\n        else:\n            st.error(\"\u274c Failed to generate a storyboard. Please try again.\")\n\n    if 'storyboard' in st.session_state:\n        if st.button(\"\ud83c\udfac Create Video\", use_container_width=True):\n            create_video_workflow(prompt, duration, music_style)\n\ndef display_storyboard_preview(storyboard):\n    with st.expander(\"\ud83d\udd0d Preview Storyboard\", expanded=True):\n        st.markdown(f\"### {storyboard['title']}\")\n        st.markdown(\"### Scene Breakdown\")\n        for scene in storyboard['scenes']:\n            with st.container():\n                col1, col2 = st.columns([1, 2])\n                with col1:\n                    st.markdown(f\"**Scene {scene['scene_number']}**\")\n                    st.write(f\"Duration: {scene['duration']} seconds\")\n                with col2:\n                    st.markdown(f\"**{scene['title']}**\")\n                    st.write(f\"{scene['description']}\")\n                st.markdown(\"---\")\n\n# Add this function definition near the top of the file, after the imports\ndef color_gradient(size, p1, p2, color1, color2):\n    x = np.linspace(0, 1, size[0])[:, None]\n    y = np.linspace(0, 1, size[1])[None, :]\n    gradient = x * (p2[0] - p1[0]) + y * (p2[1] - p1[1])\n    gradient = np.clip(gradient, 0, 1)\n    return np.array(color1) * (1 - gradient[:, :, None]) + np.array(color2) * gradient[:, :, None]\n\nif __name__ == \"__main__\":\n    main()", "function_list": ["toggle_theme", "cached_download", "generate_storyboard", "validate_storyboard", "parse_storyboard", "fetch_video_clips", "create_fallback_clip", "generate_voiceover", "create_silent_audio", "create_scene_clip", "add_fade_transition", "create_animated_text", "apply_color_grading", "create_lower_third", "smart_cut", "apply_speed_changes", "create_video", "enhance_clip", "process_clip", "apply_fade_effects", "add_text_overlay", "add_narration", "add_background_music", "add_watermark", "split_video", "merge_video_parts", "save_storyboard_backup", "load_storyboard_backup", "add_subtitles_to_video", "preview_storyboard_slideshow", "add_logo_to_video", "compress_video", "apply_bw_filter", "overlay_image_on_video", "adjust_video_speed", "crop_video", "adjust_resolution_based_on_system", "generate_video_thumbnail", "add_intro_outro", "adjust_audio_volume", "generate_gradient_text_overlay", "run_video_rendering_thread", "check_system_capabilities", "log_system_resources", "download_additional_assets", "calculate_estimated_render_time", "manage_temp_directory", "handle_session_expiration", "split_storyboard_scenes", "add_transition_effects_between_scenes", "optimize_storyboard_text_prompts", "select_background_music", "analyze_script", "apply_transition", "cleanup_temp_files", "process_scene_with_progress", "generate_valid_storyboard", "prompt_card", "predict_processing_issues", "generate_script", "fetch_video_clips", "create_video_workflow", "main", "display_storyboard_preview", "color_gradient", "toggle_theme", "cached_download", "generate_storyboard", "validate_storyboard", "parse_storyboard", "fetch_video_clips", "create_fallback_clip", "generate_voiceover", "create_silent_audio", "create_scene_clip", "add_fade_transition", "create_animated_text", "apply_color_grading", "create_lower_third", "smart_cut", "apply_speed_changes", "create_video", "enhance_clip", "process_clip", "apply_fade_effects", "add_text_overlay", "add_narration", "add_background_music", "add_watermark", "split_video", "merge_video_parts", "save_storyboard_backup", "load_storyboard_backup", "add_subtitles_to_video", "preview_storyboard_slideshow", "add_logo_to_video", "compress_video", "apply_bw_filter", "overlay_image_on_video", "adjust_video_speed", "crop_video", "adjust_resolution_based_on_system", "generate_video_thumbnail", "add_intro_outro", "adjust_audio_volume", "generate_gradient_text_overlay", "run_video_rendering_thread", "check_system_capabilities", "log_system_resources", "download_additional_assets", "calculate_estimated_render_time", "manage_temp_directory", "handle_session_expiration", "split_storyboard_scenes", "add_transition_effects_between_scenes", "optimize_storyboard_text_prompts", "select_background_music", "analyze_script", "apply_transition", "cleanup_temp_files", "process_scene_with_progress", "generate_valid_storyboard", "prompt_card", "predict_processing_issues", "generate_script", "fetch_video_clips", "create_video_workflow", "main", "display_storyboard_preview", "color_gradient"], "progress": 0, "final_script": "# Imports\nimport streamlit as st\nfrom openai import OpenAI\nimport os\nimport moviepy.editor as mpe\nimport requests\nfrom tempfile import NamedTemporaryFile\nfrom moviepy.video.fx.all import fadein, fadeout, resize\nimport psutil\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport shutil\nimport logging\nfrom gtts import gTTS\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment\nimport random\nimport moviepy.video.fx.all as vfx\nimport cv2\nfrom pydub.silence import split_on_silence\nimport numpy as np\nimport tempfile\nfrom functools import lru_cache\nfrom huggingface_hub import InferenceClient, hf_hub_download\nfrom huggingface_hub import login, HfApi\nfrom datasets import load_dataset\nimport textwrap\nfrom scenedetect import detect, ContentDetector\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\nimport re\n\n# Settings\nlogger = logging.getLogger(__name__)\nhf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\napi = HfApi()\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nSAMPLE_PROMPTS = [\n    \"Create a motivational video about overcoming challenges\",\n    \"Make an educational video explaining photosynthesis\",\n    \"Design a funny video about the struggles of working from home\",\n]\nMUSIC_TRACKS = {\n    \"Electronic\": \"https://www.bensound.com/bensound-dubstep\",\n    \"Experimental\": \"https://www.bensound.com/bensound-enigmatic\",\n    \"Folk\": \"https://www.bensound.com/bensound-acousticbreeze\",\n    \"Hip-Hop\": \"https://www.bensound.com/bensound-groovyhiphop\",\n    \"Instrumental\": \"https://www.bensound.com/bensound-pianomoment\",\n    \"Pop\": \"https://www.bensound.com/bensound-ukulele\",\n    \"Rock\": \"https://www.bensound.com/bensound-extremeaction\"\n}\nresponse = requests.get(url)\nrequired_fields = [\"scene_number\", \"title\", \"description\", \"narration\", \"keywords\", \"duration\", \"overlay_text\", \"visual_elements\", \"audio_cues\", \"transitions\"]\nvideo_clips = []\napi = HfApi()\ntext = scene.get('title', 'Scene')\nsize = (1280, 720)\nimg = Image.new('RGB', size, color='black')\ndraw = ImageDraw.Draw(img)\nfont = ImageFont.load_default()\nwrapped_text = textwrap.wrap(text, width=20)\ny_text = (size[1] - len(wrapped_text) * 80) // 2\nimg_array = np.array(img)\nclip = mpe.ImageClip(img_array).set_duration(duration)\nsilent_segment = AudioSegment.silent(duration=int(duration * 1000))\ntemp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\nclip = clip.crossfadein(0.5).crossfadeout(0.5)\nscene = clip_data['scene']\nduration = float(scene['duration'])\nprocessed_clip = enhanced_clip.set_duration(duration)\nlower_third = create_lower_third(scene['title'], duration)\nprocessed_clip = mpe.CompositeVideoClip([processed_clip, lower_third])\nintro_clip = create_animated_text(video_title, duration=3, font_size=60).fx(vfx.fadeout, duration=1)\noutro_clip = create_animated_text(\"Thanks for Watching!\", duration=3, font_size=60).fx(vfx.fadein, duration=1)\ntransition_functions = {\n        'fade': lambda: clip1.crossfadeout(1).crossfadein(1),\n        'slide': lambda: clip1.slide_out(1, 'left').slide_in(1, 'right'),\n        'whip': lambda: clip1.fx(vfx.speedx, 2).fx(vfx.crop, x1=0, y1=0, x2=0.5, y2=1).crossfadeout(0.5),\n        'zoom': lambda: clip1.fx(vfx.resize, 1.5).fx(vfx.crop, x_center=0.5, y_center=0.5, width=1/1.5, height=1/1.5).crossfadeout(1)\n    }\nscene_progress = st.empty()\npotential_issues = []\nvideo_clips = []\napi = HfApi()\nx = np.linspace(0, 1, size[0])[:, None]\ny = np.linspace(0, 1, size[1])[None, :]\ngradient = x * (p2[0] - p1[0]) + y * (p2[1] - p1[1])\ngradient = np.clip(gradient, 0, 1)\ninput_text = f\"\"\"Generate a detailed {style} video storyboard based on this prompt: \"{prompt}\"\n        Provide the storyboard in JSON format with the following structure:\n        {{\n            \"title\": \"Overall video title\",\n            \"scenes\": [\n                {{\n                    \"scene_number\": 1,\n                    \"title\": \"Scene title\",\n                    \"description\": \"Detailed scene description\",\n                    \"narration\": \"Narration text for the scene\",\n                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n                    \"duration\": \"Duration in seconds\",\n                    \"visual_elements\": [\"List of visual elements to include\"],\n                    \"transitions\": {{\n                        \"in\": \"Transition type for entering the scene\",\n                        \"out\": \"Transition type for exiting the scene\"\n                    }}\n                }}\n            ],\n            \"target_audience\": \"Description of the target audience\",\n            \"overall_tone\": \"Description of the overall tone of the video\"\n        }}\n        Ensure there are at least 3 scenes in the storyboard.\"\"\"\nresponse = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative video storyboard generator. Respond with valid JSON following the specified structure.\"},\n                {\"role\": \"user\", \"content\": input_text}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\nstoryboard = json.loads(response.choices[0].message.content)\nposition = ((size[0] - line_width) / 2, y_text)\ntts = gTTS(text=narration_text, lang='en', slow=False)\ntemp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\ngradient = np.linspace(0, 255, 1280)\nbackground = np.tile(gradient, (720, 1)).astype(np.uint8)\nbackground = np.stack((background,) * 3, axis=-1)\nclip = mpe.ImageClip(background).set_duration(duration)\ntxt_clip = mpe.TextClip(scene['title'], fontsize=70, color='white', font='Arial-Bold')\ntxt_clip = txt_clip.set_position('center').set_duration(duration)\nclip = clip.fadein(0.5).fadeout(0.5)\nimg = Image.new('RGB', (1280, 720), color='black')\ndraw = ImageDraw.Draw(img)\nfont = ImageFont.load_default().font_variant(size=font_size)\ntext_bbox = draw.textbbox((0, 0), text, font=font)\ntext_width = text_bbox[2] - text_bbox[0]\ntext_height = text_bbox[3] - text_bbox[1]\nposition = ((1280 - text_width) / 2, (720 - text_height) / 2)\nimg_array = np.array(img)\nclip = mpe.ImageClip(img_array).set_duration(duration)\nclip = clip.fadein(1).fadeout(1)\nimg = Image.new('RGBA', (1280, 720), (0, 0, 0, 0))\ndraw = ImageDraw.Draw(img)\nfont = ImageFont.load_default().font_variant(size=30)\ntext_bbox = draw.textbbox((0, 0), text, font=font)\ntext_width = text_bbox[2] - text_bbox[0]\ntext_height = text_bbox[3] - text_bbox[1]\nposition = ((1280 - text_width) / 2, 720 - text_height - 50)\nbg_bbox = (position[0]-10, position[1]-10, position[0]+text_width+10, position[1]+text_height+10)\nimg_array = np.array(img)\nchunks = split_on_silence(audio_segment, min_silence_len=500, silence_thresh=-40)\ncut_times = [0]\ncut_clips = [video_clip.subclip(start, end) for start, end in zip(cut_times[:-1], cut_times[1:])]\nframes = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in clip.iter_frames()]\nmotion = [np.mean(cv2.absdiff(frames[i], frames[i+1])) for i in range(len(frames)-1)]\nspeed_clip = clip.fl(lambda gf, t: gf(t * speed_factor) if motion[int(t*clip.fps)] < threshold else gf(t))\nclips = []\nfinal_clip = mpe.concatenate_videoclips(clips)\nintro_clip = mpe.TextClip(video_title, fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\noutro_clip = mpe.TextClip(\"Thanks for watching!\", fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\nfinal_clip = mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\noutput_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name\nclip = apply_color_grading(clip, brightness=1.1, saturation=1.2)\ntext_clip = create_animated_text(clip.scene['title'], duration=2)\nclip = mpe.CompositeVideoClip([clip, text_clip.set_start(1)])\nprocessed_clip = apply_color_grading(processed_clip, brightness=1.1)\nprocessed_clip = apply_color_grading(processed_clip, brightness=0.9)\nprocessed_clip = processed_clip.fx(vfx.colorx, 1.2)\ntext_clip = create_animated_text(scene['overlay_text'], duration)\nprocessed_clip = mpe.CompositeVideoClip([processed_clip, text_clip])\nbackground_audio = mpe.AudioFileClip(music_file)\nwatermark = mpe.TextClip(watermark_text, fontsize=30, color='white', font='Arial')\nwatermark = watermark.set_position(('right', 'bottom')).set_duration(clip.duration)\nsubtitle_clips = [\n            mpe.TextClip(subtitle['text'], fontsize=50, color='white', size=clip.size, font='Arial-Bold')\n            .set_position(('bottom')).set_start(subtitle['start']).set_duration(subtitle['duration'])\n            for subtitle in subtitles\n        ]\nslides = [create_animated_text(scene['title'], duration=duration_per_scene) for scene in scenes]\nslideshow = mpe.concatenate_videoclips(slides, method='compose')\nlogo = mpe.ImageClip(logo_path).set_duration(clip.duration).resize(height=100).set_position(position)\nimage = mpe.ImageClip(image_path).set_duration(clip.duration).set_position(position)\nmemory = psutil.virtual_memory()\nresolution = (640, 360) if memory.available < 1000 * 1024 * 1024 else (1280, 720)\nframe = clip.get_frame(1)\nimage = Image.fromarray(frame)\ngradient = color_gradient(size, p1=(0, 0), p2=(size[0], size[1]), color1=(255, 0, 0), color2=(0, 0, 255))\nimage = Image.fromarray(gradient)\ndraw = ImageDraw.Draw(image)\nfont = ImageFont.load_default()\ntext_size = draw.textsize(text, font=font)\nrendering_thread = threading.Thread(target=target_function, args=args)\nmemory = psutil.virtual_memory()\ncpu_usage = psutil.cpu_percent()\nmemory = psutil.virtual_memory()\ncpu = psutil.cpu_percent()\nresponse = requests.get(url)\nestimated_time = duration * (resolution[0] * resolution[1]) / 1e6\nresponse = requests.get(track_url)\nresponse = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Analyze the script and provide a JSON response with keys: 'sentiment' (POSITIVE, NEGATIVE, or NEUTRAL), 'style' (e.g., formal, casual, humorous), and 'transitions' (list of transition types).\"},\n                {\"role\": \"user\", \"content\": f\"Analyze this script: {script}\"}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\nanalysis = json.loads(response.choices[0].message.content)\ntemp_dir = tempfile.gettempdir()\nclip = create_fallback_clip(scene)\nnarration_file = generate_voiceover(scene['narration'])\nstoryboard = generate_storyboard(prompt, style)\nresponse = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert video scriptwriter. Create a JSON script for a video based on the given prompt and duration. Use the following schema:\"},\n                {\"role\": \"user\", \"content\": f\"\"\"\n                Create a script for a {duration} second video about: {prompt}. \n                Return the script as a JSON object with the following structure:\n                {{\n                    \"title\": \"Overall video title\",\n                    \"scenes\": [\n                        {{\n                            \"scene_number\": 1,\n                            \"title\": \"Scene title\",\n                            \"description\": \"Detailed scene description\",\n                            \"narration\": \"Narration text for the scene\",\n                            \"duration\": \"Duration in seconds\"\n                        }}\n                    ]\n                }}\n                Include 3-5 scenes in total.\n                \"\"\"}\n            ],\n            max_tokens=1000,\n            response_format={\"type\": \"json_object\"}\n        )\nscript = json.loads(response.choices[0].message.content)\nkeywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\nsearch_query = \" OR \".join(keywords)\nprogress_bar = st.progress(0)\nstatus_text = st.empty()\nscript = generate_script(prompt, duration)\nscenes = script['scenes']\nvideo_clips = fetch_video_clips(scenes)\nbackground_music_file = select_background_music(music_style)\nvideo_file = create_video(video_clips, background_music_file, script['title'])\nprompt = st.text_area(\"What's your video about?\", height=100, value=st.session_state.get('prompt', ''))\nstyle = st.selectbox(\"Video Style \ud83c\udfad\", [\"Motivational\", \"Dramatic\", \"Educational\", \"Funny\"])\nduration = st.slider(\"Estimated Duration \u23f1\ufe0f\", 30, 300, 60, help=\"Duration in seconds\")\nmusic_style = st.selectbox(\"Background Music \ud83c\udfb5\", list(MUSIC_TRACKS.keys()))\nkeywords_txt = \", \".join(scene['keywords'])\nkeywords_clip = mpe.TextClip(keywords_txt, fontsize=30, color='yellow', font='Arial')\nkeywords_clip = keywords_clip.set_position(('center', 0.8), relative=True).set_duration(duration)\nclip = mpe.CompositeVideoClip([clip, txt_clip, keywords_clip])\nclip = mpe.CompositeVideoClip([clip, txt_clip])\naudio_segment = AudioSegment.from_wav(audio_clip)\nclip = clip.set_fps(24)\nbackground_music = mpe.AudioFileClip(background_music_file).volumex(0.1)\nbackground_music = background_music.audio_loop(duration=final_clip.duration)\nfinal_audio = mpe.CompositeAudioClip([final_clip.audio, background_music])\nfinal_clip = final_clip.set_audio(final_audio)\nclip = apply_color_grading(clip, brightness=0.9, contrast=1.1)\nprocessed_clip = processed_clip.fx(vfx.lum_contrast, contrast=1.2)\ntext_clip = mpe.TextClip(text, fontsize=70, color='white', font='Arial-Bold')\ntext_clip = text_clip.set_position('center').set_duration(clip.duration)\nstoryboard = json.load(f)\ntemp_asset_file = NamedTemporaryFile(delete=False, suffix='.mp3')\ntrack_url = MUSIC_TRACKS[genre]\ntrack_url = random.choice(list(MUSIC_TRACKS.values()))\ntemp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\nsearch_results = api.list_datasets(search=search_query, limit=20)\nmatching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\nstoryboard = generate_script(prompt, duration)\nkeywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\nsearch_query = \" OR \".join(keywords)\nsearch_results = api.list_datasets(search=search_query, limit=20)\nmatching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\naudio_array = audio_clip.to_soundarray()\naudio_segment = AudioSegment(\n                audio_array.tobytes(),\n                frame_rate=audio_clip.fps,\n                sample_width=audio_array.dtype.itemsize,\n                channels=1 if audio_array.ndim == 1 else audio_array.shape[1]\n            )\nclip = clip_data['clip']\nnarration = clip_data['narration']\nclip = clip.set_audio(narration)\nfile_path = os.path.join(temp_dir, filename)\nchosen_dataset = random.choice(matching_datasets)\ndataset_info = api.dataset_info(chosen_dataset.id)\nclip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\nchosen_dataset = random.choice(matching_datasets)\ndataset_info = api.dataset_info(chosen_dataset.id)\nsample = random.choice(dataset_info.card_data['samples'])\nsample = random.choice(dataset_info.card_data['samples'])\nclip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\nvideo_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\nclip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))\nvideo_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\nclip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))\n\ndef toggle_theme():\n    st.session_state.theme = 'dark' if st.session_state.theme == 'light' else 'light'\n\ndef cached_download(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.content\n    return None\n\ndef generate_storyboard(prompt, style=\"motivational\"):\n    try:\n        input_text = f\"\"\"Generate a detailed {style} video storyboard based on this prompt: \"{prompt}\"\n        Provide the storyboard in JSON format with the following structure:\n        {{\n            \"title\": \"Overall video title\",\n            \"scenes\": [\n                {{\n                    \"scene_number\": 1,\n                    \"title\": \"Scene title\",\n                    \"description\": \"Detailed scene description\",\n                    \"narration\": \"Narration text for the scene\",\n                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n                    \"duration\": \"Duration in seconds\",\n                    \"visual_elements\": [\"List of visual elements to include\"],\n                    \"transitions\": {{\n                        \"in\": \"Transition type for entering the scene\",\n                        \"out\": \"Transition type for exiting the scene\"\n                    }}\n                }}\n            ],\n            \"target_audience\": \"Description of the target audience\",\n            \"overall_tone\": \"Description of the overall tone of the video\"\n        }}\n        Ensure there are at least 3 scenes in the storyboard.\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative video storyboard generator. Respond with valid JSON following the specified structure.\"},\n                {\"role\": \"user\", \"content\": input_text}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        storyboard = json.loads(response.choices[0].message.content)\n        return storyboard\n    except Exception as e:\n        logger.error(f\"Error generating storyboard: {str(e)}\")\n        st.error(\"An error occurred while generating the storyboard. Please try again.\")\n    return None\n\ndef validate_storyboard(storyboard):\n    if \"title\" not in storyboard or \"scenes\" not in storyboard or not isinstance(storyboard[\"scenes\"], list):\n        return False\n    if len(storyboard[\"scenes\"]) < 3:\n        return False\n    \n    required_fields = [\"scene_number\", \"title\", \"description\", \"narration\", \"keywords\", \"duration\", \"overlay_text\", \"visual_elements\", \"audio_cues\", \"transitions\"]\n    return all(all(field in scene for field in required_fields) for scene in storyboard[\"scenes\"])\n\ndef parse_storyboard(storyboard):\n    try:\n        return json.loads(storyboard).get(\"scenes\", [])\n    except json.JSONDecodeError:\n        return []\n\ndef fetch_video_clips(scenes):\n    logger.info(f\"Fetching video clips for {len(scenes)} scenes\")\n    video_clips = []\n    \n    api = HfApi()\n    \n    for i, scene in enumerate(scenes):\n        logger.info(f\"Fetching clip for scene {i+1}: {scene['title']}\")\n        \n        # Expand the search query to include more relevant terms\n        keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\n        search_query = \" OR \".join(keywords)\n        \n        try:\n            search_results = api.list_datasets(search=search_query, limit=20)  # Increase the limit\n            matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\n            \n            if matching_datasets:\n                chosen_dataset = random.choice(matching_datasets)\n                dataset_info = api.dataset_info(chosen_dataset.id)\n                \n                if dataset_info.card_data and 'samples' in dataset_info.card_data:\n                    sample = random.choice(dataset_info.card_data['samples'])\n                    if 'video' in sample:\n                        video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\n                        clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))  # Limit to 10 seconds max\n                        logger.info(f\"Clip fetched for scene {i+1}: duration={clip.duration}s\")\n                    else:\n                        raise ValueError(\"No video found in the sample\")\n                else:\n                    raise ValueError(\"No samples found in the dataset\")\n            else:\n                raise ValueError(\"No matching datasets found\")\n        \n        except Exception as e:\n            logger.warning(f\"Error fetching video for scene {i+1}: {str(e)}. Creating fallback clip.\")\n            clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\n        \n        video_clips.append({'clip': clip, 'scene': scene})\n    \n    return video_clips\n\ndef create_fallback_clip(scene, duration=5):\n    text = scene.get('title', 'Scene')\n    size = (1280, 720)\n    \n    # Create a black background\n    img = Image.new('RGB', size, color='black')\n    draw = ImageDraw.Draw(img)\n    \n    # Use a default font\n    font = ImageFont.load_default()\n    \n    # Wrap text\n    wrapped_text = textwrap.wrap(text, width=20)\n    \n    # Calculate text position\n    y_text = (size[1] - len(wrapped_text) * 80) // 2\n    \n    # Draw text\n    for line in wrapped_text:\n        line_width, line_height = draw.textbbox((0, 0), line, font=font)[2:]\n        position = ((size[0] - line_width) / 2, y_text)\n        draw.text(position, line, font=font, fill='white')\n        y_text += line_height + 10\n    \n    # Convert PIL Image to numpy array\n    img_array = np.array(img)\n    \n    # Create video clip from the image\n    clip = mpe.ImageClip(img_array).set_duration(duration)\n    \n    return clip\n\ndef generate_voiceover(narration_text):\n    logger.info(f\"Generating voiceover for text: {narration_text[:50]}...\")\n    try:\n        tts = gTTS(text=narration_text, lang='en', slow=False)\n        temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n        tts.save(temp_audio_file.name)\n        return mpe.AudioFileClip(temp_audio_file.name)\n    except Exception as e:\n        logger.error(f\"Error generating voiceover: {str(e)}\")\n        return create_silent_audio(len(narration_text.split()) / 2)\n\ndef create_silent_audio(duration):\n    silent_segment = AudioSegment.silent(duration=int(duration * 1000))  # pydub uses milliseconds\n    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n    silent_segment.export(temp_audio_file.name, format=\"mp3\")\n    return mpe.AudioFileClip(temp_audio_file.name)\n\ndef create_scene_clip(scene, duration=5):\n    try:\n        # Create a gradient background\n        gradient = np.linspace(0, 255, 1280)\n        background = np.tile(gradient, (720, 1)).astype(np.uint8)\n        background = np.stack((background,) * 3, axis=-1)\n        \n        # Create video clip from the background\n        clip = mpe.ImageClip(background).set_duration(duration)\n        \n        # Add text\n        txt_clip = mpe.TextClip(scene['title'], fontsize=70, color='white', font='Arial-Bold')\n        txt_clip = txt_clip.set_position('center').set_duration(duration)\n        \n        # Add keywords as subtitles\n        if 'keywords' in scene:\n            keywords_txt = \", \".join(scene['keywords'])\n            keywords_clip = mpe.TextClip(keywords_txt, fontsize=30, color='yellow', font='Arial')\n            keywords_clip = keywords_clip.set_position(('center', 0.8), relative=True).set_duration(duration)\n            clip = mpe.CompositeVideoClip([clip, txt_clip, keywords_clip])\n        else:\n            clip = mpe.CompositeVideoClip([clip, txt_clip])\n        \n        # Add fade in and out\n        clip = clip.fadein(0.5).fadeout(0.5)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating scene clip: {str(e)}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)\n\ndef add_fade_transition(clip1, clip2, duration=1):\n    return mpe.CompositeVideoClip([clip1.crossfadeout(duration), clip2.crossfadein(duration)])\n\ndef create_animated_text(text, duration=5, font_size=70, color='white'):\n    try:\n        # Create a black background image\n        img = Image.new('RGB', (1280, 720), color='black')\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=font_size)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position to center the text\n        position = ((1280 - text_width) / 2, (720 - text_height) / 2)\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill=color)\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        clip = mpe.ImageClip(img_array).set_duration(duration)\n        \n        # Add fade in and fade out effects\n        clip = clip.fadein(1).fadeout(1)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating animated text: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)\n\ndef apply_color_grading(clip, brightness=1.0, contrast=1.0, saturation=1.0):\n    return clip.fx(vfx.colorx, brightness).fx(vfx.lum_contrast, contrast=contrast).fx(vfx.colorx, saturation)\n\ndef create_lower_third(text, duration):\n    try:\n        # Create a transparent background\n        img = Image.new('RGBA', (1280, 720), (0, 0, 0, 0))\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=30)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position for lower third\n        position = ((1280 - text_width) / 2, 720 - text_height - 50)\n        \n        # Draw semi-transparent background\n        bg_bbox = (position[0]-10, position[1]-10, position[0]+text_width+10, position[1]+text_height+10)\n        draw.rectangle(bg_bbox, fill=(0,0,0,153))\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill='white')\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        return mpe.ImageClip(img_array).set_duration(duration)\n    except Exception as e:\n        logger.error(f\"Error creating lower third: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0,0)).set_duration(duration)\n\ndef smart_cut(video_clip, audio_clip):\n    try:\n        # Check if audio_clip is a file path or an AudioClip object\n        if isinstance(audio_clip, str):\n            audio_segment = AudioSegment.from_wav(audio_clip)\n        elif isinstance(audio_clip, mpe.AudioClip):\n            # Convert AudioClip to numpy array\n            audio_array = audio_clip.to_soundarray()\n            audio_segment = AudioSegment(\n                audio_array.tobytes(),\n                frame_rate=audio_clip.fps,\n                sample_width=audio_array.dtype.itemsize,\n                channels=1 if audio_array.ndim == 1 else audio_array.shape[1]\n            )\n        else:\n            logger.warning(\"Unsupported audio type. Returning original video clip.\")\n            return video_clip\n\n        # Split audio on silences\n        chunks = split_on_silence(audio_segment, min_silence_len=500, silence_thresh=-40)\n        \n        # Calculate timestamps for cuts\n        cut_times = [0]\n        for chunk in chunks:\n            cut_times.append(cut_times[-1] + len(chunk) / 1000)\n        \n        # Cut video based on audio\n        cut_clips = [video_clip.subclip(start, end) for start, end in zip(cut_times[:-1], cut_times[1:])]\n        \n        return mpe.concatenate_videoclips(cut_clips)\n    except Exception as e:\n        logger.error(f\"Error in smart_cut: {str(e)}\")\n        return video_clip\n\ndef apply_speed_changes(clip, speed_factor=1.5, threshold=0.1):\n    try:\n        if not hasattr(clip, 'fps') or clip.fps is None:\n            clip = clip.set_fps(24)  # Set a default fps if not present\n        \n        frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in clip.iter_frames()]\n        motion = [np.mean(cv2.absdiff(frames[i], frames[i+1])) for i in range(len(frames)-1)]\n        \n        speed_clip = clip.fl(lambda gf, t: gf(t * speed_factor) if motion[int(t*clip.fps)] < threshold else gf(t))\n        \n        return speed_clip\n    except Exception as e:\n        logger.error(f\"Error in apply_speed_changes: {str(e)}\")\n        return clip\n\ndef create_video(video_clips, background_music_file, video_title):\n    logger.info(f\"Starting video creation process for '{video_title}'\")\n    try:\n        clips = []\n        for i, clip_data in enumerate(video_clips):\n            try:\n                clip = clip_data['clip']\n                narration = clip_data['narration']\n                clip = clip.set_audio(narration)\n                clips.append(clip)\n                logger.info(f\"Processed clip {i+1} successfully\")\n            except Exception as e:\n                logger.error(f\"Error processing clip {i+1}: {str(e)}\")\n        \n        if not clips:\n            raise ValueError(\"No valid clips were created\")\n        \n        logger.info(f\"Concatenating {len(clips)} scene clips\")\n        final_clip = mpe.concatenate_videoclips(clips)\n        \n        if background_music_file:\n            logger.info(\"Adding background music\")\n            background_music = mpe.AudioFileClip(background_music_file).volumex(0.1)\n            background_music = background_music.audio_loop(duration=final_clip.duration)\n            final_audio = mpe.CompositeAudioClip([final_clip.audio, background_music])\n            final_clip = final_clip.set_audio(final_audio)\n        \n        # Add intro and outro\n        intro_clip = mpe.TextClip(video_title, fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        outro_clip = mpe.TextClip(\"Thanks for watching!\", fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        final_clip = mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\n        \n        output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name\n        logger.info(f\"Writing final video to {output_file}\")\n        final_clip.write_videofile(output_file, codec='libx264', audio_codec='aac', fps=24)\n        logger.info(\"Video creation process completed\")\n        return output_file\n    except Exception as e:\n        logger.error(f\"Error in create_video: {str(e)}\")\n        st.error(f\"An error occurred while creating the video: {str(e)}\")\n        return None\n\ndef enhance_clip(clip, script_analysis):\n    # Apply color grading based on sentiment\n    if script_analysis['sentiment'] == 'POSITIVE':\n        clip = apply_color_grading(clip, brightness=1.1, saturation=1.2)\n    elif script_analysis['sentiment'] == 'NEGATIVE':\n        clip = apply_color_grading(clip, brightness=0.9, contrast=1.1)\n    \n    # Add dynamic text animations\n    if clip.duration > 2:\n        text_clip = create_animated_text(clip.scene['title'], duration=2)\n        clip = mpe.CompositeVideoClip([clip, text_clip.set_start(1)])\n    \n    # Add smooth transitions\n    clip = clip.crossfadein(0.5).crossfadeout(0.5)\n    \n    return clip\n\ndef process_clip(enhanced_clip, clip_data, script_analysis):\n    scene = clip_data['scene']\n    duration = float(scene['duration'])\n    processed_clip = enhanced_clip.set_duration(duration)\n    \n    # Apply color grading based on sentiment and style\n    if script_analysis['sentiment'] == 'POSITIVE':\n        processed_clip = apply_color_grading(processed_clip, brightness=1.1)\n    else:\n        processed_clip = apply_color_grading(processed_clip, brightness=0.9)\n    \n    if script_analysis['style'] in ['humorous', 'casual']:\n        processed_clip = processed_clip.fx(vfx.colorx, 1.2)  # More vibrant for humorous/casual content\n    elif script_analysis['style'] in ['dramatic', 'formal']:\n        processed_clip = processed_clip.fx(vfx.lum_contrast, contrast=1.2)  # More contrast for dramatic/formal content\n    \n    if scene.get('overlay_text'):\n        text_clip = create_animated_text(scene['overlay_text'], duration)\n        processed_clip = mpe.CompositeVideoClip([processed_clip, text_clip])\n    \n    lower_third = create_lower_third(scene['title'], duration)\n    processed_clip = mpe.CompositeVideoClip([processed_clip, lower_third])\n    \n    return processed_clip\n\ndef apply_fade_effects(clip, duration=1):\n    try:\n        return fadein(clip, duration).fx(fadeout, duration)\n    except Exception as e:\n        raise ValueError(f\"Error applying fade effects: {e}\")\n\ndef add_text_overlay(clip, text):\n    if text:\n        try:\n            text_clip = mpe.TextClip(text, fontsize=70, color='white', font='Arial-Bold')\n            text_clip = text_clip.set_position('center').set_duration(clip.duration)\n            return mpe.CompositeVideoClip([clip, text_clip])\n        except Exception as e:\n            raise ValueError(f\"Error adding text overlay: {e}\")\n    return clip\n\ndef add_narration(clip, narration_file):\n    try:\n        return clip.set_audio(mpe.AudioFileClip(narration_file))\n    except Exception as e:\n        raise ValueError(f\"Error adding narration: {e}\")\n\ndef add_background_music(clip, music_file):\n    try:\n        background_audio = mpe.AudioFileClip(music_file)\n        return clip.set_audio(mpe.CompositeAudioClip([clip.audio, background_audio.volumex(0.1)]))\n    except Exception as e:\n        raise ValueError(f\"Error adding background music: {e}\")\n\ndef add_watermark(clip, watermark_text=\"Sample Watermark\"):\n    try:\n        watermark = mpe.TextClip(watermark_text, fontsize=30, color='white', font='Arial')\n        watermark = watermark.set_position(('right', 'bottom')).set_duration(clip.duration)\n        return mpe.CompositeVideoClip([clip, watermark])\n    except Exception as e:\n        st.error(f\"Error adding watermark: {e}\")\n        return clip\n\ndef split_video(video_clip, part_duration=10):\n    try:\n        return [video_clip.subclip(start, min(start + part_duration, video_clip.duration)) for start in range(0, int(video_clip.duration), part_duration)]\n    except Exception as e:\n        st.error(f\"Error splitting video: {e}\")\n        return [video_clip]\n\ndef merge_video_parts(video_parts):\n    try:\n        return mpe.concatenate_videoclips(video_parts, method=\"compose\")\n    except Exception as e:\n        st.error(f\"Error merging video parts: {e}\")\n        return video_parts[0] if video_parts else None\n\ndef save_storyboard_backup(storyboard, filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'w') as f:\n            json.dump(storyboard, f)\n        st.success(f\"Storyboard backup saved to {filename}\")\n    except Exception as e:\n        st.error(f\"Error saving storyboard backup: {e}\")\n\ndef load_storyboard_backup(filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'r') as f:\n            storyboard = json.load(f)\n        st.success(f\"Storyboard loaded from {filename}\")\n        return storyboard\n    except FileNotFoundError:\n        st.warning(f\"Backup file {filename} not found.\")\n        return None\n    except json.JSONDecodeError:\n        st.error(f\"Error decoding JSON from {filename}\")\n        return None\n    except Exception as e:\n        st.error(f\"Error loading storyboard backup: {e}\")\n        return None\n\ndef add_subtitles_to_video(clip, subtitles):\n    try:\n        subtitle_clips = [\n            mpe.TextClip(subtitle['text'], fontsize=50, color='white', size=clip.size, font='Arial-Bold')\n            .set_position(('bottom')).set_start(subtitle['start']).set_duration(subtitle['duration'])\n            for subtitle in subtitles\n        ]\n        return mpe.CompositeVideoClip([clip] + subtitle_clips)\n    except Exception as e:\n        st.error(f\"Error adding subtitles: {e}\")\n        return clip\n\ndef preview_storyboard_slideshow(scenes, duration_per_scene=5):\n    try:\n        slides = [create_animated_text(scene['title'], duration=duration_per_scene) for scene in scenes]\n        slideshow = mpe.concatenate_videoclips(slides, method='compose')\n        slideshow.write_videofile(\"storyboard_preview.mp4\", codec='libx264')\n        st.success(\"Storyboard preview created successfully.\")\n        st.video(\"storyboard_preview.mp4\")\n    except Exception as e:\n        st.error(f\"Error creating storyboard slideshow: {e}\")\n\ndef add_logo_to_video(clip, logo_path, position=('right', 'top')):\n    try:\n        logo = mpe.ImageClip(logo_path).set_duration(clip.duration).resize(height=100).set_position(position)\n        return mpe.CompositeVideoClip([clip, logo])\n    except FileNotFoundError:\n        st.error(f\"Logo file not found: {logo_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error adding logo to video: {e}\")\n        return clip\n\ndef compress_video(input_path, output_path=\"compressed_video.mp4\", bitrate=\"500k\"):\n    try:\n        os.system(f\"ffmpeg -i {input_path} -b:v {bitrate} -bufsize {bitrate} {output_path}\")\n        st.success(f\"Video compressed successfully. Saved to {output_path}\")\n    except Exception as e:\n        st.error(f\"Error compressing video: {e}\")\n\ndef apply_bw_filter(clip):\n    try:\n        return clip.fx(mpe.vfx.blackwhite)\n    except Exception as e:\n        st.error(f\"Error applying black-and-white filter: {e}\")\n        return clip\n\ndef overlay_image_on_video(clip, image_path, position=(0, 0)):\n    try:\n        image = mpe.ImageClip(image_path).set_duration(clip.duration).set_position(position)\n        return mpe.CompositeVideoClip([clip, image])\n    except FileNotFoundError:\n        st.error(f\"Image file not found: {image_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error overlaying image on video: {e}\")\n        return clip\n\ndef adjust_video_speed(clip, speed=1.0):\n    try:\n        return clip.fx(mpe.vfx.speedx, speed)\n    except Exception as e:\n        st.error(f\"Error adjusting video speed: {e}\")\n        return clip\n\ndef crop_video(clip, x1, y1, x2, y2):\n    try:\n        return clip.crop(x1=x1, y1=y1, x2=x2, y2=y2)\n    except Exception as e:\n        st.error(f\"Error cropping video: {e}\")\n        return clip\n\ndef adjust_resolution_based_on_system(clip):\n    try:\n        memory = psutil.virtual_memory()\n        resolution = (640, 360) if memory.available < 1000 * 1024 * 1024 else (1280, 720)\n        return resize(clip, newsize=resolution)\n    except Exception as e:\n        st.error(f\"Error adjusting resolution: {e}\")\n        return clip\n\ndef generate_video_thumbnail(clip, output_path=\"thumbnail.png\"):\n    try:\n        frame = clip.get_frame(1)\n        image = Image.fromarray(frame)\n        image.save(output_path)\n        st.success(f\"Thumbnail generated successfully. Saved to {output_path}\")\n        return output_path\n    except Exception as e:\n        st.error(f\"Error generating video thumbnail: {e}\")\n        return None\n\ndef add_intro_outro(final_clip, video_title):\n    intro_clip = create_animated_text(video_title, duration=3, font_size=60).fx(vfx.fadeout, duration=1)\n    outro_clip = create_animated_text(\"Thanks for Watching!\", duration=3, font_size=60).fx(vfx.fadein, duration=1)\n    return mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\n\ndef adjust_audio_volume(audio_clip, volume_level=1.0):\n    try:\n        return audio_clip.volumex(volume_level)\n    except Exception as e:\n        st.error(f\"Error adjusting audio volume: {e}\")\n        return audio_clip\n\ndef generate_gradient_text_overlay(text, clip_duration, size=(1920, 1080)):\n    try:\n        gradient = color_gradient(size, p1=(0, 0), p2=(size[0], size[1]), color1=(255, 0, 0), color2=(0, 0, 255))\n        image = Image.fromarray(gradient)\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default()\n        text_size = draw.textsize(text, font=font)\n        draw.text(((size[0] - text_size[0]) / 2, (size[1] - text_size[1]) / 2), text, font=font, fill=(255, 255, 255))\n        image.save(\"gradient_overlay.png\")\n        return mpe.ImageClip(\"gradient_overlay.png\").set_duration(clip_duration)\n    except Exception as e:\n        st.error(f\"Error generating gradient text overlay: {e}\")\n        return mpe.TextClip(text, fontsize=70, color='white', size=size).set_duration(clip_duration)\n\ndef run_video_rendering_thread(target_function, *args):\n    try:\n        rendering_thread = threading.Thread(target=target_function, args=args)\n        rendering_thread.start()\n        return rendering_thread\n    except Exception as e:\n        st.error(f\"Error running rendering thread: {e}\")\n        return None\n\ndef check_system_capabilities():\n    try:\n        memory = psutil.virtual_memory()\n        if memory.available < 500 * 1024 * 1024:  # Less than 500MB\n            st.warning(\"Low memory detected. Consider closing other applications.\")\n        cpu_usage = psutil.cpu_percent()\n        if cpu_usage > 80:\n            st.warning(\"High CPU usage detected. Rendering may be slow.\")\n    except Exception as e:\n        st.error(f\"Error checking system capabilities: {e}\")\n\ndef log_system_resources():\n    try:\n        memory = psutil.virtual_memory()\n        cpu = psutil.cpu_percent()\n        st.write(f\"Memory Usage: {memory.percent}% | CPU Usage: {cpu}%\")\n    except Exception as e:\n        st.error(f\"Error logging system resources: {e}\")\n\ndef download_additional_assets(url):\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            temp_asset_file = NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_asset_file.write(response.content)\n            temp_asset_file.flush()\n            st.success(f\"Asset downloaded successfully: {temp_asset_file.name}\")\n            return temp_asset_file.name\n        else:\n            st.error(\"Failed to download asset. Invalid URL or server error.\")\n            return None\n    except Exception as e:\n        st.error(f\"Error downloading asset: {e}\")\n        return None\n\ndef calculate_estimated_render_time(duration, resolution=(1280, 720)):\n    try:\n        estimated_time = duration * (resolution[0] * resolution[1]) / 1e6\n        st.info(f\"Estimated rendering time: {estimated_time:.2f} seconds\")\n        return estimated_time\n    except Exception as e:\n        st.error(f\"Error calculating render time: {e}\")\n        return None\n\ndef manage_temp_directory(directory_path):\n    try:\n        if os.path.exists(directory_path):\n            shutil.rmtree(directory_path)\n        os.makedirs(directory_path)\n        st.success(f\"Temporary directory created: {directory_path}\")\n    except Exception as e:\n        st.error(f\"Error managing temporary directory: {e}\")\n\ndef handle_session_expiration():\n    try:\n        st.error(\"Session expired. Please refresh and try again.\")\n        if st.button(\"Refresh Page\"):\n            st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n    except Exception as e:\n        st.error(f\"Error handling session expiration: {e}\")\n\ndef split_storyboard_scenes(scenes, batch_size=5):\n    try:\n        return [scenes[i:i + batch_size] for i in range(0, len(scenes), batch_size)]\n    except Exception as e:\n        st.error(f\"Error splitting storyboard scenes: {e}\")\n        return [scenes]\n\ndef add_transition_effects_between_scenes(scenes):\n    try:\n        return [animate_scene_transition(scene1, scene2) for scene1, scene2 in zip(scenes, scenes[1:])]\n    except Exception as e:\n        st.error(f\"Error adding transition effects: {e}\")\n        return scenes\n\ndef optimize_storyboard_text_prompts(scenes):\n    try:\n        for scene in scenes:\n            scene['title'] = scene['title'].capitalize()\n        return scenes\n    except Exception as e:\n        st.error(f\"Error optimizing storyboard text prompts: {e}\")\n        return scenes\n\ndef select_background_music(genre):\n    try:\n        if genre in MUSIC_TRACKS:\n            track_url = MUSIC_TRACKS[genre]\n        else:\n            track_url = random.choice(list(MUSIC_TRACKS.values()))\n        \n        response = requests.get(track_url)\n        if response.status_code == 200:\n            temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_audio_file.write(response.content)\n            temp_audio_file.close()\n            return temp_audio_file.name\n    except Exception as e:\n        logger.error(f\"Error selecting background music: {str(e)}\")\n    \n    return None\n\ndef analyze_script(script):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Analyze the script and provide a JSON response with keys: 'sentiment' (POSITIVE, NEGATIVE, or NEUTRAL), 'style' (e.g., formal, casual, humorous), and 'transitions' (list of transition types).\"},\n                {\"role\": \"user\", \"content\": f\"Analyze this script: {script}\"}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        analysis = json.loads(response.choices[0].message.content)\n        \n        if not all(key in analysis for key in ['sentiment', 'style', 'transitions']):\n            raise ValueError(\"Invalid analysis structure\")\n        \n        return analysis\n    except Exception as e:\n        logger.error(f\"Error analyzing script: {str(e)}\")\n        return {\n            'sentiment': 'NEUTRAL',\n            'style': 'formal',\n            'transitions': ['fade', 'cut']\n        }\n\ndef apply_transition(clip1, clip2, transition_type):\n    transition_functions = {\n        'fade': lambda: clip1.crossfadeout(1).crossfadein(1),\n        'slide': lambda: clip1.slide_out(1, 'left').slide_in(1, 'right'),\n        'whip': lambda: clip1.fx(vfx.speedx, 2).fx(vfx.crop, x1=0, y1=0, x2=0.5, y2=1).crossfadeout(0.5),\n        'zoom': lambda: clip1.fx(vfx.resize, 1.5).fx(vfx.crop, x_center=0.5, y_center=0.5, width=1/1.5, height=1/1.5).crossfadeout(1)\n    }\n    return transition_functions.get(transition_type, lambda: clip1)()\n\ndef cleanup_temp_files():\n    try:\n        temp_dir = tempfile.gettempdir()\n        for filename in os.listdir(temp_dir):\n            if filename.startswith('videocreator_'):\n                file_path = os.path.join(temp_dir, filename)\n                if os.path.isfile(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n        logger.info(\"Temporary files cleaned up successfully.\")\n    except Exception as e:\n        logger.error(f\"Error cleaning up temporary files: {str(e)}\")\n\ndef process_scene_with_progress(scene, index, total_scenes):\n    scene_progress = st.empty()\n    scene_progress.text(f\"Processing scene {index + 1} of {total_scenes}: {scene['title']}\")\n    \n    clip_progress, voice_progress = st.columns(2)\n    \n    with clip_progress:\n        st.text(\"Creating video clip...\")\n        clip = create_fallback_clip(scene)\n        st.success(\"Video clip processed\")\n    \n    with voice_progress:\n        st.text(\"Generating voiceover...\")\n        narration_file = generate_voiceover(scene['narration'])\n        st.success(\"Voiceover generated\")\n    \n    scene_progress.success(f\"Scene {index + 1} processed successfully!\")\n    return {'clip': clip, 'scene': scene, 'narration': narration_file}\n\ndef generate_valid_storyboard(prompt, style, max_attempts=3):\n    for attempt in range(max_attempts):\n        storyboard = generate_storyboard(prompt, style)\n        if storyboard is not None:\n            return storyboard\n        logger.warning(f\"Storyboard generation attempt {attempt + 1} failed. Retrying...\")\n    logger.error(\"Failed to generate a valid storyboard after multiple attempts.\")\n    st.error(\"Failed to generate a valid storyboard after multiple attempts. Please try again with a different prompt or style.\")\n    return None\n\ndef prompt_card(prompt):\n    st.markdown(f\"**Sample Prompt:** {prompt}\")\n    if st.button(\"Use this prompt\", key=f\"btn_{prompt}\"):\n        st.session_state.prompt = prompt\n\ndef predict_processing_issues(video_clips, system_resources):\n    potential_issues = []\n    if len(video_clips) * 5 > system_resources['available_memory'] / 1e6:  # Assuming 5 seconds per clip\n        potential_issues.append(\"Insufficient memory for processing all clips\")\n    if system_resources['cpu_usage'] > 80:\n        potential_issues.append(\"High CPU usage may slow down processing\")\n    return potential_issues\n\ndef main():\n    st.markdown(\"<h1 style='text-align: center; color: #4A90E2;'>AutovideoAI</h1>\", unsafe_allow_html=True)\n    st.markdown(\"<p style='text-align: center; font-size: 1.2em;'>Create Amazing Videos with AI</p>\", unsafe_allow_html=True)\n\n    with st.expander(\"\u2139\ufe0f How to use AutovideoAI\", expanded=False):\n        st.markdown(\"\"\"\n        1. Enter your video idea or choose a sample prompt.\n        2. Customize your video style, duration, and background music.\n        3. Generate a storyboard and preview it.\n        4. Create your AI-powered video!\n        \"\"\")\n\n    col1, col2 = st.columns([2, 1])\n    \n    with col1:\n        st.subheader(\"1\ufe0f\u20e3 Enter Your Video Idea\")\n        prompt = st.text_area(\"What's your video about?\", height=100, value=st.session_state.get('prompt', ''))\n    \n    with col2:\n        st.subheader(\"Sample Prompts\")\n        for sample_prompt in SAMPLE_PROMPTS:\n            if st.button(f\"\ud83d\udccc {sample_prompt}\", key=f\"btn_{sample_prompt}\"):\n                st.session_state.prompt = sample_prompt\n                st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n\n    st.subheader(\"2\ufe0f\u20e3 Customize Your Video\")\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        style = st.selectbox(\"Video Style \ud83c\udfad\", [\"Motivational\", \"Dramatic\", \"Educational\", \"Funny\"])\n    with col2:\n        duration = st.slider(\"Estimated Duration \u23f1\ufe0f\", 30, 300, 60, help=\"Duration in seconds\")\n    with col3:\n        music_style = st.selectbox(\"Background Music \ud83c\udfb5\", list(MUSIC_TRACKS.keys()))\n\n    if st.button(\"\ud83d\udd8b\ufe0f Generate Storyboard\", use_container_width=True):\n        with st.spinner(\"Crafting your storyboard...\"):\n            storyboard = generate_script(prompt, duration)\n        if storyboard:\n            st.session_state.storyboard = storyboard\n            st.success(\"\u2705 Storyboard generated successfully!\")\n            display_storyboard_preview(storyboard)\n        else:\n            st.error(\"\u274c Failed to generate a storyboard. Please try again.\")\n\n    if 'storyboard' in st.session_state:\n        if st.button(\"\ud83c\udfac Create Video\", use_container_width=True):\n            create_video_workflow(prompt, duration, music_style)\n\ndef create_video_workflow(prompt, duration, music_style):\n    try:\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n\n        logger.info(\"Starting video creation workflow\")\n        status_text.text(\"Generating script...\")\n        script = generate_script(prompt, duration)\n        if not script:\n            raise ValueError(\"Failed to generate script\")\n        logger.info(\"Script generated successfully\")\n        progress_bar.progress(20)\n\n        scenes = script['scenes']\n        if not scenes:\n            logger.error(f\"No scenes found in the script. Script content: {script}\")\n            raise ValueError(\"No scenes found in the script\")\n        logger.info(f\"Successfully extracted {len(scenes)} scenes\")\n        progress_bar.progress(30)\n\n        status_text.text(\"Fetching video clips...\")\n        video_clips = fetch_video_clips(scenes)\n        if not video_clips:\n            raise ValueError(\"Failed to fetch video clips\")\n        progress_bar.progress(50)\n\n        status_text.text(\"Generating voiceovers...\")\n        for clip_data in video_clips:\n            clip_data['narration'] = generate_voiceover(clip_data['scene']['narration'])\n        progress_bar.progress(70)\n\n        status_text.text(\"Selecting background music...\")\n        background_music_file = select_background_music(music_style)\n        progress_bar.progress(80)\n\n        status_text.text(\"Creating your video...\")\n        video_file = create_video(video_clips, background_music_file, script['title'])\n        progress_bar.progress(90)\n\n        if video_file and os.path.exists(video_file):\n            status_text.text(\"Finalizing...\")\n            progress_bar.progress(100)\n            logger.info(\"Video creation successful\")\n            st.success(\"\ud83c\udf89 Video created successfully!\")\n            st.video(video_file)\n            with open(video_file, 'rb') as vf:\n                st.download_button(label=\"\ud83d\udce5 Download Video\", data=vf, file_name=\"AutovideoAI_creation.mp4\")\n        else:\n            logger.error(\"Failed to create the video\")\n            st.error(\"\u274c Failed to create the video. Please try again.\")\n    except Exception as e:\n        logger.error(f\"An error occurred during video creation: {str(e)}\")\n        st.error(f\"An error occurred: {str(e)}\")\n    finally:\n        status_text.empty()\n        progress_bar.empty()\n\ndef generate_script(prompt, duration):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert video scriptwriter. Create a JSON script for a video based on the given prompt and duration. Use the following schema:\"},\n                {\"role\": \"user\", \"content\": f\"\"\"\n                Create a script for a {duration} second video about: {prompt}. \n                Return the script as a JSON object with the following structure:\n                {{\n                    \"title\": \"Overall video title\",\n                    \"scenes\": [\n                        {{\n                            \"scene_number\": 1,\n                            \"title\": \"Scene title\",\n                            \"description\": \"Detailed scene description\",\n                            \"narration\": \"Narration text for the scene\",\n                            \"duration\": \"Duration in seconds\"\n                        }}\n                    ]\n                }}\n                Include 3-5 scenes in total.\n                \"\"\"}\n            ],\n            max_tokens=1000,\n            response_format={\"type\": \"json_object\"}\n        )\n        script = json.loads(response.choices[0].message.content)\n        logger.info(f\"Generated script: {script}\")\n        return script\n    except Exception as e:\n        logger.error(f\"Error generating script: {str(e)}\")\n        return None\n\ndef display_storyboard_preview(storyboard):\n    with st.expander(\"\ud83d\udd0d Preview Storyboard\", expanded=True):\n        st.markdown(f\"### {storyboard['title']}\")\n        st.markdown(\"### Scene Breakdown\")\n        for scene in storyboard['scenes']:\n            with st.container():\n                col1, col2 = st.columns([1, 2])\n                with col1:\n                    st.markdown(f\"**Scene {scene['scene_number']}**\")\n                    st.write(f\"Duration: {scene['duration']} seconds\")\n                with col2:\n                    st.markdown(f\"**{scene['title']}**\")\n                    st.write(f\"{scene['description']}\")\n                st.markdown(\"---\")\n\ndef color_gradient(size, p1, p2, color1, color2):\n    x = np.linspace(0, 1, size[0])[:, None]\n    y = np.linspace(0, 1, size[1])[None, :]\n    gradient = x * (p2[0] - p1[0]) + y * (p2[1] - p1[1])\n    gradient = np.clip(gradient, 0, 1)\n    return np.array(color1) * (1 - gradient[:, :, None]) + np.array(color2) * gradient[:, :, None]", "optimized_sections": {"settings": ["logger = logging.getLogger(__name__)", "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")", "api = HfApi()", "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))", "SAMPLE_PROMPTS = [", "    \"Create a motivational video about overcoming challenges\",", "    \"Make an educational video explaining photosynthesis\",", "    \"Design a funny video about the struggles of working from home\",", "]", "MUSIC_TRACKS = {", "    \"Electronic\": \"https://www.bensound.com/bensound-dubstep\",", "    \"Experimental\": \"https://www.bensound.com/bensound-enigmatic\",", "    \"Folk\": \"https://www.bensound.com/bensound-acousticbreeze\",", "    \"Hip-Hop\": \"https://www.bensound.com/bensound-groovyhiphop\",", "    \"Instrumental\": \"https://www.bensound.com/bensound-pianomoment\",", "    \"Pop\": \"https://www.bensound.com/bensound-ukulele\",", "    \"Rock\": \"https://www.bensound.com/bensound-extremeaction\"", "}", "response = requests.get(url)", "required_fields = [\"scene_number\", \"title\", \"description\", \"narration\", \"keywords\", \"duration\", \"overlay_text\", \"visual_elements\", \"audio_cues\", \"transitions\"]", "video_clips = []", "api = HfApi()", "text = scene.get('title', 'Scene')", "size = (1280, 720)", "img = Image.new('RGB', size, color='black')", "draw = ImageDraw.Draw(img)", "font = ImageFont.load_default()", "wrapped_text = textwrap.wrap(text, width=20)", "y_text = (size[1] - len(wrapped_text) * 80) // 2", "img_array = np.array(img)", "clip = mpe.ImageClip(img_array).set_duration(duration)", "silent_segment = AudioSegment.silent(duration=int(duration * 1000))", "temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')", "clip = clip.crossfadein(0.5).crossfadeout(0.5)", "scene = clip_data['scene']", "duration = float(scene['duration'])", "processed_clip = enhanced_clip.set_duration(duration)", "lower_third = create_lower_third(scene['title'], duration)", "processed_clip = mpe.CompositeVideoClip([processed_clip, lower_third])", "intro_clip = create_animated_text(video_title, duration=3, font_size=60).fx(vfx.fadeout, duration=1)", "outro_clip = create_animated_text(\"Thanks for Watching!\", duration=3, font_size=60).fx(vfx.fadein, duration=1)", "transition_functions = {", "        'fade': lambda: clip1.crossfadeout(1).crossfadein(1),", "        'slide': lambda: clip1.slide_out(1, 'left').slide_in(1, 'right'),", "        'whip': lambda: clip1.fx(vfx.speedx, 2).fx(vfx.crop, x1=0, y1=0, x2=0.5, y2=1).crossfadeout(0.5),", "        'zoom': lambda: clip1.fx(vfx.resize, 1.5).fx(vfx.crop, x_center=0.5, y_center=0.5, width=1/1.5, height=1/1.5).crossfadeout(1)", "    }", "scene_progress = st.empty()", "potential_issues = []", "video_clips = []", "api = HfApi()", "x = np.linspace(0, 1, size[0])[:, None]", "y = np.linspace(0, 1, size[1])[None, :]", "gradient = x * (p2[0] - p1[0]) + y * (p2[1] - p1[1])", "gradient = np.clip(gradient, 0, 1)", "input_text = f\"\"\"Generate a detailed {style} video storyboard based on this prompt: \"{prompt}\"", "        Provide the storyboard in JSON format with the following structure:", "        {{", "            \"title\": \"Overall video title\",", "            \"scenes\": [", "                {{", "                    \"scene_number\": 1,", "                    \"title\": \"Scene title\",", "                    \"description\": \"Detailed scene description\",", "                    \"narration\": \"Narration text for the scene\",", "                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],", "                    \"duration\": \"Duration in seconds\",", "                    \"visual_elements\": [\"List of visual elements to include\"],", "                    \"transitions\": {{", "                        \"in\": \"Transition type for entering the scene\",", "                        \"out\": \"Transition type for exiting the scene\"", "                    }}", "                }}", "            ],", "            \"target_audience\": \"Description of the target audience\",", "            \"overall_tone\": \"Description of the overall tone of the video\"", "        }}", "        Ensure there are at least 3 scenes in the storyboard.\"\"\"", "response = client.chat.completions.create(", "            model=\"gpt-4o-mini\",", "            messages=[", "                {\"role\": \"system\", \"content\": \"You are a creative video storyboard generator. Respond with valid JSON following the specified structure.\"},", "                {\"role\": \"user\", \"content\": input_text}", "            ],", "            response_format={\"type\": \"json_object\"},", "            temperature=0.7", "        )", "storyboard = json.loads(response.choices[0].message.content)", "position = ((size[0] - line_width) / 2, y_text)", "tts = gTTS(text=narration_text, lang='en', slow=False)", "temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')", "gradient = np.linspace(0, 255, 1280)", "background = np.tile(gradient, (720, 1)).astype(np.uint8)", "background = np.stack((background,) * 3, axis=-1)", "clip = mpe.ImageClip(background).set_duration(duration)", "txt_clip = mpe.TextClip(scene['title'], fontsize=70, color='white', font='Arial-Bold')", "txt_clip = txt_clip.set_position('center').set_duration(duration)", "clip = clip.fadein(0.5).fadeout(0.5)", "img = Image.new('RGB', (1280, 720), color='black')", "draw = ImageDraw.Draw(img)", "font = ImageFont.load_default().font_variant(size=font_size)", "text_bbox = draw.textbbox((0, 0), text, font=font)", "text_width = text_bbox[2] - text_bbox[0]", "text_height = text_bbox[3] - text_bbox[1]", "position = ((1280 - text_width) / 2, (720 - text_height) / 2)", "img_array = np.array(img)", "clip = mpe.ImageClip(img_array).set_duration(duration)", "clip = clip.fadein(1).fadeout(1)", "img = Image.new('RGBA', (1280, 720), (0, 0, 0, 0))", "draw = ImageDraw.Draw(img)", "font = ImageFont.load_default().font_variant(size=30)", "text_bbox = draw.textbbox((0, 0), text, font=font)", "text_width = text_bbox[2] - text_bbox[0]", "text_height = text_bbox[3] - text_bbox[1]", "position = ((1280 - text_width) / 2, 720 - text_height - 50)", "bg_bbox = (position[0]-10, position[1]-10, position[0]+text_width+10, position[1]+text_height+10)", "img_array = np.array(img)", "chunks = split_on_silence(audio_segment, min_silence_len=500, silence_thresh=-40)", "cut_times = [0]", "cut_clips = [video_clip.subclip(start, end) for start, end in zip(cut_times[:-1], cut_times[1:])]", "frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in clip.iter_frames()]", "motion = [np.mean(cv2.absdiff(frames[i], frames[i+1])) for i in range(len(frames)-1)]", "speed_clip = clip.fl(lambda gf, t: gf(t * speed_factor) if motion[int(t*clip.fps)] < threshold else gf(t))", "clips = []", "final_clip = mpe.concatenate_videoclips(clips)", "intro_clip = mpe.TextClip(video_title, fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)", "outro_clip = mpe.TextClip(\"Thanks for watching!\", fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)", "final_clip = mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])", "output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name", "clip = apply_color_grading(clip, brightness=1.1, saturation=1.2)", "text_clip = create_animated_text(clip.scene['title'], duration=2)", "clip = mpe.CompositeVideoClip([clip, text_clip.set_start(1)])", "processed_clip = apply_color_grading(processed_clip, brightness=1.1)", "processed_clip = apply_color_grading(processed_clip, brightness=0.9)", "processed_clip = processed_clip.fx(vfx.colorx, 1.2)", "text_clip = create_animated_text(scene['overlay_text'], duration)", "processed_clip = mpe.CompositeVideoClip([processed_clip, text_clip])", "background_audio = mpe.AudioFileClip(music_file)", "watermark = mpe.TextClip(watermark_text, fontsize=30, color='white', font='Arial')", "watermark = watermark.set_position(('right', 'bottom')).set_duration(clip.duration)", "subtitle_clips = [", "            mpe.TextClip(subtitle['text'], fontsize=50, color='white', size=clip.size, font='Arial-Bold')", "            .set_position(('bottom')).set_start(subtitle['start']).set_duration(subtitle['duration'])", "            for subtitle in subtitles", "        ]", "slides = [create_animated_text(scene['title'], duration=duration_per_scene) for scene in scenes]", "slideshow = mpe.concatenate_videoclips(slides, method='compose')", "logo = mpe.ImageClip(logo_path).set_duration(clip.duration).resize(height=100).set_position(position)", "image = mpe.ImageClip(image_path).set_duration(clip.duration).set_position(position)", "memory = psutil.virtual_memory()", "resolution = (640, 360) if memory.available < 1000 * 1024 * 1024 else (1280, 720)", "frame = clip.get_frame(1)", "image = Image.fromarray(frame)", "gradient = color_gradient(size, p1=(0, 0), p2=(size[0], size[1]), color1=(255, 0, 0), color2=(0, 0, 255))", "image = Image.fromarray(gradient)", "draw = ImageDraw.Draw(image)", "font = ImageFont.load_default()", "text_size = draw.textsize(text, font=font)", "rendering_thread = threading.Thread(target=target_function, args=args)", "memory = psutil.virtual_memory()", "cpu_usage = psutil.cpu_percent()", "memory = psutil.virtual_memory()", "cpu = psutil.cpu_percent()", "response = requests.get(url)", "estimated_time = duration * (resolution[0] * resolution[1]) / 1e6", "response = requests.get(track_url)", "response = client.chat.completions.create(", "            model=\"gpt-4o-mini\",", "            messages=[", "                {\"role\": \"system\", \"content\": \"Analyze the script and provide a JSON response with keys: 'sentiment' (POSITIVE, NEGATIVE, or NEUTRAL), 'style' (e.g., formal, casual, humorous), and 'transitions' (list of transition types).\"},", "                {\"role\": \"user\", \"content\": f\"Analyze this script: {script}\"}", "            ],", "            response_format={\"type\": \"json_object\"},", "            temperature=0.7", "        )", "analysis = json.loads(response.choices[0].message.content)", "temp_dir = tempfile.gettempdir()", "clip = create_fallback_clip(scene)", "narration_file = generate_voiceover(scene['narration'])", "storyboard = generate_storyboard(prompt, style)", "response = client.chat.completions.create(", "            model=\"gpt-4o-mini\",", "            messages=[", "                {\"role\": \"system\", \"content\": \"You are an expert video scriptwriter. Create a JSON script for a video based on the given prompt and duration. Use the following schema:\"},", "                {\"role\": \"user\", \"content\": f\"\"\"", "                Create a script for a {duration} second video about: {prompt}. ", "                Return the script as a JSON object with the following structure:", "                {{", "                    \"title\": \"Overall video title\",", "                    \"scenes\": [", "                        {{", "                            \"scene_number\": 1,", "                            \"title\": \"Scene title\",", "                            \"description\": \"Detailed scene description\",", "                            \"narration\": \"Narration text for the scene\",", "                            \"duration\": \"Duration in seconds\"", "                        }}", "                    ]", "                }}", "                Include 3-5 scenes in total.", "                \"\"\"}", "            ],", "            max_tokens=1000,", "            response_format={\"type\": \"json_object\"}", "        )", "script = json.loads(response.choices[0].message.content)", "keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']", "search_query = \" OR \".join(keywords)", "progress_bar = st.progress(0)", "status_text = st.empty()", "script = generate_script(prompt, duration)", "scenes = script['scenes']", "video_clips = fetch_video_clips(scenes)", "background_music_file = select_background_music(music_style)", "video_file = create_video(video_clips, background_music_file, script['title'])", "prompt = st.text_area(\"What's your video about?\", height=100, value=st.session_state.get('prompt', ''))", "style = st.selectbox(\"Video Style \ud83c\udfad\", [\"Motivational\", \"Dramatic\", \"Educational\", \"Funny\"])", "duration = st.slider(\"Estimated Duration \u23f1\ufe0f\", 30, 300, 60, help=\"Duration in seconds\")", "music_style = st.selectbox(\"Background Music \ud83c\udfb5\", list(MUSIC_TRACKS.keys()))", "keywords_txt = \", \".join(scene['keywords'])", "keywords_clip = mpe.TextClip(keywords_txt, fontsize=30, color='yellow', font='Arial')", "keywords_clip = keywords_clip.set_position(('center', 0.8), relative=True).set_duration(duration)", "clip = mpe.CompositeVideoClip([clip, txt_clip, keywords_clip])", "clip = mpe.CompositeVideoClip([clip, txt_clip])", "audio_segment = AudioSegment.from_wav(audio_clip)", "clip = clip.set_fps(24)", "background_music = mpe.AudioFileClip(background_music_file).volumex(0.1)", "background_music = background_music.audio_loop(duration=final_clip.duration)", "final_audio = mpe.CompositeAudioClip([final_clip.audio, background_music])", "final_clip = final_clip.set_audio(final_audio)", "clip = apply_color_grading(clip, brightness=0.9, contrast=1.1)", "processed_clip = processed_clip.fx(vfx.lum_contrast, contrast=1.2)", "text_clip = mpe.TextClip(text, fontsize=70, color='white', font='Arial-Bold')", "text_clip = text_clip.set_position('center').set_duration(clip.duration)", "storyboard = json.load(f)", "temp_asset_file = NamedTemporaryFile(delete=False, suffix='.mp3')", "track_url = MUSIC_TRACKS[genre]", "track_url = random.choice(list(MUSIC_TRACKS.values()))", "temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')", "search_results = api.list_datasets(search=search_query, limit=20)", "matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]", "storyboard = generate_script(prompt, duration)", "keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']", "search_query = \" OR \".join(keywords)", "search_results = api.list_datasets(search=search_query, limit=20)", "matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]", "audio_array = audio_clip.to_soundarray()", "audio_segment = AudioSegment(", "                audio_array.tobytes(),", "                frame_rate=audio_clip.fps,", "                sample_width=audio_array.dtype.itemsize,", "                channels=1 if audio_array.ndim == 1 else audio_array.shape[1]", "            )", "clip = clip_data['clip']", "narration = clip_data['narration']", "clip = clip.set_audio(narration)", "file_path = os.path.join(temp_dir, filename)", "chosen_dataset = random.choice(matching_datasets)", "dataset_info = api.dataset_info(chosen_dataset.id)", "clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))", "chosen_dataset = random.choice(matching_datasets)", "dataset_info = api.dataset_info(chosen_dataset.id)", "sample = random.choice(dataset_info.card_data['samples'])", "sample = random.choice(dataset_info.card_data['samples'])", "clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))", "video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])", "clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))", "video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])", "clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))"], "function_definitions": {"toggle_theme": "def toggle_theme():\n    st.session_state.theme = 'dark' if st.session_state.theme == 'light' else 'light'", "cached_download": "def cached_download(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.content\n    return None", "generate_storyboard": "def generate_storyboard(prompt, style=\"motivational\"):\n    try:\n        input_text = f\"\"\"Generate a detailed {style} video storyboard based on this prompt: \"{prompt}\"\n        Provide the storyboard in JSON format with the following structure:\n        {{\n            \"title\": \"Overall video title\",\n            \"scenes\": [\n                {{\n                    \"scene_number\": 1,\n                    \"title\": \"Scene title\",\n                    \"description\": \"Detailed scene description\",\n                    \"narration\": \"Narration text for the scene\",\n                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n                    \"duration\": \"Duration in seconds\",\n                    \"visual_elements\": [\"List of visual elements to include\"],\n                    \"transitions\": {{\n                        \"in\": \"Transition type for entering the scene\",\n                        \"out\": \"Transition type for exiting the scene\"\n                    }}\n                }}\n            ],\n            \"target_audience\": \"Description of the target audience\",\n            \"overall_tone\": \"Description of the overall tone of the video\"\n        }}\n        Ensure there are at least 3 scenes in the storyboard.\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative video storyboard generator. Respond with valid JSON following the specified structure.\"},\n                {\"role\": \"user\", \"content\": input_text}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        storyboard = json.loads(response.choices[0].message.content)\n        return storyboard\n    except Exception as e:\n        logger.error(f\"Error generating storyboard: {str(e)}\")\n        st.error(\"An error occurred while generating the storyboard. Please try again.\")\n    return None", "validate_storyboard": "def validate_storyboard(storyboard):\n    if \"title\" not in storyboard or \"scenes\" not in storyboard or not isinstance(storyboard[\"scenes\"], list):\n        return False\n    if len(storyboard[\"scenes\"]) < 3:\n        return False\n    \n    required_fields = [\"scene_number\", \"title\", \"description\", \"narration\", \"keywords\", \"duration\", \"overlay_text\", \"visual_elements\", \"audio_cues\", \"transitions\"]\n    return all(all(field in scene for field in required_fields) for scene in storyboard[\"scenes\"])", "parse_storyboard": "def parse_storyboard(storyboard):\n    try:\n        return json.loads(storyboard).get(\"scenes\", [])\n    except json.JSONDecodeError:\n        return []", "fetch_video_clips": "def fetch_video_clips(scenes):\n    logger.info(f\"Fetching video clips for {len(scenes)} scenes\")\n    video_clips = []\n    \n    api = HfApi()\n    \n    for i, scene in enumerate(scenes):\n        logger.info(f\"Fetching clip for scene {i+1}: {scene['title']}\")\n        \n        # Expand the search query to include more relevant terms\n        keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\n        search_query = \" OR \".join(keywords)\n        \n        try:\n            search_results = api.list_datasets(search=search_query, limit=20)  # Increase the limit\n            matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\n            \n            if matching_datasets:\n                chosen_dataset = random.choice(matching_datasets)\n                dataset_info = api.dataset_info(chosen_dataset.id)\n                \n                if dataset_info.card_data and 'samples' in dataset_info.card_data:\n                    sample = random.choice(dataset_info.card_data['samples'])\n                    if 'video' in sample:\n                        video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\n                        clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))  # Limit to 10 seconds max\n                        logger.info(f\"Clip fetched for scene {i+1}: duration={clip.duration}s\")\n                    else:\n                        raise ValueError(\"No video found in the sample\")\n                else:\n                    raise ValueError(\"No samples found in the dataset\")\n            else:\n                raise ValueError(\"No matching datasets found\")\n        \n        except Exception as e:\n            logger.warning(f\"Error fetching video for scene {i+1}: {str(e)}. Creating fallback clip.\")\n            clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\n        \n        video_clips.append({'clip': clip, 'scene': scene})\n    \n    return video_clips", "create_fallback_clip": "def create_fallback_clip(scene, duration=5):\n    text = scene.get('title', 'Scene')\n    size = (1280, 720)\n    \n    # Create a black background\n    img = Image.new('RGB', size, color='black')\n    draw = ImageDraw.Draw(img)\n    \n    # Use a default font\n    font = ImageFont.load_default()\n    \n    # Wrap text\n    wrapped_text = textwrap.wrap(text, width=20)\n    \n    # Calculate text position\n    y_text = (size[1] - len(wrapped_text) * 80) // 2\n    \n    # Draw text\n    for line in wrapped_text:\n        line_width, line_height = draw.textbbox((0, 0), line, font=font)[2:]\n        position = ((size[0] - line_width) / 2, y_text)\n        draw.text(position, line, font=font, fill='white')\n        y_text += line_height + 10\n    \n    # Convert PIL Image to numpy array\n    img_array = np.array(img)\n    \n    # Create video clip from the image\n    clip = mpe.ImageClip(img_array).set_duration(duration)\n    \n    return clip", "generate_voiceover": "def generate_voiceover(narration_text):\n    logger.info(f\"Generating voiceover for text: {narration_text[:50]}...\")\n    try:\n        tts = gTTS(text=narration_text, lang='en', slow=False)\n        temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n        tts.save(temp_audio_file.name)\n        return mpe.AudioFileClip(temp_audio_file.name)\n    except Exception as e:\n        logger.error(f\"Error generating voiceover: {str(e)}\")\n        return create_silent_audio(len(narration_text.split()) / 2)", "create_silent_audio": "def create_silent_audio(duration):\n    silent_segment = AudioSegment.silent(duration=int(duration * 1000))  # pydub uses milliseconds\n    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n    silent_segment.export(temp_audio_file.name, format=\"mp3\")\n    return mpe.AudioFileClip(temp_audio_file.name)", "create_scene_clip": "def create_scene_clip(scene, duration=5):\n    try:\n        # Create a gradient background\n        gradient = np.linspace(0, 255, 1280)\n        background = np.tile(gradient, (720, 1)).astype(np.uint8)\n        background = np.stack((background,) * 3, axis=-1)\n        \n        # Create video clip from the background\n        clip = mpe.ImageClip(background).set_duration(duration)\n        \n        # Add text\n        txt_clip = mpe.TextClip(scene['title'], fontsize=70, color='white', font='Arial-Bold')\n        txt_clip = txt_clip.set_position('center').set_duration(duration)\n        \n        # Add keywords as subtitles\n        if 'keywords' in scene:\n            keywords_txt = \", \".join(scene['keywords'])\n            keywords_clip = mpe.TextClip(keywords_txt, fontsize=30, color='yellow', font='Arial')\n            keywords_clip = keywords_clip.set_position(('center', 0.8), relative=True).set_duration(duration)\n            clip = mpe.CompositeVideoClip([clip, txt_clip, keywords_clip])\n        else:\n            clip = mpe.CompositeVideoClip([clip, txt_clip])\n        \n        # Add fade in and out\n        clip = clip.fadein(0.5).fadeout(0.5)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating scene clip: {str(e)}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)", "add_fade_transition": "def add_fade_transition(clip1, clip2, duration=1):\n    return mpe.CompositeVideoClip([clip1.crossfadeout(duration), clip2.crossfadein(duration)])", "create_animated_text": "def create_animated_text(text, duration=5, font_size=70, color='white'):\n    try:\n        # Create a black background image\n        img = Image.new('RGB', (1280, 720), color='black')\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=font_size)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position to center the text\n        position = ((1280 - text_width) / 2, (720 - text_height) / 2)\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill=color)\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        clip = mpe.ImageClip(img_array).set_duration(duration)\n        \n        # Add fade in and fade out effects\n        clip = clip.fadein(1).fadeout(1)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating animated text: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)", "apply_color_grading": "def apply_color_grading(clip, brightness=1.0, contrast=1.0, saturation=1.0):\n    return clip.fx(vfx.colorx, brightness).fx(vfx.lum_contrast, contrast=contrast).fx(vfx.colorx, saturation)", "create_lower_third": "def create_lower_third(text, duration):\n    try:\n        # Create a transparent background\n        img = Image.new('RGBA', (1280, 720), (0, 0, 0, 0))\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=30)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position for lower third\n        position = ((1280 - text_width) / 2, 720 - text_height - 50)\n        \n        # Draw semi-transparent background\n        bg_bbox = (position[0]-10, position[1]-10, position[0]+text_width+10, position[1]+text_height+10)\n        draw.rectangle(bg_bbox, fill=(0,0,0,153))\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill='white')\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        return mpe.ImageClip(img_array).set_duration(duration)\n    except Exception as e:\n        logger.error(f\"Error creating lower third: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0,0)).set_duration(duration)", "smart_cut": "def smart_cut(video_clip, audio_clip):\n    try:\n        # Check if audio_clip is a file path or an AudioClip object\n        if isinstance(audio_clip, str):\n            audio_segment = AudioSegment.from_wav(audio_clip)\n        elif isinstance(audio_clip, mpe.AudioClip):\n            # Convert AudioClip to numpy array\n            audio_array = audio_clip.to_soundarray()\n            audio_segment = AudioSegment(\n                audio_array.tobytes(),\n                frame_rate=audio_clip.fps,\n                sample_width=audio_array.dtype.itemsize,\n                channels=1 if audio_array.ndim == 1 else audio_array.shape[1]\n            )\n        else:\n            logger.warning(\"Unsupported audio type. Returning original video clip.\")\n            return video_clip\n\n        # Split audio on silences\n        chunks = split_on_silence(audio_segment, min_silence_len=500, silence_thresh=-40)\n        \n        # Calculate timestamps for cuts\n        cut_times = [0]\n        for chunk in chunks:\n            cut_times.append(cut_times[-1] + len(chunk) / 1000)\n        \n        # Cut video based on audio\n        cut_clips = [video_clip.subclip(start, end) for start, end in zip(cut_times[:-1], cut_times[1:])]\n        \n        return mpe.concatenate_videoclips(cut_clips)\n    except Exception as e:\n        logger.error(f\"Error in smart_cut: {str(e)}\")\n        return video_clip", "apply_speed_changes": "def apply_speed_changes(clip, speed_factor=1.5, threshold=0.1):\n    try:\n        if not hasattr(clip, 'fps') or clip.fps is None:\n            clip = clip.set_fps(24)  # Set a default fps if not present\n        \n        frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in clip.iter_frames()]\n        motion = [np.mean(cv2.absdiff(frames[i], frames[i+1])) for i in range(len(frames)-1)]\n        \n        speed_clip = clip.fl(lambda gf, t: gf(t * speed_factor) if motion[int(t*clip.fps)] < threshold else gf(t))\n        \n        return speed_clip\n    except Exception as e:\n        logger.error(f\"Error in apply_speed_changes: {str(e)}\")\n        return clip", "create_video": "def create_video(video_clips, background_music_file, video_title):\n    logger.info(f\"Starting video creation process for '{video_title}'\")\n    try:\n        clips = []\n        for i, clip_data in enumerate(video_clips):\n            try:\n                clip = clip_data['clip']\n                narration = clip_data['narration']\n                clip = clip.set_audio(narration)\n                clips.append(clip)\n                logger.info(f\"Processed clip {i+1} successfully\")\n            except Exception as e:\n                logger.error(f\"Error processing clip {i+1}: {str(e)}\")\n        \n        if not clips:\n            raise ValueError(\"No valid clips were created\")\n        \n        logger.info(f\"Concatenating {len(clips)} scene clips\")\n        final_clip = mpe.concatenate_videoclips(clips)\n        \n        if background_music_file:\n            logger.info(\"Adding background music\")\n            background_music = mpe.AudioFileClip(background_music_file).volumex(0.1)\n            background_music = background_music.audio_loop(duration=final_clip.duration)\n            final_audio = mpe.CompositeAudioClip([final_clip.audio, background_music])\n            final_clip = final_clip.set_audio(final_audio)\n        \n        # Add intro and outro\n        intro_clip = mpe.TextClip(video_title, fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        outro_clip = mpe.TextClip(\"Thanks for watching!\", fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        final_clip = mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\n        \n        output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name\n        logger.info(f\"Writing final video to {output_file}\")\n        final_clip.write_videofile(output_file, codec='libx264', audio_codec='aac', fps=24)\n        logger.info(\"Video creation process completed\")\n        return output_file\n    except Exception as e:\n        logger.error(f\"Error in create_video: {str(e)}\")\n        st.error(f\"An error occurred while creating the video: {str(e)}\")\n        return None", "enhance_clip": "def enhance_clip(clip, script_analysis):\n    # Apply color grading based on sentiment\n    if script_analysis['sentiment'] == 'POSITIVE':\n        clip = apply_color_grading(clip, brightness=1.1, saturation=1.2)\n    elif script_analysis['sentiment'] == 'NEGATIVE':\n        clip = apply_color_grading(clip, brightness=0.9, contrast=1.1)\n    \n    # Add dynamic text animations\n    if clip.duration > 2:\n        text_clip = create_animated_text(clip.scene['title'], duration=2)\n        clip = mpe.CompositeVideoClip([clip, text_clip.set_start(1)])\n    \n    # Add smooth transitions\n    clip = clip.crossfadein(0.5).crossfadeout(0.5)\n    \n    return clip", "process_clip": "def process_clip(enhanced_clip, clip_data, script_analysis):\n    scene = clip_data['scene']\n    duration = float(scene['duration'])\n    processed_clip = enhanced_clip.set_duration(duration)\n    \n    # Apply color grading based on sentiment and style\n    if script_analysis['sentiment'] == 'POSITIVE':\n        processed_clip = apply_color_grading(processed_clip, brightness=1.1)\n    else:\n        processed_clip = apply_color_grading(processed_clip, brightness=0.9)\n    \n    if script_analysis['style'] in ['humorous', 'casual']:\n        processed_clip = processed_clip.fx(vfx.colorx, 1.2)  # More vibrant for humorous/casual content\n    elif script_analysis['style'] in ['dramatic', 'formal']:\n        processed_clip = processed_clip.fx(vfx.lum_contrast, contrast=1.2)  # More contrast for dramatic/formal content\n    \n    if scene.get('overlay_text'):\n        text_clip = create_animated_text(scene['overlay_text'], duration)\n        processed_clip = mpe.CompositeVideoClip([processed_clip, text_clip])\n    \n    lower_third = create_lower_third(scene['title'], duration)\n    processed_clip = mpe.CompositeVideoClip([processed_clip, lower_third])\n    \n    return processed_clip", "apply_fade_effects": "def apply_fade_effects(clip, duration=1):\n    try:\n        return fadein(clip, duration).fx(fadeout, duration)\n    except Exception as e:\n        raise ValueError(f\"Error applying fade effects: {e}\")", "add_text_overlay": "def add_text_overlay(clip, text):\n    if text:\n        try:\n            text_clip = mpe.TextClip(text, fontsize=70, color='white', font='Arial-Bold')\n            text_clip = text_clip.set_position('center').set_duration(clip.duration)\n            return mpe.CompositeVideoClip([clip, text_clip])\n        except Exception as e:\n            raise ValueError(f\"Error adding text overlay: {e}\")\n    return clip", "add_narration": "def add_narration(clip, narration_file):\n    try:\n        return clip.set_audio(mpe.AudioFileClip(narration_file))\n    except Exception as e:\n        raise ValueError(f\"Error adding narration: {e}\")", "add_background_music": "def add_background_music(clip, music_file):\n    try:\n        background_audio = mpe.AudioFileClip(music_file)\n        return clip.set_audio(mpe.CompositeAudioClip([clip.audio, background_audio.volumex(0.1)]))\n    except Exception as e:\n        raise ValueError(f\"Error adding background music: {e}\")", "add_watermark": "def add_watermark(clip, watermark_text=\"Sample Watermark\"):\n    try:\n        watermark = mpe.TextClip(watermark_text, fontsize=30, color='white', font='Arial')\n        watermark = watermark.set_position(('right', 'bottom')).set_duration(clip.duration)\n        return mpe.CompositeVideoClip([clip, watermark])\n    except Exception as e:\n        st.error(f\"Error adding watermark: {e}\")\n        return clip", "split_video": "def split_video(video_clip, part_duration=10):\n    try:\n        return [video_clip.subclip(start, min(start + part_duration, video_clip.duration)) for start in range(0, int(video_clip.duration), part_duration)]\n    except Exception as e:\n        st.error(f\"Error splitting video: {e}\")\n        return [video_clip]", "merge_video_parts": "def merge_video_parts(video_parts):\n    try:\n        return mpe.concatenate_videoclips(video_parts, method=\"compose\")\n    except Exception as e:\n        st.error(f\"Error merging video parts: {e}\")\n        return video_parts[0] if video_parts else None", "save_storyboard_backup": "def save_storyboard_backup(storyboard, filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'w') as f:\n            json.dump(storyboard, f)\n        st.success(f\"Storyboard backup saved to {filename}\")\n    except Exception as e:\n        st.error(f\"Error saving storyboard backup: {e}\")", "load_storyboard_backup": "def load_storyboard_backup(filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'r') as f:\n            storyboard = json.load(f)\n        st.success(f\"Storyboard loaded from {filename}\")\n        return storyboard\n    except FileNotFoundError:\n        st.warning(f\"Backup file {filename} not found.\")\n        return None\n    except json.JSONDecodeError:\n        st.error(f\"Error decoding JSON from {filename}\")\n        return None\n    except Exception as e:\n        st.error(f\"Error loading storyboard backup: {e}\")\n        return None", "add_subtitles_to_video": "def add_subtitles_to_video(clip, subtitles):\n    try:\n        subtitle_clips = [\n            mpe.TextClip(subtitle['text'], fontsize=50, color='white', size=clip.size, font='Arial-Bold')\n            .set_position(('bottom')).set_start(subtitle['start']).set_duration(subtitle['duration'])\n            for subtitle in subtitles\n        ]\n        return mpe.CompositeVideoClip([clip] + subtitle_clips)\n    except Exception as e:\n        st.error(f\"Error adding subtitles: {e}\")\n        return clip", "preview_storyboard_slideshow": "def preview_storyboard_slideshow(scenes, duration_per_scene=5):\n    try:\n        slides = [create_animated_text(scene['title'], duration=duration_per_scene) for scene in scenes]\n        slideshow = mpe.concatenate_videoclips(slides, method='compose')\n        slideshow.write_videofile(\"storyboard_preview.mp4\", codec='libx264')\n        st.success(\"Storyboard preview created successfully.\")\n        st.video(\"storyboard_preview.mp4\")\n    except Exception as e:\n        st.error(f\"Error creating storyboard slideshow: {e}\")", "add_logo_to_video": "def add_logo_to_video(clip, logo_path, position=('right', 'top')):\n    try:\n        logo = mpe.ImageClip(logo_path).set_duration(clip.duration).resize(height=100).set_position(position)\n        return mpe.CompositeVideoClip([clip, logo])\n    except FileNotFoundError:\n        st.error(f\"Logo file not found: {logo_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error adding logo to video: {e}\")\n        return clip", "compress_video": "def compress_video(input_path, output_path=\"compressed_video.mp4\", bitrate=\"500k\"):\n    try:\n        os.system(f\"ffmpeg -i {input_path} -b:v {bitrate} -bufsize {bitrate} {output_path}\")\n        st.success(f\"Video compressed successfully. Saved to {output_path}\")\n    except Exception as e:\n        st.error(f\"Error compressing video: {e}\")", "apply_bw_filter": "def apply_bw_filter(clip):\n    try:\n        return clip.fx(mpe.vfx.blackwhite)\n    except Exception as e:\n        st.error(f\"Error applying black-and-white filter: {e}\")\n        return clip", "overlay_image_on_video": "def overlay_image_on_video(clip, image_path, position=(0, 0)):\n    try:\n        image = mpe.ImageClip(image_path).set_duration(clip.duration).set_position(position)\n        return mpe.CompositeVideoClip([clip, image])\n    except FileNotFoundError:\n        st.error(f\"Image file not found: {image_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error overlaying image on video: {e}\")\n        return clip", "adjust_video_speed": "def adjust_video_speed(clip, speed=1.0):\n    try:\n        return clip.fx(mpe.vfx.speedx, speed)\n    except Exception as e:\n        st.error(f\"Error adjusting video speed: {e}\")\n        return clip", "crop_video": "def crop_video(clip, x1, y1, x2, y2):\n    try:\n        return clip.crop(x1=x1, y1=y1, x2=x2, y2=y2)\n    except Exception as e:\n        st.error(f\"Error cropping video: {e}\")\n        return clip", "adjust_resolution_based_on_system": "def adjust_resolution_based_on_system(clip):\n    try:\n        memory = psutil.virtual_memory()\n        resolution = (640, 360) if memory.available < 1000 * 1024 * 1024 else (1280, 720)\n        return resize(clip, newsize=resolution)\n    except Exception as e:\n        st.error(f\"Error adjusting resolution: {e}\")\n        return clip", "generate_video_thumbnail": "def generate_video_thumbnail(clip, output_path=\"thumbnail.png\"):\n    try:\n        frame = clip.get_frame(1)\n        image = Image.fromarray(frame)\n        image.save(output_path)\n        st.success(f\"Thumbnail generated successfully. Saved to {output_path}\")\n        return output_path\n    except Exception as e:\n        st.error(f\"Error generating video thumbnail: {e}\")\n        return None", "add_intro_outro": "def add_intro_outro(final_clip, video_title):\n    intro_clip = create_animated_text(video_title, duration=3, font_size=60).fx(vfx.fadeout, duration=1)\n    outro_clip = create_animated_text(\"Thanks for Watching!\", duration=3, font_size=60).fx(vfx.fadein, duration=1)\n    return mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])", "adjust_audio_volume": "def adjust_audio_volume(audio_clip, volume_level=1.0):\n    try:\n        return audio_clip.volumex(volume_level)\n    except Exception as e:\n        st.error(f\"Error adjusting audio volume: {e}\")\n        return audio_clip", "generate_gradient_text_overlay": "def generate_gradient_text_overlay(text, clip_duration, size=(1920, 1080)):\n    try:\n        gradient = color_gradient(size, p1=(0, 0), p2=(size[0], size[1]), color1=(255, 0, 0), color2=(0, 0, 255))\n        image = Image.fromarray(gradient)\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default()\n        text_size = draw.textsize(text, font=font)\n        draw.text(((size[0] - text_size[0]) / 2, (size[1] - text_size[1]) / 2), text, font=font, fill=(255, 255, 255))\n        image.save(\"gradient_overlay.png\")\n        return mpe.ImageClip(\"gradient_overlay.png\").set_duration(clip_duration)\n    except Exception as e:\n        st.error(f\"Error generating gradient text overlay: {e}\")\n        return mpe.TextClip(text, fontsize=70, color='white', size=size).set_duration(clip_duration)", "run_video_rendering_thread": "def run_video_rendering_thread(target_function, *args):\n    try:\n        rendering_thread = threading.Thread(target=target_function, args=args)\n        rendering_thread.start()\n        return rendering_thread\n    except Exception as e:\n        st.error(f\"Error running rendering thread: {e}\")\n        return None", "check_system_capabilities": "def check_system_capabilities():\n    try:\n        memory = psutil.virtual_memory()\n        if memory.available < 500 * 1024 * 1024:  # Less than 500MB\n            st.warning(\"Low memory detected. Consider closing other applications.\")\n        cpu_usage = psutil.cpu_percent()\n        if cpu_usage > 80:\n            st.warning(\"High CPU usage detected. Rendering may be slow.\")\n    except Exception as e:\n        st.error(f\"Error checking system capabilities: {e}\")", "log_system_resources": "def log_system_resources():\n    try:\n        memory = psutil.virtual_memory()\n        cpu = psutil.cpu_percent()\n        st.write(f\"Memory Usage: {memory.percent}% | CPU Usage: {cpu}%\")\n    except Exception as e:\n        st.error(f\"Error logging system resources: {e}\")", "download_additional_assets": "def download_additional_assets(url):\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            temp_asset_file = NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_asset_file.write(response.content)\n            temp_asset_file.flush()\n            st.success(f\"Asset downloaded successfully: {temp_asset_file.name}\")\n            return temp_asset_file.name\n        else:\n            st.error(\"Failed to download asset. Invalid URL or server error.\")\n            return None\n    except Exception as e:\n        st.error(f\"Error downloading asset: {e}\")\n        return None", "calculate_estimated_render_time": "def calculate_estimated_render_time(duration, resolution=(1280, 720)):\n    try:\n        estimated_time = duration * (resolution[0] * resolution[1]) / 1e6\n        st.info(f\"Estimated rendering time: {estimated_time:.2f} seconds\")\n        return estimated_time\n    except Exception as e:\n        st.error(f\"Error calculating render time: {e}\")\n        return None", "manage_temp_directory": "def manage_temp_directory(directory_path):\n    try:\n        if os.path.exists(directory_path):\n            shutil.rmtree(directory_path)\n        os.makedirs(directory_path)\n        st.success(f\"Temporary directory created: {directory_path}\")\n    except Exception as e:\n        st.error(f\"Error managing temporary directory: {e}\")", "handle_session_expiration": "def handle_session_expiration():\n    try:\n        st.error(\"Session expired. Please refresh and try again.\")\n        if st.button(\"Refresh Page\"):\n            st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n    except Exception as e:\n        st.error(f\"Error handling session expiration: {e}\")", "split_storyboard_scenes": "def split_storyboard_scenes(scenes, batch_size=5):\n    try:\n        return [scenes[i:i + batch_size] for i in range(0, len(scenes), batch_size)]\n    except Exception as e:\n        st.error(f\"Error splitting storyboard scenes: {e}\")\n        return [scenes]", "add_transition_effects_between_scenes": "def add_transition_effects_between_scenes(scenes):\n    try:\n        return [animate_scene_transition(scene1, scene2) for scene1, scene2 in zip(scenes, scenes[1:])]\n    except Exception as e:\n        st.error(f\"Error adding transition effects: {e}\")\n        return scenes", "optimize_storyboard_text_prompts": "def optimize_storyboard_text_prompts(scenes):\n    try:\n        for scene in scenes:\n            scene['title'] = scene['title'].capitalize()\n        return scenes\n    except Exception as e:\n        st.error(f\"Error optimizing storyboard text prompts: {e}\")\n        return scenes", "select_background_music": "def select_background_music(genre):\n    try:\n        if genre in MUSIC_TRACKS:\n            track_url = MUSIC_TRACKS[genre]\n        else:\n            track_url = random.choice(list(MUSIC_TRACKS.values()))\n        \n        response = requests.get(track_url)\n        if response.status_code == 200:\n            temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_audio_file.write(response.content)\n            temp_audio_file.close()\n            return temp_audio_file.name\n    except Exception as e:\n        logger.error(f\"Error selecting background music: {str(e)}\")\n    \n    return None", "analyze_script": "def analyze_script(script):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Analyze the script and provide a JSON response with keys: 'sentiment' (POSITIVE, NEGATIVE, or NEUTRAL), 'style' (e.g., formal, casual, humorous), and 'transitions' (list of transition types).\"},\n                {\"role\": \"user\", \"content\": f\"Analyze this script: {script}\"}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        analysis = json.loads(response.choices[0].message.content)\n        \n        if not all(key in analysis for key in ['sentiment', 'style', 'transitions']):\n            raise ValueError(\"Invalid analysis structure\")\n        \n        return analysis\n    except Exception as e:\n        logger.error(f\"Error analyzing script: {str(e)}\")\n        return {\n            'sentiment': 'NEUTRAL',\n            'style': 'formal',\n            'transitions': ['fade', 'cut']\n        }", "apply_transition": "def apply_transition(clip1, clip2, transition_type):\n    transition_functions = {\n        'fade': lambda: clip1.crossfadeout(1).crossfadein(1),\n        'slide': lambda: clip1.slide_out(1, 'left').slide_in(1, 'right'),\n        'whip': lambda: clip1.fx(vfx.speedx, 2).fx(vfx.crop, x1=0, y1=0, x2=0.5, y2=1).crossfadeout(0.5),\n        'zoom': lambda: clip1.fx(vfx.resize, 1.5).fx(vfx.crop, x_center=0.5, y_center=0.5, width=1/1.5, height=1/1.5).crossfadeout(1)\n    }\n    return transition_functions.get(transition_type, lambda: clip1)()", "cleanup_temp_files": "def cleanup_temp_files():\n    try:\n        temp_dir = tempfile.gettempdir()\n        for filename in os.listdir(temp_dir):\n            if filename.startswith('videocreator_'):\n                file_path = os.path.join(temp_dir, filename)\n                if os.path.isfile(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n        logger.info(\"Temporary files cleaned up successfully.\")\n    except Exception as e:\n        logger.error(f\"Error cleaning up temporary files: {str(e)}\")", "process_scene_with_progress": "def process_scene_with_progress(scene, index, total_scenes):\n    scene_progress = st.empty()\n    scene_progress.text(f\"Processing scene {index + 1} of {total_scenes}: {scene['title']}\")\n    \n    clip_progress, voice_progress = st.columns(2)\n    \n    with clip_progress:\n        st.text(\"Creating video clip...\")\n        clip = create_fallback_clip(scene)\n        st.success(\"Video clip processed\")\n    \n    with voice_progress:\n        st.text(\"Generating voiceover...\")\n        narration_file = generate_voiceover(scene['narration'])\n        st.success(\"Voiceover generated\")\n    \n    scene_progress.success(f\"Scene {index + 1} processed successfully!\")\n    return {'clip': clip, 'scene': scene, 'narration': narration_file}", "generate_valid_storyboard": "def generate_valid_storyboard(prompt, style, max_attempts=3):\n    for attempt in range(max_attempts):\n        storyboard = generate_storyboard(prompt, style)\n        if storyboard is not None:\n            return storyboard\n        logger.warning(f\"Storyboard generation attempt {attempt + 1} failed. Retrying...\")\n    logger.error(\"Failed to generate a valid storyboard after multiple attempts.\")\n    st.error(\"Failed to generate a valid storyboard after multiple attempts. Please try again with a different prompt or style.\")\n    return None", "prompt_card": "def prompt_card(prompt):\n    st.markdown(f\"**Sample Prompt:** {prompt}\")\n    if st.button(\"Use this prompt\", key=f\"btn_{prompt}\"):\n        st.session_state.prompt = prompt", "predict_processing_issues": "def predict_processing_issues(video_clips, system_resources):\n    potential_issues = []\n    if len(video_clips) * 5 > system_resources['available_memory'] / 1e6:  # Assuming 5 seconds per clip\n        potential_issues.append(\"Insufficient memory for processing all clips\")\n    if system_resources['cpu_usage'] > 80:\n        potential_issues.append(\"High CPU usage may slow down processing\")\n    return potential_issues", "main": "def main():\n    st.markdown(\"<h1 style='text-align: center; color: #4A90E2;'>AutovideoAI</h1>\", unsafe_allow_html=True)\n    st.markdown(\"<p style='text-align: center; font-size: 1.2em;'>Create Amazing Videos with AI</p>\", unsafe_allow_html=True)\n\n    with st.expander(\"\u2139\ufe0f How to use AutovideoAI\", expanded=False):\n        st.markdown(\"\"\"\n        1. Enter your video idea or choose a sample prompt.\n        2. Customize your video style, duration, and background music.\n        3. Generate a storyboard and preview it.\n        4. Create your AI-powered video!\n        \"\"\")\n\n    col1, col2 = st.columns([2, 1])\n    \n    with col1:\n        st.subheader(\"1\ufe0f\u20e3 Enter Your Video Idea\")\n        prompt = st.text_area(\"What's your video about?\", height=100, value=st.session_state.get('prompt', ''))\n    \n    with col2:\n        st.subheader(\"Sample Prompts\")\n        for sample_prompt in SAMPLE_PROMPTS:\n            if st.button(f\"\ud83d\udccc {sample_prompt}\", key=f\"btn_{sample_prompt}\"):\n                st.session_state.prompt = sample_prompt\n                st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n\n    st.subheader(\"2\ufe0f\u20e3 Customize Your Video\")\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        style = st.selectbox(\"Video Style \ud83c\udfad\", [\"Motivational\", \"Dramatic\", \"Educational\", \"Funny\"])\n    with col2:\n        duration = st.slider(\"Estimated Duration \u23f1\ufe0f\", 30, 300, 60, help=\"Duration in seconds\")\n    with col3:\n        music_style = st.selectbox(\"Background Music \ud83c\udfb5\", list(MUSIC_TRACKS.keys()))\n\n    if st.button(\"\ud83d\udd8b\ufe0f Generate Storyboard\", use_container_width=True):\n        with st.spinner(\"Crafting your storyboard...\"):\n            storyboard = generate_script(prompt, duration)\n        if storyboard:\n            st.session_state.storyboard = storyboard\n            st.success(\"\u2705 Storyboard generated successfully!\")\n            display_storyboard_preview(storyboard)\n        else:\n            st.error(\"\u274c Failed to generate a storyboard. Please try again.\")\n\n    if 'storyboard' in st.session_state:\n        if st.button(\"\ud83c\udfac Create Video\", use_container_width=True):\n            create_video_workflow(prompt, duration, music_style)", "create_video_workflow": "def create_video_workflow(prompt, duration, music_style):\n    try:\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n\n        logger.info(\"Starting video creation workflow\")\n        status_text.text(\"Generating script...\")\n        script = generate_script(prompt, duration)\n        if not script:\n            raise ValueError(\"Failed to generate script\")\n        logger.info(\"Script generated successfully\")\n        progress_bar.progress(20)\n\n        scenes = script['scenes']\n        if not scenes:\n            logger.error(f\"No scenes found in the script. Script content: {script}\")\n            raise ValueError(\"No scenes found in the script\")\n        logger.info(f\"Successfully extracted {len(scenes)} scenes\")\n        progress_bar.progress(30)\n\n        status_text.text(\"Fetching video clips...\")\n        video_clips = fetch_video_clips(scenes)\n        if not video_clips:\n            raise ValueError(\"Failed to fetch video clips\")\n        progress_bar.progress(50)\n\n        status_text.text(\"Generating voiceovers...\")\n        for clip_data in video_clips:\n            clip_data['narration'] = generate_voiceover(clip_data['scene']['narration'])\n        progress_bar.progress(70)\n\n        status_text.text(\"Selecting background music...\")\n        background_music_file = select_background_music(music_style)\n        progress_bar.progress(80)\n\n        status_text.text(\"Creating your video...\")\n        video_file = create_video(video_clips, background_music_file, script['title'])\n        progress_bar.progress(90)\n\n        if video_file and os.path.exists(video_file):\n            status_text.text(\"Finalizing...\")\n            progress_bar.progress(100)\n            logger.info(\"Video creation successful\")\n            st.success(\"\ud83c\udf89 Video created successfully!\")\n            st.video(video_file)\n            with open(video_file, 'rb') as vf:\n                st.download_button(label=\"\ud83d\udce5 Download Video\", data=vf, file_name=\"AutovideoAI_creation.mp4\")\n        else:\n            logger.error(\"Failed to create the video\")\n            st.error(\"\u274c Failed to create the video. Please try again.\")\n    except Exception as e:\n        logger.error(f\"An error occurred during video creation: {str(e)}\")\n        st.error(f\"An error occurred: {str(e)}\")\n    finally:\n        status_text.empty()\n        progress_bar.empty()", "generate_script": "def generate_script(prompt, duration):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert video scriptwriter. Create a JSON script for a video based on the given prompt and duration. Use the following schema:\"},\n                {\"role\": \"user\", \"content\": f\"\"\"\n                Create a script for a {duration} second video about: {prompt}. \n                Return the script as a JSON object with the following structure:\n                {{\n                    \"title\": \"Overall video title\",\n                    \"scenes\": [\n                        {{\n                            \"scene_number\": 1,\n                            \"title\": \"Scene title\",\n                            \"description\": \"Detailed scene description\",\n                            \"narration\": \"Narration text for the scene\",\n                            \"duration\": \"Duration in seconds\"\n                        }}\n                    ]\n                }}\n                Include 3-5 scenes in total.\n                \"\"\"}\n            ],\n            max_tokens=1000,\n            response_format={\"type\": \"json_object\"}\n        )\n        script = json.loads(response.choices[0].message.content)\n        logger.info(f\"Generated script: {script}\")\n        return script\n    except Exception as e:\n        logger.error(f\"Error generating script: {str(e)}\")\n        return None", "display_storyboard_preview": "def display_storyboard_preview(storyboard):\n    with st.expander(\"\ud83d\udd0d Preview Storyboard\", expanded=True):\n        st.markdown(f\"### {storyboard['title']}\")\n        st.markdown(\"### Scene Breakdown\")\n        for scene in storyboard['scenes']:\n            with st.container():\n                col1, col2 = st.columns([1, 2])\n                with col1:\n                    st.markdown(f\"**Scene {scene['scene_number']}**\")\n                    st.write(f\"Duration: {scene['duration']} seconds\")\n                with col2:\n                    st.markdown(f\"**{scene['title']}**\")\n                    st.write(f\"{scene['description']}\")\n                st.markdown(\"---\")", "color_gradient": "def color_gradient(size, p1, p2, color1, color2):\n    x = np.linspace(0, 1, size[0])[:, None]\n    y = np.linspace(0, 1, size[1])[None, :]\n    gradient = x * (p2[0] - p1[0]) + y * (p2[1] - p1[1])\n    gradient = np.clip(gradient, 0, 1)\n    return np.array(color1) * (1 - gradient[:, :, None]) + np.array(color2) * gradient[:, :, None]"}, "imports": ["import streamlit as st", "from openai import OpenAI", "import os", "import moviepy.editor as mpe", "import requests", "from tempfile import NamedTemporaryFile", "from moviepy.video.fx.all import fadein, fadeout, resize", "import psutil", "from tenacity import retry, wait_random_exponential, stop_after_attempt", "import json", "from PIL import Image, ImageDraw, ImageFont", "import shutil", "import logging", "from gtts import gTTS", "from dotenv import load_dotenv", "from pydub import AudioSegment", "import random", "import moviepy.video.fx.all as vfx", "import cv2", "from pydub.silence import split_on_silence", "import numpy as np", "import tempfile", "from functools import lru_cache", "from huggingface_hub import InferenceClient, hf_hub_download", "from huggingface_hub import login, HfApi", "from datasets import load_dataset", "import textwrap", "from scenedetect import detect, ContentDetector", "from pydub import AudioSegment", "from pydub.silence import split_on_silence", "import re"]}, "script_sections": {"package_installations": [], "imports": ["import streamlit as st", "from openai import OpenAI", "import os", "import moviepy.editor as mpe", "import requests", "from tempfile import NamedTemporaryFile", "from moviepy.video.fx.all import fadein, fadeout, resize", "import psutil", "from tenacity import retry, wait_random_exponential, stop_after_attempt", "import json", "from PIL import Image, ImageDraw, ImageFont", "import shutil", "import logging", "from gtts import gTTS", "from dotenv import load_dotenv", "from pydub import AudioSegment", "import random", "import moviepy.video.fx.all as vfx", "import cv2", "from pydub.silence import split_on_silence", "import numpy as np", "import tempfile", "from functools import lru_cache", "from huggingface_hub import InferenceClient, hf_hub_download", "from huggingface_hub import login, HfApi", "from datasets import load_dataset", "import textwrap", "from scenedetect import detect, ContentDetector", "from pydub import AudioSegment", "from pydub.silence import split_on_silence", "import re"], "settings": ["logger = logging.getLogger(__name__)", "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")", "api = HfApi()", "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))", "SAMPLE_PROMPTS = [\n    \"Create a motivational video about overcoming challenges\",\n    \"Make an educational video explaining photosynthesis\",\n    \"Design a funny video about the struggles of working from home\",\n]", "MUSIC_TRACKS = {\n    \"Electronic\": \"https://www.bensound.com/bensound-dubstep\",\n    \"Experimental\": \"https://www.bensound.com/bensound-enigmatic\",\n    \"Folk\": \"https://www.bensound.com/bensound-acousticbreeze\",\n    \"Hip-Hop\": \"https://www.bensound.com/bensound-groovyhiphop\",\n    \"Instrumental\": \"https://www.bensound.com/bensound-pianomoment\",\n    \"Pop\": \"https://www.bensound.com/bensound-ukulele\",\n    \"Rock\": \"https://www.bensound.com/bensound-extremeaction\"\n}", "response = requests.get(url)", "required_fields = [\"scene_number\", \"title\", \"description\", \"narration\", \"keywords\", \"duration\", \"overlay_text\", \"visual_elements\", \"audio_cues\", \"transitions\"]", "video_clips = []", "api = HfApi()", "text = scene.get('title', 'Scene')", "size = (1280, 720)", "img = Image.new('RGB', size, color='black')", "draw = ImageDraw.Draw(img)", "font = ImageFont.load_default()", "wrapped_text = textwrap.wrap(text, width=20)", "y_text = (size[1] - len(wrapped_text) * 80) // 2", "img_array = np.array(img)", "clip = mpe.ImageClip(img_array).set_duration(duration)", "silent_segment = AudioSegment.silent(duration=int(duration * 1000))", "temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')", "clip = clip.crossfadein(0.5).crossfadeout(0.5)", "scene = clip_data['scene']", "duration = float(scene['duration'])", "processed_clip = enhanced_clip.set_duration(duration)", "lower_third = create_lower_third(scene['title'], duration)", "processed_clip = mpe.CompositeVideoClip([processed_clip, lower_third])", "intro_clip = create_animated_text(video_title, duration=3, font_size=60).fx(vfx.fadeout, duration=1)", "outro_clip = create_animated_text(\"Thanks for Watching!\", duration=3, font_size=60).fx(vfx.fadein, duration=1)", "transition_functions = {\n        'fade': lambda: clip1.crossfadeout(1).crossfadein(1),\n        'slide': lambda: clip1.slide_out(1, 'left').slide_in(1, 'right'),\n        'whip': lambda: clip1.fx(vfx.speedx, 2).fx(vfx.crop, x1=0, y1=0, x2=0.5, y2=1).crossfadeout(0.5),\n        'zoom': lambda: clip1.fx(vfx.resize, 1.5).fx(vfx.crop, x_center=0.5, y_center=0.5, width=1/1.5, height=1/1.5).crossfadeout(1)\n    }", "scene_progress = st.empty()", "potential_issues = []", "video_clips = []", "api = HfApi()", "x = np.linspace(0, 1, size[0])[:, None]", "y = np.linspace(0, 1, size[1])[None, :]", "gradient = x * (p2[0] - p1[0]) + y * (p2[1] - p1[1])", "gradient = np.clip(gradient, 0, 1)", "input_text = f\"\"\"Generate a detailed {style} video storyboard based on this prompt: \"{prompt}\"\n        Provide the storyboard in JSON format with the following structure:\n        {{\n            \"title\": \"Overall video title\",\n            \"scenes\": [\n                {{\n                    \"scene_number\": 1,\n                    \"title\": \"Scene title\",\n                    \"description\": \"Detailed scene description\",\n                    \"narration\": \"Narration text for the scene\",\n                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n                    \"duration\": \"Duration in seconds\",\n                    \"visual_elements\": [\"List of visual elements to include\"],\n                    \"transitions\": {{\n                        \"in\": \"Transition type for entering the scene\",\n                        \"out\": \"Transition type for exiting the scene\"\n                    }}\n                }}\n            ],\n            \"target_audience\": \"Description of the target audience\",\n            \"overall_tone\": \"Description of the overall tone of the video\"\n        }}\n        Ensure there are at least 3 scenes in the storyboard.\"\"\"", "response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative video storyboard generator. Respond with valid JSON following the specified structure.\"},\n                {\"role\": \"user\", \"content\": input_text}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )", "storyboard = json.loads(response.choices[0].message.content)", "position = ((size[0] - line_width) / 2, y_text)", "tts = gTTS(text=narration_text, lang='en', slow=False)", "temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')", "gradient = np.linspace(0, 255, 1280)", "background = np.tile(gradient, (720, 1)).astype(np.uint8)", "background = np.stack((background,) * 3, axis=-1)", "clip = mpe.ImageClip(background).set_duration(duration)", "txt_clip = mpe.TextClip(scene['title'], fontsize=70, color='white', font='Arial-Bold')", "txt_clip = txt_clip.set_position('center').set_duration(duration)", "clip = clip.fadein(0.5).fadeout(0.5)", "img = Image.new('RGB', (1280, 720), color='black')", "draw = ImageDraw.Draw(img)", "font = ImageFont.load_default().font_variant(size=font_size)", "text_bbox = draw.textbbox((0, 0), text, font=font)", "text_width = text_bbox[2] - text_bbox[0]", "text_height = text_bbox[3] - text_bbox[1]", "position = ((1280 - text_width) / 2, (720 - text_height) / 2)", "img_array = np.array(img)", "clip = mpe.ImageClip(img_array).set_duration(duration)", "clip = clip.fadein(1).fadeout(1)", "img = Image.new('RGBA', (1280, 720), (0, 0, 0, 0))", "draw = ImageDraw.Draw(img)", "font = ImageFont.load_default().font_variant(size=30)", "text_bbox = draw.textbbox((0, 0), text, font=font)", "text_width = text_bbox[2] - text_bbox[0]", "text_height = text_bbox[3] - text_bbox[1]", "position = ((1280 - text_width) / 2, 720 - text_height - 50)", "bg_bbox = (position[0]-10, position[1]-10, position[0]+text_width+10, position[1]+text_height+10)", "img_array = np.array(img)", "chunks = split_on_silence(audio_segment, min_silence_len=500, silence_thresh=-40)", "cut_times = [0]", "cut_clips = [video_clip.subclip(start, end) for start, end in zip(cut_times[:-1], cut_times[1:])]", "frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in clip.iter_frames()]", "motion = [np.mean(cv2.absdiff(frames[i], frames[i+1])) for i in range(len(frames)-1)]", "speed_clip = clip.fl(lambda gf, t: gf(t * speed_factor) if motion[int(t*clip.fps)] < threshold else gf(t))", "clips = []", "final_clip = mpe.concatenate_videoclips(clips)", "intro_clip = mpe.TextClip(video_title, fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)", "outro_clip = mpe.TextClip(\"Thanks for watching!\", fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)", "final_clip = mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])", "output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name", "clip = apply_color_grading(clip, brightness=1.1, saturation=1.2)", "text_clip = create_animated_text(clip.scene['title'], duration=2)", "clip = mpe.CompositeVideoClip([clip, text_clip.set_start(1)])", "processed_clip = apply_color_grading(processed_clip, brightness=1.1)", "processed_clip = apply_color_grading(processed_clip, brightness=0.9)", "processed_clip = processed_clip.fx(vfx.colorx, 1.2)", "text_clip = create_animated_text(scene['overlay_text'], duration)", "processed_clip = mpe.CompositeVideoClip([processed_clip, text_clip])", "background_audio = mpe.AudioFileClip(music_file)", "watermark = mpe.TextClip(watermark_text, fontsize=30, color='white', font='Arial')", "watermark = watermark.set_position(('right', 'bottom')).set_duration(clip.duration)", "subtitle_clips = [\n            mpe.TextClip(subtitle['text'], fontsize=50, color='white', size=clip.size, font='Arial-Bold')\n            .set_position(('bottom')).set_start(subtitle['start']).set_duration(subtitle['duration'])\n            for subtitle in subtitles\n        ]", "slides = [create_animated_text(scene['title'], duration=duration_per_scene) for scene in scenes]", "slideshow = mpe.concatenate_videoclips(slides, method='compose')", "logo = mpe.ImageClip(logo_path).set_duration(clip.duration).resize(height=100).set_position(position)", "image = mpe.ImageClip(image_path).set_duration(clip.duration).set_position(position)", "memory = psutil.virtual_memory()", "resolution = (640, 360) if memory.available < 1000 * 1024 * 1024 else (1280, 720)", "frame = clip.get_frame(1)", "image = Image.fromarray(frame)", "gradient = color_gradient(size, p1=(0, 0), p2=(size[0], size[1]), color1=(255, 0, 0), color2=(0, 0, 255))", "image = Image.fromarray(gradient)", "draw = ImageDraw.Draw(image)", "font = ImageFont.load_default()", "text_size = draw.textsize(text, font=font)", "rendering_thread = threading.Thread(target=target_function, args=args)", "memory = psutil.virtual_memory()", "cpu_usage = psutil.cpu_percent()", "memory = psutil.virtual_memory()", "cpu = psutil.cpu_percent()", "response = requests.get(url)", "estimated_time = duration * (resolution[0] * resolution[1]) / 1e6", "response = requests.get(track_url)", "response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Analyze the script and provide a JSON response with keys: 'sentiment' (POSITIVE, NEGATIVE, or NEUTRAL), 'style' (e.g., formal, casual, humorous), and 'transitions' (list of transition types).\"},\n                {\"role\": \"user\", \"content\": f\"Analyze this script: {script}\"}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )", "analysis = json.loads(response.choices[0].message.content)", "temp_dir = tempfile.gettempdir()", "clip = create_fallback_clip(scene)", "narration_file = generate_voiceover(scene['narration'])", "storyboard = generate_storyboard(prompt, style)", "response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert video scriptwriter. Create a JSON script for a video based on the given prompt and duration. Use the following schema:\"},\n                {\"role\": \"user\", \"content\": f\"\"\"\n                Create a script for a {duration} second video about: {prompt}. \n                Return the script as a JSON object with the following structure:\n                {{\n                    \"title\": \"Overall video title\",\n                    \"scenes\": [\n                        {{\n                            \"scene_number\": 1,\n                            \"title\": \"Scene title\",\n                            \"description\": \"Detailed scene description\",\n                            \"narration\": \"Narration text for the scene\",\n                            \"duration\": \"Duration in seconds\"\n                        }}\n                    ]\n                }}\n                Include 3-5 scenes in total.\n                \"\"\"}\n            ],\n            max_tokens=1000,\n            response_format={\"type\": \"json_object\"}\n        )", "script = json.loads(response.choices[0].message.content)", "keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']", "search_query = \" OR \".join(keywords)", "progress_bar = st.progress(0)", "status_text = st.empty()", "script = generate_script(prompt, duration)", "scenes = script['scenes']", "video_clips = fetch_video_clips(scenes)", "background_music_file = select_background_music(music_style)", "video_file = create_video(video_clips, background_music_file, script['title'])", "prompt = st.text_area(\"What's your video about?\", height=100, value=st.session_state.get('prompt', ''))", "style = st.selectbox(\"Video Style \ud83c\udfad\", [\"Motivational\", \"Dramatic\", \"Educational\", \"Funny\"])", "duration = st.slider(\"Estimated Duration \u23f1\ufe0f\", 30, 300, 60, help=\"Duration in seconds\")", "music_style = st.selectbox(\"Background Music \ud83c\udfb5\", list(MUSIC_TRACKS.keys()))", "keywords_txt = \", \".join(scene['keywords'])", "keywords_clip = mpe.TextClip(keywords_txt, fontsize=30, color='yellow', font='Arial')", "keywords_clip = keywords_clip.set_position(('center', 0.8), relative=True).set_duration(duration)", "clip = mpe.CompositeVideoClip([clip, txt_clip, keywords_clip])", "clip = mpe.CompositeVideoClip([clip, txt_clip])", "audio_segment = AudioSegment.from_wav(audio_clip)", "clip = clip.set_fps(24)", "background_music = mpe.AudioFileClip(background_music_file).volumex(0.1)", "background_music = background_music.audio_loop(duration=final_clip.duration)", "final_audio = mpe.CompositeAudioClip([final_clip.audio, background_music])", "final_clip = final_clip.set_audio(final_audio)", "clip = apply_color_grading(clip, brightness=0.9, contrast=1.1)", "processed_clip = processed_clip.fx(vfx.lum_contrast, contrast=1.2)", "text_clip = mpe.TextClip(text, fontsize=70, color='white', font='Arial-Bold')", "text_clip = text_clip.set_position('center').set_duration(clip.duration)", "storyboard = json.load(f)", "temp_asset_file = NamedTemporaryFile(delete=False, suffix='.mp3')", "track_url = MUSIC_TRACKS[genre]", "track_url = random.choice(list(MUSIC_TRACKS.values()))", "temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')", "search_results = api.list_datasets(search=search_query, limit=20)", "matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]", "storyboard = generate_script(prompt, duration)", "keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']", "search_query = \" OR \".join(keywords)", "search_results = api.list_datasets(search=search_query, limit=20)", "matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]", "audio_array = audio_clip.to_soundarray()", "audio_segment = AudioSegment(\n                audio_array.tobytes(),\n                frame_rate=audio_clip.fps,\n                sample_width=audio_array.dtype.itemsize,\n                channels=1 if audio_array.ndim == 1 else audio_array.shape[1]\n            )", "clip = clip_data['clip']", "narration = clip_data['narration']", "clip = clip.set_audio(narration)", "file_path = os.path.join(temp_dir, filename)", "chosen_dataset = random.choice(matching_datasets)", "dataset_info = api.dataset_info(chosen_dataset.id)", "clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))", "chosen_dataset = random.choice(matching_datasets)", "dataset_info = api.dataset_info(chosen_dataset.id)", "sample = random.choice(dataset_info.card_data['samples'])", "sample = random.choice(dataset_info.card_data['samples'])", "clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))", "video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])", "clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))", "video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])", "clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))"], "function_definitions": {"toggle_theme": "def toggle_theme():\n    st.session_state.theme = 'dark' if st.session_state.theme == 'light' else 'light'", "cached_download": "def cached_download(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.content\n    return None", "generate_storyboard": "def generate_storyboard(prompt, style=\"motivational\"):\n    try:\n        input_text = f\"\"\"Generate a detailed {style} video storyboard based on this prompt: \"{prompt}\"\n        Provide the storyboard in JSON format with the following structure:\n        {{\n            \"title\": \"Overall video title\",\n            \"scenes\": [\n                {{\n                    \"scene_number\": 1,\n                    \"title\": \"Scene title\",\n                    \"description\": \"Detailed scene description\",\n                    \"narration\": \"Narration text for the scene\",\n                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n                    \"duration\": \"Duration in seconds\",\n                    \"visual_elements\": [\"List of visual elements to include\"],\n                    \"transitions\": {{\n                        \"in\": \"Transition type for entering the scene\",\n                        \"out\": \"Transition type for exiting the scene\"\n                    }}\n                }}\n            ],\n            \"target_audience\": \"Description of the target audience\",\n            \"overall_tone\": \"Description of the overall tone of the video\"\n        }}\n        Ensure there are at least 3 scenes in the storyboard.\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative video storyboard generator. Respond with valid JSON following the specified structure.\"},\n                {\"role\": \"user\", \"content\": input_text}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        storyboard = json.loads(response.choices[0].message.content)\n        return storyboard\n    except Exception as e:\n        logger.error(f\"Error generating storyboard: {str(e)}\")\n        st.error(\"An error occurred while generating the storyboard. Please try again.\")\n    return None", "validate_storyboard": "def validate_storyboard(storyboard):\n    if \"title\" not in storyboard or \"scenes\" not in storyboard or not isinstance(storyboard[\"scenes\"], list):\n        return False\n    if len(storyboard[\"scenes\"]) < 3:\n        return False\n    \n    required_fields = [\"scene_number\", \"title\", \"description\", \"narration\", \"keywords\", \"duration\", \"overlay_text\", \"visual_elements\", \"audio_cues\", \"transitions\"]\n    return all(all(field in scene for field in required_fields) for scene in storyboard[\"scenes\"])", "parse_storyboard": "def parse_storyboard(storyboard):\n    try:\n        return json.loads(storyboard).get(\"scenes\", [])\n    except json.JSONDecodeError:\n        return []", "fetch_video_clips": "def fetch_video_clips(scenes):\n    logger.info(f\"Fetching video clips for {len(scenes)} scenes\")\n    video_clips = []\n    \n    api = HfApi()\n    \n    for i, scene in enumerate(scenes):\n        logger.info(f\"Fetching clip for scene {i+1}: {scene['title']}\")\n        \n        # Expand the search query to include more relevant terms\n        keywords = scene['title'].split() + scene['description'].split() + ['video', 'clip', 'footage']\n        search_query = \" OR \".join(keywords)\n        \n        try:\n            search_results = api.list_datasets(search=search_query, limit=20)  # Increase the limit\n            matching_datasets = [dataset for dataset in search_results if 'video' in dataset.id.lower()]\n            \n            if matching_datasets:\n                chosen_dataset = random.choice(matching_datasets)\n                dataset_info = api.dataset_info(chosen_dataset.id)\n                \n                if dataset_info.card_data and 'samples' in dataset_info.card_data:\n                    sample = random.choice(dataset_info.card_data['samples'])\n                    if 'video' in sample:\n                        video_path = hf_hub_download(repo_id=chosen_dataset.id, filename=sample['video'])\n                        clip = mpe.VideoFileClip(video_path).subclip(0, min(float(scene['duration']), 10))  # Limit to 10 seconds max\n                        logger.info(f\"Clip fetched for scene {i+1}: duration={clip.duration}s\")\n                    else:\n                        raise ValueError(\"No video found in the sample\")\n                else:\n                    raise ValueError(\"No samples found in the dataset\")\n            else:\n                raise ValueError(\"No matching datasets found\")\n        \n        except Exception as e:\n            logger.warning(f\"Error fetching video for scene {i+1}: {str(e)}. Creating fallback clip.\")\n            clip = create_fallback_clip(scene, duration=min(float(scene['duration']), 10))\n        \n        video_clips.append({'clip': clip, 'scene': scene})\n    \n    return video_clips", "create_fallback_clip": "def create_fallback_clip(scene, duration=5):\n    text = scene.get('title', 'Scene')\n    size = (1280, 720)\n    \n    # Create a black background\n    img = Image.new('RGB', size, color='black')\n    draw = ImageDraw.Draw(img)\n    \n    # Use a default font\n    font = ImageFont.load_default()\n    \n    # Wrap text\n    wrapped_text = textwrap.wrap(text, width=20)\n    \n    # Calculate text position\n    y_text = (size[1] - len(wrapped_text) * 80) // 2\n    \n    # Draw text\n    for line in wrapped_text:\n        line_width, line_height = draw.textbbox((0, 0), line, font=font)[2:]\n        position = ((size[0] - line_width) / 2, y_text)\n        draw.text(position, line, font=font, fill='white')\n        y_text += line_height + 10\n    \n    # Convert PIL Image to numpy array\n    img_array = np.array(img)\n    \n    # Create video clip from the image\n    clip = mpe.ImageClip(img_array).set_duration(duration)\n    \n    return clip", "generate_voiceover": "def generate_voiceover(narration_text):\n    logger.info(f\"Generating voiceover for text: {narration_text[:50]}...\")\n    try:\n        tts = gTTS(text=narration_text, lang='en', slow=False)\n        temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n        tts.save(temp_audio_file.name)\n        return mpe.AudioFileClip(temp_audio_file.name)\n    except Exception as e:\n        logger.error(f\"Error generating voiceover: {str(e)}\")\n        return create_silent_audio(len(narration_text.split()) / 2)", "create_silent_audio": "def create_silent_audio(duration):\n    silent_segment = AudioSegment.silent(duration=int(duration * 1000))  # pydub uses milliseconds\n    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n    silent_segment.export(temp_audio_file.name, format=\"mp3\")\n    return mpe.AudioFileClip(temp_audio_file.name)", "create_scene_clip": "def create_scene_clip(scene, duration=5):\n    try:\n        # Create a gradient background\n        gradient = np.linspace(0, 255, 1280)\n        background = np.tile(gradient, (720, 1)).astype(np.uint8)\n        background = np.stack((background,) * 3, axis=-1)\n        \n        # Create video clip from the background\n        clip = mpe.ImageClip(background).set_duration(duration)\n        \n        # Add text\n        txt_clip = mpe.TextClip(scene['title'], fontsize=70, color='white', font='Arial-Bold')\n        txt_clip = txt_clip.set_position('center').set_duration(duration)\n        \n        # Add keywords as subtitles\n        if 'keywords' in scene:\n            keywords_txt = \", \".join(scene['keywords'])\n            keywords_clip = mpe.TextClip(keywords_txt, fontsize=30, color='yellow', font='Arial')\n            keywords_clip = keywords_clip.set_position(('center', 0.8), relative=True).set_duration(duration)\n            clip = mpe.CompositeVideoClip([clip, txt_clip, keywords_clip])\n        else:\n            clip = mpe.CompositeVideoClip([clip, txt_clip])\n        \n        # Add fade in and out\n        clip = clip.fadein(0.5).fadeout(0.5)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating scene clip: {str(e)}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)", "add_fade_transition": "def add_fade_transition(clip1, clip2, duration=1):\n    return mpe.CompositeVideoClip([clip1.crossfadeout(duration), clip2.crossfadein(duration)])", "create_animated_text": "def create_animated_text(text, duration=5, font_size=70, color='white'):\n    try:\n        # Create a black background image\n        img = Image.new('RGB', (1280, 720), color='black')\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=font_size)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position to center the text\n        position = ((1280 - text_width) / 2, (720 - text_height) / 2)\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill=color)\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        clip = mpe.ImageClip(img_array).set_duration(duration)\n        \n        # Add fade in and fade out effects\n        clip = clip.fadein(1).fadeout(1)\n        \n        return clip\n    except Exception as e:\n        logger.error(f\"Error creating animated text: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0)).set_duration(duration)", "apply_color_grading": "def apply_color_grading(clip, brightness=1.0, contrast=1.0, saturation=1.0):\n    return clip.fx(vfx.colorx, brightness).fx(vfx.lum_contrast, contrast=contrast).fx(vfx.colorx, saturation)", "create_lower_third": "def create_lower_third(text, duration):\n    try:\n        # Create a transparent background\n        img = Image.new('RGBA', (1280, 720), (0, 0, 0, 0))\n        draw = ImageDraw.Draw(img)\n        \n        # Use a default font\n        font = ImageFont.load_default().font_variant(size=30)\n        \n        # Get text size\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        \n        # Calculate position for lower third\n        position = ((1280 - text_width) / 2, 720 - text_height - 50)\n        \n        # Draw semi-transparent background\n        bg_bbox = (position[0]-10, position[1]-10, position[0]+text_width+10, position[1]+text_height+10)\n        draw.rectangle(bg_bbox, fill=(0,0,0,153))\n        \n        # Draw the text\n        draw.text(position, text, font=font, fill='white')\n        \n        # Convert to numpy array and create video clip\n        img_array = np.array(img)\n        return mpe.ImageClip(img_array).set_duration(duration)\n    except Exception as e:\n        logger.error(f\"Error creating lower third: {e}\")\n        return mpe.ColorClip(size=(1280, 720), color=(0,0,0,0)).set_duration(duration)", "smart_cut": "def smart_cut(video_clip, audio_clip):\n    try:\n        # Check if audio_clip is a file path or an AudioClip object\n        if isinstance(audio_clip, str):\n            audio_segment = AudioSegment.from_wav(audio_clip)\n        elif isinstance(audio_clip, mpe.AudioClip):\n            # Convert AudioClip to numpy array\n            audio_array = audio_clip.to_soundarray()\n            audio_segment = AudioSegment(\n                audio_array.tobytes(),\n                frame_rate=audio_clip.fps,\n                sample_width=audio_array.dtype.itemsize,\n                channels=1 if audio_array.ndim == 1 else audio_array.shape[1]\n            )\n        else:\n            logger.warning(\"Unsupported audio type. Returning original video clip.\")\n            return video_clip\n\n        # Split audio on silences\n        chunks = split_on_silence(audio_segment, min_silence_len=500, silence_thresh=-40)\n        \n        # Calculate timestamps for cuts\n        cut_times = [0]\n        for chunk in chunks:\n            cut_times.append(cut_times[-1] + len(chunk) / 1000)\n        \n        # Cut video based on audio\n        cut_clips = [video_clip.subclip(start, end) for start, end in zip(cut_times[:-1], cut_times[1:])]\n        \n        return mpe.concatenate_videoclips(cut_clips)\n    except Exception as e:\n        logger.error(f\"Error in smart_cut: {str(e)}\")\n        return video_clip", "apply_speed_changes": "def apply_speed_changes(clip, speed_factor=1.5, threshold=0.1):\n    try:\n        if not hasattr(clip, 'fps') or clip.fps is None:\n            clip = clip.set_fps(24)  # Set a default fps if not present\n        \n        frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in clip.iter_frames()]\n        motion = [np.mean(cv2.absdiff(frames[i], frames[i+1])) for i in range(len(frames)-1)]\n        \n        speed_clip = clip.fl(lambda gf, t: gf(t * speed_factor) if motion[int(t*clip.fps)] < threshold else gf(t))\n        \n        return speed_clip\n    except Exception as e:\n        logger.error(f\"Error in apply_speed_changes: {str(e)}\")\n        return clip", "create_video": "def create_video(video_clips, background_music_file, video_title):\n    logger.info(f\"Starting video creation process for '{video_title}'\")\n    try:\n        clips = []\n        for i, clip_data in enumerate(video_clips):\n            try:\n                clip = clip_data['clip']\n                narration = clip_data['narration']\n                clip = clip.set_audio(narration)\n                clips.append(clip)\n                logger.info(f\"Processed clip {i+1} successfully\")\n            except Exception as e:\n                logger.error(f\"Error processing clip {i+1}: {str(e)}\")\n        \n        if not clips:\n            raise ValueError(\"No valid clips were created\")\n        \n        logger.info(f\"Concatenating {len(clips)} scene clips\")\n        final_clip = mpe.concatenate_videoclips(clips)\n        \n        if background_music_file:\n            logger.info(\"Adding background music\")\n            background_music = mpe.AudioFileClip(background_music_file).volumex(0.1)\n            background_music = background_music.audio_loop(duration=final_clip.duration)\n            final_audio = mpe.CompositeAudioClip([final_clip.audio, background_music])\n            final_clip = final_clip.set_audio(final_audio)\n        \n        # Add intro and outro\n        intro_clip = mpe.TextClip(video_title, fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        outro_clip = mpe.TextClip(\"Thanks for watching!\", fontsize=70, color='white', size=(1280, 720), bg_color='black').set_duration(3)\n        final_clip = mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])\n        \n        output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name\n        logger.info(f\"Writing final video to {output_file}\")\n        final_clip.write_videofile(output_file, codec='libx264', audio_codec='aac', fps=24)\n        logger.info(\"Video creation process completed\")\n        return output_file\n    except Exception as e:\n        logger.error(f\"Error in create_video: {str(e)}\")\n        st.error(f\"An error occurred while creating the video: {str(e)}\")\n        return None", "enhance_clip": "def enhance_clip(clip, script_analysis):\n    # Apply color grading based on sentiment\n    if script_analysis['sentiment'] == 'POSITIVE':\n        clip = apply_color_grading(clip, brightness=1.1, saturation=1.2)\n    elif script_analysis['sentiment'] == 'NEGATIVE':\n        clip = apply_color_grading(clip, brightness=0.9, contrast=1.1)\n    \n    # Add dynamic text animations\n    if clip.duration > 2:\n        text_clip = create_animated_text(clip.scene['title'], duration=2)\n        clip = mpe.CompositeVideoClip([clip, text_clip.set_start(1)])\n    \n    # Add smooth transitions\n    clip = clip.crossfadein(0.5).crossfadeout(0.5)\n    \n    return clip", "process_clip": "def process_clip(enhanced_clip, clip_data, script_analysis):\n    scene = clip_data['scene']\n    duration = float(scene['duration'])\n    processed_clip = enhanced_clip.set_duration(duration)\n    \n    # Apply color grading based on sentiment and style\n    if script_analysis['sentiment'] == 'POSITIVE':\n        processed_clip = apply_color_grading(processed_clip, brightness=1.1)\n    else:\n        processed_clip = apply_color_grading(processed_clip, brightness=0.9)\n    \n    if script_analysis['style'] in ['humorous', 'casual']:\n        processed_clip = processed_clip.fx(vfx.colorx, 1.2)  # More vibrant for humorous/casual content\n    elif script_analysis['style'] in ['dramatic', 'formal']:\n        processed_clip = processed_clip.fx(vfx.lum_contrast, contrast=1.2)  # More contrast for dramatic/formal content\n    \n    if scene.get('overlay_text'):\n        text_clip = create_animated_text(scene['overlay_text'], duration)\n        processed_clip = mpe.CompositeVideoClip([processed_clip, text_clip])\n    \n    lower_third = create_lower_third(scene['title'], duration)\n    processed_clip = mpe.CompositeVideoClip([processed_clip, lower_third])\n    \n    return processed_clip", "apply_fade_effects": "def apply_fade_effects(clip, duration=1):\n    try:\n        return fadein(clip, duration).fx(fadeout, duration)\n    except Exception as e:\n        raise ValueError(f\"Error applying fade effects: {e}\")", "add_text_overlay": "def add_text_overlay(clip, text):\n    if text:\n        try:\n            text_clip = mpe.TextClip(text, fontsize=70, color='white', font='Arial-Bold')\n            text_clip = text_clip.set_position('center').set_duration(clip.duration)\n            return mpe.CompositeVideoClip([clip, text_clip])\n        except Exception as e:\n            raise ValueError(f\"Error adding text overlay: {e}\")\n    return clip", "add_narration": "def add_narration(clip, narration_file):\n    try:\n        return clip.set_audio(mpe.AudioFileClip(narration_file))\n    except Exception as e:\n        raise ValueError(f\"Error adding narration: {e}\")", "add_background_music": "def add_background_music(clip, music_file):\n    try:\n        background_audio = mpe.AudioFileClip(music_file)\n        return clip.set_audio(mpe.CompositeAudioClip([clip.audio, background_audio.volumex(0.1)]))\n    except Exception as e:\n        raise ValueError(f\"Error adding background music: {e}\")", "add_watermark": "def add_watermark(clip, watermark_text=\"Sample Watermark\"):\n    try:\n        watermark = mpe.TextClip(watermark_text, fontsize=30, color='white', font='Arial')\n        watermark = watermark.set_position(('right', 'bottom')).set_duration(clip.duration)\n        return mpe.CompositeVideoClip([clip, watermark])\n    except Exception as e:\n        st.error(f\"Error adding watermark: {e}\")\n        return clip", "split_video": "def split_video(video_clip, part_duration=10):\n    try:\n        return [video_clip.subclip(start, min(start + part_duration, video_clip.duration)) for start in range(0, int(video_clip.duration), part_duration)]\n    except Exception as e:\n        st.error(f\"Error splitting video: {e}\")\n        return [video_clip]", "merge_video_parts": "def merge_video_parts(video_parts):\n    try:\n        return mpe.concatenate_videoclips(video_parts, method=\"compose\")\n    except Exception as e:\n        st.error(f\"Error merging video parts: {e}\")\n        return video_parts[0] if video_parts else None", "save_storyboard_backup": "def save_storyboard_backup(storyboard, filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'w') as f:\n            json.dump(storyboard, f)\n        st.success(f\"Storyboard backup saved to {filename}\")\n    except Exception as e:\n        st.error(f\"Error saving storyboard backup: {e}\")", "load_storyboard_backup": "def load_storyboard_backup(filename=\"storyboard_backup.json\"):\n    try:\n        with open(filename, 'r') as f:\n            storyboard = json.load(f)\n        st.success(f\"Storyboard loaded from {filename}\")\n        return storyboard\n    except FileNotFoundError:\n        st.warning(f\"Backup file {filename} not found.\")\n        return None\n    except json.JSONDecodeError:\n        st.error(f\"Error decoding JSON from {filename}\")\n        return None\n    except Exception as e:\n        st.error(f\"Error loading storyboard backup: {e}\")\n        return None", "add_subtitles_to_video": "def add_subtitles_to_video(clip, subtitles):\n    try:\n        subtitle_clips = [\n            mpe.TextClip(subtitle['text'], fontsize=50, color='white', size=clip.size, font='Arial-Bold')\n            .set_position(('bottom')).set_start(subtitle['start']).set_duration(subtitle['duration'])\n            for subtitle in subtitles\n        ]\n        return mpe.CompositeVideoClip([clip] + subtitle_clips)\n    except Exception as e:\n        st.error(f\"Error adding subtitles: {e}\")\n        return clip", "preview_storyboard_slideshow": "def preview_storyboard_slideshow(scenes, duration_per_scene=5):\n    try:\n        slides = [create_animated_text(scene['title'], duration=duration_per_scene) for scene in scenes]\n        slideshow = mpe.concatenate_videoclips(slides, method='compose')\n        slideshow.write_videofile(\"storyboard_preview.mp4\", codec='libx264')\n        st.success(\"Storyboard preview created successfully.\")\n        st.video(\"storyboard_preview.mp4\")\n    except Exception as e:\n        st.error(f\"Error creating storyboard slideshow: {e}\")", "add_logo_to_video": "def add_logo_to_video(clip, logo_path, position=('right', 'top')):\n    try:\n        logo = mpe.ImageClip(logo_path).set_duration(clip.duration).resize(height=100).set_position(position)\n        return mpe.CompositeVideoClip([clip, logo])\n    except FileNotFoundError:\n        st.error(f\"Logo file not found: {logo_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error adding logo to video: {e}\")\n        return clip", "compress_video": "def compress_video(input_path, output_path=\"compressed_video.mp4\", bitrate=\"500k\"):\n    try:\n        os.system(f\"ffmpeg -i {input_path} -b:v {bitrate} -bufsize {bitrate} {output_path}\")\n        st.success(f\"Video compressed successfully. Saved to {output_path}\")\n    except Exception as e:\n        st.error(f\"Error compressing video: {e}\")", "apply_bw_filter": "def apply_bw_filter(clip):\n    try:\n        return clip.fx(mpe.vfx.blackwhite)\n    except Exception as e:\n        st.error(f\"Error applying black-and-white filter: {e}\")\n        return clip", "overlay_image_on_video": "def overlay_image_on_video(clip, image_path, position=(0, 0)):\n    try:\n        image = mpe.ImageClip(image_path).set_duration(clip.duration).set_position(position)\n        return mpe.CompositeVideoClip([clip, image])\n    except FileNotFoundError:\n        st.error(f\"Image file not found: {image_path}\")\n        return clip\n    except Exception as e:\n        st.error(f\"Error overlaying image on video: {e}\")\n        return clip", "adjust_video_speed": "def adjust_video_speed(clip, speed=1.0):\n    try:\n        return clip.fx(mpe.vfx.speedx, speed)\n    except Exception as e:\n        st.error(f\"Error adjusting video speed: {e}\")\n        return clip", "crop_video": "def crop_video(clip, x1, y1, x2, y2):\n    try:\n        return clip.crop(x1=x1, y1=y1, x2=x2, y2=y2)\n    except Exception as e:\n        st.error(f\"Error cropping video: {e}\")\n        return clip", "adjust_resolution_based_on_system": "def adjust_resolution_based_on_system(clip):\n    try:\n        memory = psutil.virtual_memory()\n        resolution = (640, 360) if memory.available < 1000 * 1024 * 1024 else (1280, 720)\n        return resize(clip, newsize=resolution)\n    except Exception as e:\n        st.error(f\"Error adjusting resolution: {e}\")\n        return clip", "generate_video_thumbnail": "def generate_video_thumbnail(clip, output_path=\"thumbnail.png\"):\n    try:\n        frame = clip.get_frame(1)\n        image = Image.fromarray(frame)\n        image.save(output_path)\n        st.success(f\"Thumbnail generated successfully. Saved to {output_path}\")\n        return output_path\n    except Exception as e:\n        st.error(f\"Error generating video thumbnail: {e}\")\n        return None", "add_intro_outro": "def add_intro_outro(final_clip, video_title):\n    intro_clip = create_animated_text(video_title, duration=3, font_size=60).fx(vfx.fadeout, duration=1)\n    outro_clip = create_animated_text(\"Thanks for Watching!\", duration=3, font_size=60).fx(vfx.fadein, duration=1)\n    return mpe.concatenate_videoclips([intro_clip, final_clip, outro_clip])", "adjust_audio_volume": "def adjust_audio_volume(audio_clip, volume_level=1.0):\n    try:\n        return audio_clip.volumex(volume_level)\n    except Exception as e:\n        st.error(f\"Error adjusting audio volume: {e}\")\n        return audio_clip", "generate_gradient_text_overlay": "def generate_gradient_text_overlay(text, clip_duration, size=(1920, 1080)):\n    try:\n        gradient = color_gradient(size, p1=(0, 0), p2=(size[0], size[1]), color1=(255, 0, 0), color2=(0, 0, 255))\n        image = Image.fromarray(gradient)\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default()\n        text_size = draw.textsize(text, font=font)\n        draw.text(((size[0] - text_size[0]) / 2, (size[1] - text_size[1]) / 2), text, font=font, fill=(255, 255, 255))\n        image.save(\"gradient_overlay.png\")\n        return mpe.ImageClip(\"gradient_overlay.png\").set_duration(clip_duration)\n    except Exception as e:\n        st.error(f\"Error generating gradient text overlay: {e}\")\n        return mpe.TextClip(text, fontsize=70, color='white', size=size).set_duration(clip_duration)", "run_video_rendering_thread": "def run_video_rendering_thread(target_function, *args):\n    try:\n        rendering_thread = threading.Thread(target=target_function, args=args)\n        rendering_thread.start()\n        return rendering_thread\n    except Exception as e:\n        st.error(f\"Error running rendering thread: {e}\")\n        return None", "check_system_capabilities": "def check_system_capabilities():\n    try:\n        memory = psutil.virtual_memory()\n        if memory.available < 500 * 1024 * 1024:  # Less than 500MB\n            st.warning(\"Low memory detected. Consider closing other applications.\")\n        cpu_usage = psutil.cpu_percent()\n        if cpu_usage > 80:\n            st.warning(\"High CPU usage detected. Rendering may be slow.\")\n    except Exception as e:\n        st.error(f\"Error checking system capabilities: {e}\")", "log_system_resources": "def log_system_resources():\n    try:\n        memory = psutil.virtual_memory()\n        cpu = psutil.cpu_percent()\n        st.write(f\"Memory Usage: {memory.percent}% | CPU Usage: {cpu}%\")\n    except Exception as e:\n        st.error(f\"Error logging system resources: {e}\")", "download_additional_assets": "def download_additional_assets(url):\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            temp_asset_file = NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_asset_file.write(response.content)\n            temp_asset_file.flush()\n            st.success(f\"Asset downloaded successfully: {temp_asset_file.name}\")\n            return temp_asset_file.name\n        else:\n            st.error(\"Failed to download asset. Invalid URL or server error.\")\n            return None\n    except Exception as e:\n        st.error(f\"Error downloading asset: {e}\")\n        return None", "calculate_estimated_render_time": "def calculate_estimated_render_time(duration, resolution=(1280, 720)):\n    try:\n        estimated_time = duration * (resolution[0] * resolution[1]) / 1e6\n        st.info(f\"Estimated rendering time: {estimated_time:.2f} seconds\")\n        return estimated_time\n    except Exception as e:\n        st.error(f\"Error calculating render time: {e}\")\n        return None", "manage_temp_directory": "def manage_temp_directory(directory_path):\n    try:\n        if os.path.exists(directory_path):\n            shutil.rmtree(directory_path)\n        os.makedirs(directory_path)\n        st.success(f\"Temporary directory created: {directory_path}\")\n    except Exception as e:\n        st.error(f\"Error managing temporary directory: {e}\")", "handle_session_expiration": "def handle_session_expiration():\n    try:\n        st.error(\"Session expired. Please refresh and try again.\")\n        if st.button(\"Refresh Page\"):\n            st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n    except Exception as e:\n        st.error(f\"Error handling session expiration: {e}\")", "split_storyboard_scenes": "def split_storyboard_scenes(scenes, batch_size=5):\n    try:\n        return [scenes[i:i + batch_size] for i in range(0, len(scenes), batch_size)]\n    except Exception as e:\n        st.error(f\"Error splitting storyboard scenes: {e}\")\n        return [scenes]", "add_transition_effects_between_scenes": "def add_transition_effects_between_scenes(scenes):\n    try:\n        return [animate_scene_transition(scene1, scene2) for scene1, scene2 in zip(scenes, scenes[1:])]\n    except Exception as e:\n        st.error(f\"Error adding transition effects: {e}\")\n        return scenes", "optimize_storyboard_text_prompts": "def optimize_storyboard_text_prompts(scenes):\n    try:\n        for scene in scenes:\n            scene['title'] = scene['title'].capitalize()\n        return scenes\n    except Exception as e:\n        st.error(f\"Error optimizing storyboard text prompts: {e}\")\n        return scenes", "select_background_music": "def select_background_music(genre):\n    try:\n        if genre in MUSIC_TRACKS:\n            track_url = MUSIC_TRACKS[genre]\n        else:\n            track_url = random.choice(list(MUSIC_TRACKS.values()))\n        \n        response = requests.get(track_url)\n        if response.status_code == 200:\n            temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n            temp_audio_file.write(response.content)\n            temp_audio_file.close()\n            return temp_audio_file.name\n    except Exception as e:\n        logger.error(f\"Error selecting background music: {str(e)}\")\n    \n    return None", "analyze_script": "def analyze_script(script):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Analyze the script and provide a JSON response with keys: 'sentiment' (POSITIVE, NEGATIVE, or NEUTRAL), 'style' (e.g., formal, casual, humorous), and 'transitions' (list of transition types).\"},\n                {\"role\": \"user\", \"content\": f\"Analyze this script: {script}\"}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0.7\n        )\n        \n        analysis = json.loads(response.choices[0].message.content)\n        \n        if not all(key in analysis for key in ['sentiment', 'style', 'transitions']):\n            raise ValueError(\"Invalid analysis structure\")\n        \n        return analysis\n    except Exception as e:\n        logger.error(f\"Error analyzing script: {str(e)}\")\n        return {\n            'sentiment': 'NEUTRAL',\n            'style': 'formal',\n            'transitions': ['fade', 'cut']\n        }", "apply_transition": "def apply_transition(clip1, clip2, transition_type):\n    transition_functions = {\n        'fade': lambda: clip1.crossfadeout(1).crossfadein(1),\n        'slide': lambda: clip1.slide_out(1, 'left').slide_in(1, 'right'),\n        'whip': lambda: clip1.fx(vfx.speedx, 2).fx(vfx.crop, x1=0, y1=0, x2=0.5, y2=1).crossfadeout(0.5),\n        'zoom': lambda: clip1.fx(vfx.resize, 1.5).fx(vfx.crop, x_center=0.5, y_center=0.5, width=1/1.5, height=1/1.5).crossfadeout(1)\n    }\n    return transition_functions.get(transition_type, lambda: clip1)()", "cleanup_temp_files": "def cleanup_temp_files():\n    try:\n        temp_dir = tempfile.gettempdir()\n        for filename in os.listdir(temp_dir):\n            if filename.startswith('videocreator_'):\n                file_path = os.path.join(temp_dir, filename)\n                if os.path.isfile(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n        logger.info(\"Temporary files cleaned up successfully.\")\n    except Exception as e:\n        logger.error(f\"Error cleaning up temporary files: {str(e)}\")", "process_scene_with_progress": "def process_scene_with_progress(scene, index, total_scenes):\n    scene_progress = st.empty()\n    scene_progress.text(f\"Processing scene {index + 1} of {total_scenes}: {scene['title']}\")\n    \n    clip_progress, voice_progress = st.columns(2)\n    \n    with clip_progress:\n        st.text(\"Creating video clip...\")\n        clip = create_fallback_clip(scene)\n        st.success(\"Video clip processed\")\n    \n    with voice_progress:\n        st.text(\"Generating voiceover...\")\n        narration_file = generate_voiceover(scene['narration'])\n        st.success(\"Voiceover generated\")\n    \n    scene_progress.success(f\"Scene {index + 1} processed successfully!\")\n    return {'clip': clip, 'scene': scene, 'narration': narration_file}", "generate_valid_storyboard": "def generate_valid_storyboard(prompt, style, max_attempts=3):\n    for attempt in range(max_attempts):\n        storyboard = generate_storyboard(prompt, style)\n        if storyboard is not None:\n            return storyboard\n        logger.warning(f\"Storyboard generation attempt {attempt + 1} failed. Retrying...\")\n    logger.error(\"Failed to generate a valid storyboard after multiple attempts.\")\n    st.error(\"Failed to generate a valid storyboard after multiple attempts. Please try again with a different prompt or style.\")\n    return None", "prompt_card": "def prompt_card(prompt):\n    st.markdown(f\"**Sample Prompt:** {prompt}\")\n    if st.button(\"Use this prompt\", key=f\"btn_{prompt}\"):\n        st.session_state.prompt = prompt", "predict_processing_issues": "def predict_processing_issues(video_clips, system_resources):\n    potential_issues = []\n    if len(video_clips) * 5 > system_resources['available_memory'] / 1e6:  # Assuming 5 seconds per clip\n        potential_issues.append(\"Insufficient memory for processing all clips\")\n    if system_resources['cpu_usage'] > 80:\n        potential_issues.append(\"High CPU usage may slow down processing\")\n    return potential_issues", "generate_script": "def generate_script(prompt, duration):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert video scriptwriter. Create a JSON script for a video based on the given prompt and duration. Use the following schema:\"},\n                {\"role\": \"user\", \"content\": f\"\"\"\n                Create a script for a {duration} second video about: {prompt}. \n                Return the script as a JSON object with the following structure:\n                {{\n                    \"title\": \"Overall video title\",\n                    \"scenes\": [\n                        {{\n                            \"scene_number\": 1,\n                            \"title\": \"Scene title\",\n                            \"description\": \"Detailed scene description\",\n                            \"narration\": \"Narration text for the scene\",\n                            \"duration\": \"Duration in seconds\"\n                        }}\n                    ]\n                }}\n                Include 3-5 scenes in total.\n                \"\"\"}\n            ],\n            max_tokens=1000,\n            response_format={\"type\": \"json_object\"}\n        )\n        script = json.loads(response.choices[0].message.content)\n        logger.info(f\"Generated script: {script}\")\n        return script\n    except Exception as e:\n        logger.error(f\"Error generating script: {str(e)}\")\n        return None", "create_video_workflow": "def create_video_workflow(prompt, duration, music_style):\n    try:\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n\n        logger.info(\"Starting video creation workflow\")\n        status_text.text(\"Generating script...\")\n        script = generate_script(prompt, duration)\n        if not script:\n            raise ValueError(\"Failed to generate script\")\n        logger.info(\"Script generated successfully\")\n        progress_bar.progress(20)\n\n        scenes = script['scenes']\n        if not scenes:\n            logger.error(f\"No scenes found in the script. Script content: {script}\")\n            raise ValueError(\"No scenes found in the script\")\n        logger.info(f\"Successfully extracted {len(scenes)} scenes\")\n        progress_bar.progress(30)\n\n        status_text.text(\"Fetching video clips...\")\n        video_clips = fetch_video_clips(scenes)\n        if not video_clips:\n            raise ValueError(\"Failed to fetch video clips\")\n        progress_bar.progress(50)\n\n        status_text.text(\"Generating voiceovers...\")\n        for clip_data in video_clips:\n            clip_data['narration'] = generate_voiceover(clip_data['scene']['narration'])\n        progress_bar.progress(70)\n\n        status_text.text(\"Selecting background music...\")\n        background_music_file = select_background_music(music_style)\n        progress_bar.progress(80)\n\n        status_text.text(\"Creating your video...\")\n        video_file = create_video(video_clips, background_music_file, script['title'])\n        progress_bar.progress(90)\n\n        if video_file and os.path.exists(video_file):\n            status_text.text(\"Finalizing...\")\n            progress_bar.progress(100)\n            logger.info(\"Video creation successful\")\n            st.success(\"\ud83c\udf89 Video created successfully!\")\n            st.video(video_file)\n            with open(video_file, 'rb') as vf:\n                st.download_button(label=\"\ud83d\udce5 Download Video\", data=vf, file_name=\"AutovideoAI_creation.mp4\")\n        else:\n            logger.error(\"Failed to create the video\")\n            st.error(\"\u274c Failed to create the video. Please try again.\")\n    except Exception as e:\n        logger.error(f\"An error occurred during video creation: {str(e)}\")\n        st.error(f\"An error occurred: {str(e)}\")\n    finally:\n        status_text.empty()\n        progress_bar.empty()", "main": "def main():\n    st.markdown(\"<h1 style='text-align: center; color: #4A90E2;'>AutovideoAI</h1>\", unsafe_allow_html=True)\n    st.markdown(\"<p style='text-align: center; font-size: 1.2em;'>Create Amazing Videos with AI</p>\", unsafe_allow_html=True)\n\n    with st.expander(\"\u2139\ufe0f How to use AutovideoAI\", expanded=False):\n        st.markdown(\"\"\"\n        1. Enter your video idea or choose a sample prompt.\n        2. Customize your video style, duration, and background music.\n        3. Generate a storyboard and preview it.\n        4. Create your AI-powered video!\n        \"\"\")\n\n    col1, col2 = st.columns([2, 1])\n    \n    with col1:\n        st.subheader(\"1\ufe0f\u20e3 Enter Your Video Idea\")\n        prompt = st.text_area(\"What's your video about?\", height=100, value=st.session_state.get('prompt', ''))\n    \n    with col2:\n        st.subheader(\"Sample Prompts\")\n        for sample_prompt in SAMPLE_PROMPTS:\n            if st.button(f\"\ud83d\udccc {sample_prompt}\", key=f\"btn_{sample_prompt}\"):\n                st.session_state.prompt = sample_prompt\n                st.rerun()  # Use st.rerun() instead of st.experimental_rerun()\n\n    st.subheader(\"2\ufe0f\u20e3 Customize Your Video\")\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        style = st.selectbox(\"Video Style \ud83c\udfad\", [\"Motivational\", \"Dramatic\", \"Educational\", \"Funny\"])\n    with col2:\n        duration = st.slider(\"Estimated Duration \u23f1\ufe0f\", 30, 300, 60, help=\"Duration in seconds\")\n    with col3:\n        music_style = st.selectbox(\"Background Music \ud83c\udfb5\", list(MUSIC_TRACKS.keys()))\n\n    if st.button(\"\ud83d\udd8b\ufe0f Generate Storyboard\", use_container_width=True):\n        with st.spinner(\"Crafting your storyboard...\"):\n            storyboard = generate_script(prompt, duration)\n        if storyboard:\n            st.session_state.storyboard = storyboard\n            st.success(\"\u2705 Storyboard generated successfully!\")\n            display_storyboard_preview(storyboard)\n        else:\n            st.error(\"\u274c Failed to generate a storyboard. Please try again.\")\n\n    if 'storyboard' in st.session_state:\n        if st.button(\"\ud83c\udfac Create Video\", use_container_width=True):\n            create_video_workflow(prompt, duration, music_style)", "display_storyboard_preview": "def display_storyboard_preview(storyboard):\n    with st.expander(\"\ud83d\udd0d Preview Storyboard\", expanded=True):\n        st.markdown(f\"### {storyboard['title']}\")\n        st.markdown(\"### Scene Breakdown\")\n        for scene in storyboard['scenes']:\n            with st.container():\n                col1, col2 = st.columns([1, 2])\n                with col1:\n                    st.markdown(f\"**Scene {scene['scene_number']}**\")\n                    st.write(f\"Duration: {scene['duration']} seconds\")\n                with col2:\n                    st.markdown(f\"**{scene['title']}**\")\n                    st.write(f\"{scene['description']}\")\n                st.markdown(\"---\")", "color_gradient": "def color_gradient(size, p1, p2, color1, color2):\n    x = np.linspace(0, 1, size[0])[:, None]\n    y = np.linspace(0, 1, size[1])[None, :]\n    gradient = x * (p2[0] - p1[0]) + y * (p2[1] - p1[1])\n    gradient = np.clip(gradient, 0, 1)\n    return np.array(color1) * (1 - gradient[:, :, None]) + np.array(color2) * gradient[:, :, None]"}}, "openai_model": "gpt-4o-mini"}